{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Primera Parte: An√°lisis Cuantitativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1\tPrimer examen preliminar del dataset.  ¬øEn qu√© formato est√° el dataset y qu√© tiene que ver este formato con Big Data? ¬øQu√© par√°metros hay en el dataset? ¬øCu√°l es su significado? ¬øExisten valores aparentemente incorrectos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalar los paquetes necesarios, ejecutar solo al principio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.5)\n",
            "Requirement already satisfied: pyarrow in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (20.0.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: seaborn in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.1)\n",
            "Requirement already satisfied: nbformat in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.10.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.15.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: pyspark in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.5.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from plotly) (1.37.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbformat) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from nbformat) (5.14.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\intekmedical\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (310)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\intekmedical\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Instalar paquetes necesarios\n",
        "%pip install pandas numpy pyarrow matplotlib seaborn plotly nbformat scipy scikit-learn pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importar librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8GPDQVVaGW7A"
      },
      "outputs": [],
      "source": [
        "# üì¶ Imports principales para an√°lisis de datos y visualizaci√≥n\n",
        "\n",
        "# Carga y manipulaci√≥n de datos\n",
        "import pandas as pd       # type: ignore # Manipulaci√≥n y an√°lisis de datos tabulares\n",
        "import numpy as np        # type: ignore # Operaciones num√©ricas (opcional, pero √∫til)\n",
        "\n",
        "# Lectura de archivos Parquet\n",
        "import pyarrow.parquet as pq  # type: ignore # Backend recomendado para parquet (alternativa: fastparquet)\n",
        "\n",
        "# Visualizaci√≥n b√°sica y exploratoria\n",
        "import matplotlib.pyplot as plt      # type: ignore # Visualizaciones est√°ndar (histogramas, scatter, etc.)\n",
        "import seaborn as sns                # type: ignore # Visualizaci√≥n estad√≠stica avanzada (heatmaps, countplots)\n",
        "\n",
        "# Visualizaci√≥n avanzada \n",
        "import plotly.express as px         # type: ignore # Visualizaciones interactivas (incl. Sankey, mapas, etc.)\n",
        "import plotly.graph_objects as go # type: ignore # Gr√°ficos m√°s complejos y personalizados\n",
        "\n",
        "# Configuraci√≥n general de visualizaciones\n",
        "plt.style.use('seaborn-v0_8')        # Estilo visual est√°ndar\n",
        "# %matplotlib inline                   # Para visualizar directamente en el notebook\n",
        "\n",
        "# Para calcular el z-score de las columnas num√©ricas\n",
        "from scipy.stats import zscore # type: ignore\n",
        "\n",
        "# Para manejar rutas de archivos y directorios\n",
        "import os\n",
        "\n",
        "# linear models para regresi√≥n\n",
        "from sklearn.linear_model import RANSACRegressor, LinearRegression # type: ignore\n",
        "\n",
        "# Para manejar el entorno de Spark\n",
        "from pyspark.sql import SparkSession # type: ignore\n",
        "from pyspark.sql.functions import col, when, isnull, lit, mean, median, count, udf  # type: ignore\n",
        "from pyspark.sql.types import BooleanType, FloatType # type: ignore\n",
        "from pyspark.sql.window import Window # type: ignore\n",
        "import pyspark.sql.functions as F # type: ignore\n",
        "from datetime import datetime\n",
        "import findspark  # Opcional # type: ignore\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carga los nombres de zona desde taxi_zone_lookup.csv en zona_nombres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargado desde archivo local.\n"
          ]
        }
      ],
      "source": [
        "def get_zona_nombres():\n",
        "    \"\"\"\n",
        "    Devuelve un diccionario {LocationID: Zone} con los nombres de zona de NYC.\n",
        "    1. Si el archivo local 'datasets/taxi_zone_lookup.csv' existe, lo carga desde ah√≠.\n",
        "    2. Si no existe, lo descarga de la URL y lo guarda localmente para futuras ejecuciones.\n",
        "    \"\"\"\n",
        "    local_path = \"datasets/taxi_zone_lookup.csv\"\n",
        "    url_lookup = \"https://raw.githubusercontent.com/renerubio/NYC-TAXI/refs/heads/main/datasets/taxi_zone_lookup.csv\"\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        taxi_zones = pd.read_csv(local_path)\n",
        "        print(\"Cargado desde archivo local.\")\n",
        "    else:\n",
        "        taxi_zones = pd.read_csv(url_lookup)\n",
        "        print(\"Cargado desde URL.\")\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "        taxi_zones.to_csv(local_path, index=False)\n",
        "        print(f\"Guardado en {local_path}.\")\n",
        "\n",
        "    return taxi_zones.set_index(\"LocationID\")[\"Zone\"].to_dict()\n",
        "\n",
        "# Cargar diccionario\n",
        "zona_nombres = get_zona_nombres()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qUVDhbOJ4C-"
      },
      "source": [
        "### Importar y cargar ficheros parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tTdCSulJBxm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cargado desde archivo local: datasets/yellow_tripdata_2025-02.parquet\n",
            "‚úÖ Cargado desde archivo local: datasets/green_tripdata_2025-02.parquet\n",
            "‚úÖ Cargado desde archivo local: datasets/fhv_tripdata_2025-02.parquet\n",
            "‚úÖ Cargado desde archivo local: datasets/fhvhv_tripdata_2025-02.parquet\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n reutilizable para cargar Parquet desde local o URL\n",
        "def cargar_parquet_local_o_url(local_path: str, url: str) -> pd.DataFrame:\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"‚úÖ Cargado desde archivo local: {local_path}\")\n",
        "    else:\n",
        "        print(f\"üåê Descargando desde URL: {url}\")\n",
        "        df = pd.read_parquet(url, engine=\"pyarrow\")\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "        df.to_parquet(local_path, engine=\"pyarrow\", index=False)\n",
        "        print(f\"üíæ Guardado en {local_path}\")\n",
        "        return df\n",
        "\n",
        "    return pd.read_parquet(local_path, engine=\"pyarrow\")\n",
        "\n",
        "# Ruta base de trip-data\n",
        "data_path = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
        "\n",
        "# Diccionario de datasets y sus rutas\n",
        "datasets = {\n",
        "    \"yellow_df_2025_02\": (\"datasets/yellow_tripdata_2025-02.parquet\", data_path + \"yellow_tripdata_2025-02.parquet\"),\n",
        "    \"green_df_2025_02\":  (\"datasets/green_tripdata_2025-02.parquet\",  data_path + \"green_tripdata_2025-02.parquet\"),\n",
        "    \"fhv_df_2025_02\":    (\"datasets/fhv_tripdata_2025-02.parquet\",    data_path + \"fhv_tripdata_2025-02.parquet\"),\n",
        "    \"fhvhv_df_2025_02\":  (\"datasets/fhvhv_tripdata_2025-02.parquet\",  data_path + \"fhvhv_tripdata_2025-02.parquet\"),\n",
        "    \"yellow_df_2024_12\": (\"datasets/yellow_tripdata_2024-12.parquet\", data_path + \"yellow_tripdata_2024-12.parquet\"),\n",
        "    \"green_df_2024_12\":  (\"datasets/green_tripdata_2024-12.parquet\",  data_path + \"green_tripdata_2024-12.parquet\"),\n",
        "    \"fhv_df_2024_12\":    (\"datasets/fhv_tripdata_2024-12.parquet\",    data_path + \"fhv_tripdata_2024-12.parquet\"),\n",
        "    \"fhvhv_df_2024_12\":  (\"datasets/fhvhv_tripdata_2024-12.parquet\",  data_path + \"fhvhv_tripdata_2024-12.parquet\"),\n",
        "    \"yellow_df_2009_02\": (\"datasets/yellow_tripdata_2009-02.parquet\", data_path + \"yellow_tripdata_2009-02.parquet\"), # Ejercicio 2.5\n",
        "}\n",
        "\n",
        "# Cargar todos los datasets\n",
        "for nombre, (ruta_local, url_remota) in datasets.items():\n",
        "    globals()[nombre] = cargar_parquet_local_o_url(ruta_local, url_remota)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objeto global con los datasets cargados y listos para usar "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos todos los datasets con sus nombres\n",
        "datasets = {\n",
        "    \"yellow_tripdata_2025-02\": yellow_df_2025_02, # type: ignore\n",
        "    \"green_tripdata_2025-02\": green_df_2025_02, # type: ignore\n",
        "    \"fhv_tripdata_2025-02\": fhv_df_2025_02, # type: ignore\n",
        "    \"fhvhv_tripdata_2025-02\": fhvhv_df_2025_02, # type: ignore\n",
        "    \"yellow_tripdata_2024-12\": yellow_df_2024_12, # type: ignore\n",
        "    \"green_tripdata_2024-12\": green_df_2024_12, # type: ignore\n",
        "    \"fhv_tripdata_2024-12\": fhv_df_2024_12, # type: ignore\n",
        "    \"fhvhv_tripdata_2024-12\": fhvhv_df_2024_12, # type: ignore\n",
        "    \"yellow_tripdata_2009-02\": yellow_df_2009_02, # type: ignore // Ejercicio 2.5\n",
        "}\n",
        "\n",
        "# Liberar memoria de las variables ya cargadas\n",
        "del yellow_df_2025_02 # type: ignore\n",
        "del green_df_2025_02 # type: ignore\n",
        "del fhv_df_2025_02 # type: ignore\n",
        "del fhvhv_df_2025_02 # type: ignore\n",
        "del yellow_df_2024_12 # type: ignore\n",
        "del green_df_2024_12 # type: ignore\n",
        "del fhv_df_2024_12 # type: ignore\n",
        "del fhvhv_df_2024_12 # type: ignore\n",
        "del yellow_df_2009_02 # type: ignore // Ejercicio 2.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An√°lisis de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Presencia de columnas en cada dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos todas las columnas √∫nicas\n",
        "all_columns = set()\n",
        "for df in datasets.values():\n",
        "    all_columns.update(df.columns)\n",
        "\n",
        "# Calculamos frecuencia de cada columna (en cu√°ntos datasets aparece)\n",
        "column_freq = {col: sum(col in df.columns for df in datasets.values()) for col in all_columns}\n",
        "\n",
        "# Ordenamos columnas por frecuencia (mayor a menor) y luego alfab√©ticamente\n",
        "sorted_columns = sorted(all_columns, key=lambda x: (-column_freq[x], x))\n",
        "\n",
        "# Creamos la tabla de presencia\n",
        "presence_data = []\n",
        "for name, df in datasets.items():\n",
        "    presence_data.append([name] + ['S√≠' if col in df.columns else 'No' for col in sorted_columns])\n",
        "\n",
        "# Creamos el DataFrame\n",
        "presence_df = pd.DataFrame(presence_data, columns=['Dataset'] + sorted_columns)\n",
        "\n",
        "# Mostramos la tabla\n",
        "print(\"Presencia de columnas en cada dataset (ordenadas por frecuencia):\")\n",
        "print(presence_df.to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtener todas las columnas y su frecuencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_column_presence_heatmap(datasets):\n",
        "    \"\"\"\n",
        "    Genera un heatmap de presencia de columnas a partir de un diccionario de datasets.\n",
        "\n",
        "    :param datasets: Diccionario con claves como nombres de dataset y valores como DataFrames\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Obtener todas las columnas y su frecuencia\n",
        "    all_columns = set()\n",
        "    for df in datasets.values():\n",
        "        all_columns.update(df.columns)\n",
        "    column_freq = {col: sum(col in df.columns for df in datasets.values()) for col in all_columns}\n",
        "\n",
        "    # 2. Ordenar columnas por frecuencia (de mayor a menor)\n",
        "    sorted_columns = sorted(all_columns, key=lambda x: (-column_freq[x], x))\n",
        "\n",
        "    # 3. Crear matriz de presencia (S√≠/No)\n",
        "    presence_matrix = []\n",
        "    for name, df in datasets.items():\n",
        "        presences = [1 if col in df.columns else 0 for col in sorted_columns]\n",
        "        presences = [\"S√≠\" if p == 1 else \"No\" for p in presences]\n",
        "        presence_matrix.append(presences)\n",
        "\n",
        "    # 4. Crear DataFrame para el heatmap\n",
        "    heatmap_df = pd.DataFrame(\n",
        "        presence_matrix,\n",
        "        index=datasets.keys(),\n",
        "        columns=sorted_columns\n",
        "    )\n",
        "\n",
        "    # 5. Configurar el gr√°fico\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.heatmap(\n",
        "        heatmap_df.replace({\"S√≠\": 1, \"No\": 0}),\n",
        "        cmap=[\"#FF6B6B\", \"#51CF66\"],\n",
        "        linewidths=0.5,\n",
        "        linecolor=\"lightgray\",\n",
        "        annot=heatmap_df.values,\n",
        "        fmt=\"\",\n",
        "        cbar=False\n",
        "    )\n",
        "\n",
        "    # 6. Personalizar el gr√°fico\n",
        "    plt.title(\"Presencia de Columnas en los Datasets\", fontsize=14, pad=20)\n",
        "    plt.xlabel(\"Columnas (ordenadas por frecuencia)\", fontsize=10)\n",
        "    plt.ylabel(\"Dataset\", fontsize=10)\n",
        "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_column_presence_heatmap(datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Renombrar columnas/atributos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objeto global `column_groups` son grupos de columnas equivalentes para normalizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grupos de columnas equivalentes para normalizaci√≥n\n",
        "column_groups = {\n",
        "     # Columnas de ubicaci√≥n\n",
        "    'PULocationID': ['PUlocationID', 'PULocationID'],\n",
        "    'DOLocationID': ['DOlocationID', 'DOLocationID'],\n",
        "\n",
        "    # Columnas temporales\n",
        "    'pickup_datetime': [\n",
        "        'tpep_pickup_datetime', 'lpep_pickup_datetime',\n",
        "        'pickup_datetime', 'Trip_Pickup_DateTime',\n",
        "        'request_datetime'\n",
        "    ],\n",
        "    'dropoff_datetime': [\n",
        "        'tpep_dropoff_datetime', 'lpep_dropoff_datetime',\n",
        "        'dropOff_datetime', 'dropoff_datetime', 'on_scene_datetime',\n",
        "        'Trip_Dropoff_DateTime'\n",
        "    ],\n",
        "\n",
        "    # Columnas de distancia\n",
        "    'trip_distance': [\n",
        "        'trip_distance',\n",
        "        'Trip_distance',\n",
        "        'Trip_Distance',\n",
        "        'distance',\n",
        "        'trip_miles'\n",
        "    ],\n",
        "\n",
        "    # Columnas de importe total\n",
        "    'total_amount': ['total_amount', 'Total_Amt'],\n",
        "\n",
        "    # Columnas de tarifa base\n",
        "    'fare_amount': ['fare_amount', 'Fare_Amt'],\n",
        "    \n",
        "    # Columnas de tarifas\n",
        "    'airport_fee': ['airport_fee', 'Airport_fee'],\n",
        "    'extra': ['extra', 'surcharge'],\n",
        "    \n",
        "    # Columnas de pago\n",
        "    'tolls': ['tolls_amount', 'tolls', 'Tolls_Amt'],\n",
        "    'tips': ['tip_amount', 'tips', 'Tip_Amt'],\n",
        "    'payment_type': ['payment_type', 'Payment_Type'],\n",
        "\n",
        "    # Columnas de c√≥digo de tarifa\n",
        "    'RatecodeID': ['RatecodeID', 'Rate_Code'],\n",
        "\n",
        "    # Columnas de ID de proveedor\n",
        "    'VendorID': ['VendorID', 'vendor_name'],\n",
        "\n",
        "    # Columnas de n√∫mero de pasajeros\n",
        "    'passenger_count': ['passenger_count', 'Passenger_Count'],\n",
        "\n",
        "    # Columnas de store and forward\n",
        "    'store_and_fwd_flag': ['store_and_fwd_flag', 'store_and_forward'],\n",
        "\n",
        "    # Columnas especiales de FHV y FHVHV\n",
        "    'base_number': [\n",
        "        'dispatching_base_num', 'Affiliated_base_number',\n",
        "        'originating_base_num', 'hvfhs_license_num'\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vamos a renombrar los par√°metros para tener uniformidad para los futuros an√°lisis\n",
        "- Renombra columnas en cada dataset seg√∫n el diccionario column_groups.\n",
        "- A√±ade same_location_flag cuando el punto de recogida y destino son iguales.\n",
        "- A√±ade suspicious_location_flag si adem√°s la distancia es 0 o el total_amount es muy alto (umbral > 1000$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'copy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m         datasets_normalizados[nombre] = df\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m datasets_normalizados\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m normalized_datasets = \u001b[43mrename_column_names_with_flags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_groups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrename_column_names_with_flags\u001b[39m\u001b[34m(datasets_dict, column_mapping)\u001b[39m\n\u001b[32m     15\u001b[39m datasets_normalizados = {}\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nombre, df \u001b[38;5;129;01min\u001b[39;00m datasets_dict.items():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m()\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# 1. Renombra columnas\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m standard_name, variants \u001b[38;5;129;01min\u001b[39;00m column_mapping.items():\n",
            "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'copy'"
          ]
        }
      ],
      "source": [
        "# Renombra las columnas y creaci√≥n de flags 55 segundos\n",
        "def rename_column_names_with_flags(datasets_dict, column_mapping):\n",
        "    \"\"\"\n",
        "    Renombra columnas y agrega:\n",
        "    - same_location_flag: True si PULocationID == DOLocationID\n",
        "    - suspicious_location_flag: True si PULocationID == DOLocationID y (trip_distance == 0 o total_amount > umbral)\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        column_mapping: mapeo de nombres est√°ndar a variantes\n",
        "\n",
        "    Retorna:\n",
        "        diccionario con DataFrames normalizados y enriquecidos con flags\n",
        "    \"\"\"\n",
        "    datasets_normalizados = {}\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # 1. Renombra columnas\n",
        "        for standard_name, variants in column_mapping.items():\n",
        "            for variant in variants:\n",
        "                if variant in df.columns:\n",
        "                    if standard_name in df.columns and variant != standard_name:\n",
        "                        df.drop(columns=[variant], inplace=True)\n",
        "                    else:\n",
        "                        df.rename(columns={variant: standard_name}, inplace=True)\n",
        "        df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "        # 2. Agregado de flag same_location_flag\n",
        "        if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "            df[\"same_location_flag\"] = df[\"PULocationID\"] == df[\"DOLocationID\"]\n",
        "\n",
        "            # 3. Agregado de flag suspicious_location_flag\n",
        "            if \"trip_distance\" in df.columns and \"total_amount\" in df.columns:\n",
        "                df[\"suspicious_location_flag\"] = (\n",
        "                    (df[\"same_location_flag\"]) &\n",
        "                    ((df[\"trip_distance\"] == 0) | (df[\"total_amount\"] > 1000))  # umbral arbitrario ajustable\n",
        "                )\n",
        "            else:\n",
        "                df[\"suspicious_location_flag\"] = False\n",
        "        else:\n",
        "            df[\"same_location_flag\"] = False\n",
        "            df[\"suspicious_location_flag\"] = False\n",
        "\n",
        "        datasets_normalizados[nombre] = df\n",
        "\n",
        "    return datasets_normalizados\n",
        "\n",
        "normalized_datasets = rename_column_names_with_flags(datasets, column_groups)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap antes y despu√©s de normalizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# heatmap de columnas antes de normalizar\n",
        "plot_column_presence_heatmap(datasets)\n",
        "# heatmap de columnas despu√©s de normalizar\n",
        "plot_column_presence_heatmap(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificaci√≥n de columnas y valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificamos la calidad de las columnas ( 1 minuto)\n",
        "def verificar_calidad_columnas_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Verifica la calidad de las columnas para cada DataFrame del diccionario.\n",
        "    Incluye chequeos de duplicados, nulos, NaN, negativos y tipos de datos.\n",
        "    \"\"\"\n",
        "    for nombre_df, df in datasets_dict.items():\n",
        "        print(f\"\\nüìã Verificaci√≥n de columnas para: {nombre_df}\")\n",
        "\n",
        "        # 1. Columnas duplicadas\n",
        "        duplicadas = df.columns[df.columns.duplicated()]\n",
        "        if not duplicadas.empty:\n",
        "            print(f\"üîÅ Columnas duplicadas: {duplicadas.tolist()}\")\n",
        "        else:\n",
        "            print(\"‚úÖ Sin columnas duplicadas.\")\n",
        "\n",
        "        # 2. Columnas con nulos\n",
        "        nulos = df.isnull().sum()\n",
        "        nulos = nulos[nulos > 0]\n",
        "        if not nulos.empty:\n",
        "            print(\"üï≥Ô∏è Columnas con valores nulos:\")\n",
        "            print(nulos)\n",
        "        else:\n",
        "            print(\"‚úÖ Sin valores nulos.\")\n",
        "\n",
        "        # 3. Columnas con NaN (redundante con isnull, pero mantenido por claridad)\n",
        "        nans = df.isna().sum()\n",
        "        nans = nans[nans > 0]\n",
        "        if not nans.empty:\n",
        "            print(\"üßº Columnas con valores NaN:\")\n",
        "            print(nans)\n",
        "        else:\n",
        "            print(\"‚úÖ Sin valores NaN.\")\n",
        "\n",
        "        # 4. Columnas num√©ricas con valores negativos\n",
        "        negativos = df.select_dtypes(include=[np.number]).lt(0).sum()\n",
        "        negativos = negativos[negativos > 0]\n",
        "        if not negativos.empty:\n",
        "            print(\"üîª Columnas con valores negativos:\")\n",
        "            print(negativos)\n",
        "        else:\n",
        "            print(\"‚úÖ Sin valores negativos.\")\n",
        "\n",
        "        # 5. Columnas no num√©ricas\n",
        "        no_numericas = df.select_dtypes(exclude=[np.number]).nunique()\n",
        "        if not no_numericas.empty:\n",
        "            print(\"üî§ Columnas no num√©ricas:\")\n",
        "            print(no_numericas)\n",
        "        else:\n",
        "            print(\"‚úÖ Todas las columnas son num√©ricas (o no hay no num√©ricas con valores √∫nicos).\")\n",
        "\n",
        "        # 6. Columnas datetime\n",
        "        columnas_fecha = df.select_dtypes(include=[\"datetime64[ns]\"])\n",
        "        if not columnas_fecha.empty:\n",
        "            print(f\"üìÖ Columnas de fecha v√°lidas: {list(columnas_fecha.columns)}\")\n",
        "        else:\n",
        "            print(\"‚ùå No hay columnas de tipo fecha detectadas.\")\n",
        "\n",
        "        # 7. Columnas de texto\n",
        "        columnas_texto = df.select_dtypes(include=[object]).nunique()\n",
        "        if not columnas_texto.empty:\n",
        "            print(\"‚úèÔ∏è Columnas de texto:\")\n",
        "            print(columnas_texto)\n",
        "        else:\n",
        "            print(\"‚úÖ Sin columnas de texto o con cardinalidad vac√≠a.\")\n",
        "\n",
        "\n",
        "# Ejecutar para \"Yellow Feb 2025\"\n",
        "verificar_calidad_columnas_dict(normalized_datasets)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limpieza de Datos (12min)\n",
        "(Opcional, ejecutar si da error) \n",
        "El proceso de limpieza es muy costoso a nivel de recursos, por lo que primero liberaremos memoria para su ejecuci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" import gc\n",
        "del datasets\n",
        "gc.collect() \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpieza de datos con pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 0. Limpieza de datos glogal 12min\n",
        "def limpiar_dataset_integrado(datasets_dict, zona_nombres, min_freq=5):\n",
        "    \"\"\"\n",
        "    Optimiza la limpieza de m√∫ltiples columnas clave de datasets de taxi.\n",
        "    - Imputa valores inv√°lidos en: passenger_count, trip_distance, trip_duration_min, total_amount.\n",
        "    - A√±ade la columna 'location_valid'.\n",
        "    - Evita apply por fila y usa map/merge para m√°xima eficiencia.\n",
        "    \"\"\"\n",
        "    datasets_corregidos = {}\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        try:\n",
        "            print(f\"\\nüîÑ Procesando: {nombre}\")\n",
        "            df = df.copy()\n",
        "\n",
        "            # A√±adir location_valid\n",
        "            if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "                df[\"location_valid\"] = df[\"PULocationID\"].isin(zona_nombres) & df[\"DOLocationID\"].isin(zona_nombres)\n",
        "                df[\"key\"] = list(zip(df[\"PULocationID\"], df[\"DOLocationID\"]))\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: faltan columnas de localizaci√≥n. Se omite.\")\n",
        "                continue\n",
        "\n",
        "            # Correcci√≥n de passenger_count \n",
        "            # TODO REFINAR ESTA PARTE - ESTAMOS METIENDO RUIDO\n",
        "            if \"passenger_count\" in df.columns:\n",
        "                mask = df[\"passenger_count\"].isnull() | (df[\"passenger_count\"] < 0)\n",
        "                mediana_global = df.loc[~mask, \"passenger_count\"].median()\n",
        "                medianas = df.loc[~mask].groupby(\"key\")[\"passenger_count\"].median()\n",
        "                df.loc[mask, \"passenger_count\"] = df.loc[mask, \"key\"].map(medianas).fillna(mediana_global)\n",
        "\n",
        "            # Correcci√≥n de trip_distance\n",
        "            # TODO REFINAR ESTA PARTE - ESTAMOS METIENDO RUIDO\n",
        "            if \"trip_distance\" in df.columns:\n",
        "                mask = df[\"trip_distance\"].isnull() | (df[\"trip_distance\"] <= 0)\n",
        "                mediana_global = df.loc[~mask, \"trip_distance\"].median()\n",
        "\n",
        "                # C√°lculo combinado\n",
        "                valid_df = df.loc[~mask].copy()\n",
        "                freq = valid_df.groupby(\"key\").size()\n",
        "                comunes = freq[freq >= min_freq].index\n",
        "                medianas = valid_df.groupby(\"key\")[\"trip_distance\"].median()\n",
        "                df.loc[mask, \"trip_distance\"] = df.loc[mask, \"key\"].apply(\n",
        "                    lambda k: medianas[k] if k in comunes else mediana_global\n",
        "                )\n",
        "\n",
        "            # Correcci√≥n de trip_duration_min\n",
        "            if \"pickup_datetime\" in df.columns and \"dropoff_datetime\" in df.columns:\n",
        "                if \"trip_duration_min\" not in df.columns:\n",
        "                    df[\"trip_duration_min\"] = (\n",
        "                        pd.to_datetime(df[\"dropoff_datetime\"]) - pd.to_datetime(df[\"pickup_datetime\"])\n",
        "                    ).dt.total_seconds() / 60\n",
        "\n",
        "                mask = df[\"trip_duration_min\"].isnull() | (df[\"trip_duration_min\"] <= 0)\n",
        "                media_global = df.loc[~mask, \"trip_duration_min\"].mean()\n",
        "                medias = df.loc[~mask].groupby(\"key\")[\"trip_duration_min\"].mean()\n",
        "                df.loc[mask, \"trip_duration_min\"] = df.loc[mask, \"key\"].map(medias).fillna(media_global)\n",
        "\n",
        "            # Correcci√≥n de total_amount\n",
        "            if \"total_amount\" in df.columns:\n",
        "                mask = df[\"total_amount\"].isnull() | (df[\"total_amount\"] <= 0)\n",
        "                media_global = df.loc[~mask, \"total_amount\"].mean()\n",
        "                medias = df.loc[~mask].groupby(\"key\")[\"total_amount\"].mean()\n",
        "                df.loc[mask, \"total_amount\"] = df.loc[mask, \"key\"].map(medias).fillna(media_global)\n",
        "\n",
        "            # Eliminar columna auxiliar\n",
        "            df.drop(columns=[\"key\"], inplace=True)\n",
        "\n",
        "            datasets_corregidos[nombre] = df\n",
        "            print(f\"‚úÖ {nombre}: limpieza optimizada completada.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error procesando {nombre}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return datasets_corregidos\n",
        "\n",
        "# Aplicar limpieza a los datasets normalizados\n",
        "normalized_datasets = limpiar_dataset_integrado(normalized_datasets, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpieza de datos con Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Diccionario de parquets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diccionario de datasets y sus rutas\n",
        "parquet_local_paths = {\n",
        "    \"yellow_df_2025_02\": \"datasets/yellow_tripdata_2025-02.parquet\",\n",
        "    \"green_df_2025_02\":  \"datasets/green_tripdata_2025-02.parquet\",\n",
        "    \"fhv_df_2025_02\":    \"datasets/fhv_tripdata_2025-02.parquet\",\n",
        "    \"fhvhv_df_2025_02\":  \"datasets/fhvhv_tripdata_2025-02.parquet\",\n",
        "    \"yellow_df_2024_12\": \"datasets/yellow_tripdata_2024-12.parquet\",\n",
        "    \"green_df_2024_12\":  \"datasets/green_tripdata_2024-12.parquet\", \n",
        "    \"fhv_df_2024_12\":    \"datasets/fhv_tripdata_2024-12.parquet\",\n",
        "    \"fhvhv_df_2024_12\":  \"datasets/fhvhv_tripdata_2024-12.parquet\",\n",
        "    \"yellow_df_2009_02\": \"datasets/yellow_tripdata_2009-02.parquet\", \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, unix_timestamp, concat_ws\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, unix_timestamp, concat_ws\n",
        "\n",
        "def limpiar_dataset_integrado_spark_parquet(parquet_dict, zona_nombres_set, min_freq=5):\n",
        "    \"\"\"\n",
        "    Limpieza de datasets parquet con PySpark. A√±ade columnas de validaci√≥n e imputa valores.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - parquet_dict: dict {nombre_dataset: ruta_local_parquet}\n",
        "    - zona_nombres_set: set con IDs v√°lidos de zonas\n",
        "    - min_freq: frecuencia m√≠nima para considerar trayecto como frecuente\n",
        "\n",
        "    Retorna:\n",
        "    - dict {nombre_dataset: DataFrame Spark limpio o sin modificar si no es limpiable}\n",
        "    \"\"\"\n",
        "    spark = SparkSession.builder.appName(\"LimpiezaTaxiSpark\").getOrCreate()\n",
        "    corregidos = {}\n",
        "\n",
        "    for nombre, path in parquet_dict.items():\n",
        "        try:\n",
        "            print(f\"\\nüîÑ Procesando: {nombre}\")\n",
        "            df = spark.read.parquet(path)\n",
        "\n",
        "            if not all(c in df.columns for c in [\"PULocationID\", \"DOLocationID\"]):\n",
        "                print(f\"‚ö†Ô∏è {nombre}: faltan columnas de localizaci√≥n. Se devuelve sin modificar.\")\n",
        "                corregidos[nombre] = df  # guardar tal cual\n",
        "                continue\n",
        "\n",
        "            df = df.withColumn(\"location_valid\", col(\"PULocationID\").isin(zona_nombres_set) & col(\"DOLocationID\").isin(zona_nombres_set))\n",
        "            df = df.withColumn(\"key\", concat_ws(\"_\", col(\"PULocationID\"), col(\"DOLocationID\")))\n",
        "\n",
        "            if \"pickup_datetime\" in df.columns and \"dropoff_datetime\" in df.columns:\n",
        "                df = df.withColumn(\"trip_duration_min\",\n",
        "                                   (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60)\n",
        "\n",
        "            for campo in [\"passenger_count\", \"trip_distance\", \"total_amount\"]:\n",
        "                if campo in df.columns:\n",
        "                    df = df.withColumn(campo, when((col(campo) <= 0) | col(campo).isNull(), None).otherwise(col(campo)))\n",
        "\n",
        "            corregidos[nombre] = df\n",
        "            print(f\"‚úÖ {nombre}: limpieza completada en Spark.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en {nombre}: {e}\")\n",
        "            corregidos[nombre] = None  # O guarda el error si prefieres: str(e)\n",
        "\n",
        "    return corregidos\n",
        "\n",
        "    \n",
        "zona_ids_validos = set(zona_nombres.keys())\n",
        "parquet_local_paths = {k: v[0] for k, v in datasets.items()}\n",
        "\n",
        "corregidos_spark = limpiar_dataset_integrado_spark_parquet(parquet_local_paths, zona_ids_validos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Opcional - Cargar todos los datasets *.parquet limpios en un diccionario normalized_datasets\n",
        "Es opcional si ya los tenemos creados, podr√≠amos cargarlos directamente en vez de pasar todo el proceso de normalizaci√≥n, limpieza, unificaci√≥n y guardado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cargar_datasets_parquet(carpeta=\"datasets_limpios\"):\n",
        "    \"\"\"\n",
        "    Carga los archivos .parquet uno a uno desde la carpeta especificada\n",
        "    y los guarda en un diccionario con nombres legibles.\n",
        "\n",
        "    Evita problemas de memoria al no cargar todos simult√°neamente.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    for archivo in os.listdir(carpeta):\n",
        "        if archivo.endswith(\".parquet\"):\n",
        "            nombre_dataset = archivo.replace(\".parquet\", \"\")\n",
        "            ruta = os.path.join(carpeta, archivo)\n",
        "            try:\n",
        "                print(f\"üì• Cargando: {nombre_dataset}\")\n",
        "                df = pd.read_parquet(ruta)\n",
        "                datasets[nombre_dataset] = df\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error al cargar {nombre_dataset}: {e}\")\n",
        "    return datasets\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar los datasets normalizados desde parquet\n",
        "normalized_datasets = cargar_datasets_parquet(\"datasets_limpios\")\n",
        "# Verificar la carga\n",
        "for name, df in normalized_datasets.items():\n",
        "    print(f\"{name}: {df.shape[0]:,} filas, {df.shape[1]} columnas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øExisten valores aparentemente incorrectos?\n",
        "Analiza todos los DataFrames en un diccionario (como normalized_datasets). La funci√≥n eval√∫a los mismos valores inv√°lidos y genera un resumen por dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificaci√≥n de calidad de columnas\n",
        "def analizar_valores_invalidos_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Analiza valores inv√°lidos en un diccionario de datasets (DataFrames).\n",
        "\n",
        "    Devuelve:\n",
        "        Un DataFrame resumen con el conteo de errores por tipo y dataset.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        resultados = {}\n",
        "        print(f\"üîç Analizando: {nombre}\")\n",
        "\n",
        "        # Detectar columnas relevantes\n",
        "        def get_first_existing(group):\n",
        "            return next((col for col in group if col in df.columns), None)\n",
        "\n",
        "        col_passengers = 'passenger_count' if 'passenger_count' in df.columns else None\n",
        "        col_distance = 'trip_distance' if 'trip_distance' in df.columns else None\n",
        "        col_total = 'total_amount' if 'total_amount' in df.columns else None\n",
        "        col_pu = get_first_existing(column_groups['PULocationID'])\n",
        "        col_do = get_first_existing(column_groups['DOLocationID'])\n",
        "        col_pickup = get_first_existing(column_groups['pickup_datetime'])\n",
        "        col_dropoff = get_first_existing(column_groups['dropoff_datetime'])\n",
        "\n",
        "        # 1. Pasajeros\n",
        "        if col_passengers:\n",
        "            resultados['Pasajeros = nulos'] = df[col_passengers].isna().sum()\n",
        "            resultados['Pasajeros < 0'] = (df[col_passengers] < 0).sum()\n",
        "\n",
        "        # 2. Distancia\n",
        "        if col_distance:\n",
        "            resultados['Distancia = nula'] = df[col_distance].isna().sum()\n",
        "            resultados['Distancia <= 0'] = (df[col_distance] <= 0).sum()\n",
        "\n",
        "        # 3 y 4. Duraci√≥n inv√°lida\n",
        "        if col_pickup and col_dropoff:\n",
        "            pickup = pd.to_datetime(df[col_pickup], errors='coerce')\n",
        "            dropoff = pd.to_datetime(df[col_dropoff], errors='coerce')\n",
        "            resultados['Pickup/dropoff = nulos'] = pickup.isna().sum() + dropoff.isna().sum()\n",
        "            resultados['Dropoff < Pickup'] = (dropoff < pickup).sum()\n",
        "            resultados['Duraci√≥n = 0 min'] = (dropoff == pickup).sum()\n",
        "\n",
        "        # 5. Total\n",
        "        if col_total:\n",
        "            resultados['Total_amount = nulo'] = df[col_total].isna().sum()\n",
        "            resultados['Total_amount <= 0'] = (df[col_total] <= 0).sum()\n",
        "\n",
        "        # 6. Ubicaciones\n",
        "        if col_pu:\n",
        "            resultados['PULocationID = nulo'] = df[col_pu].isna().sum()\n",
        "            resultados['PULocationID fuera de rango'] = df[~df[col_pu].between(1, 263)].shape[0]\n",
        "        if col_do:\n",
        "            resultados['DOLocationID = nulo'] = df[col_do].isna().sum()\n",
        "            resultados['DOLocationID fuera de rango'] = df[~df[col_do].between(1, 263)].shape[0]\n",
        "\n",
        "        # Agregar los resultados al resumen\n",
        "        for k, v in resultados.items():\n",
        "            resumen.append({\"Dataset\": nombre, \"Tipo de error\": k, \"N¬∫ de registros\": v})\n",
        "\n",
        "    return pd.DataFrame(resumen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### An√°lisis de la calidad de los valores limpiados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificaci√≥n de calidad de columnas\n",
        "df_errores = analizar_valores_invalidos_dict(normalized_datasets)\n",
        "display(df_errores)\n",
        "\n",
        "# Verificaci√≥n de calidad de columnas\n",
        "\"\"\" df_original_errores = analizar_valores_invalidos_dict(datasets)\n",
        "display(df_original_errores) \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gr√°fico combinado para representar de forma clara y visual los outliers en las variables m√°s propensas a errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comparar_outliers_multivariables(original_dict, limpio_dict):\n",
        "    \"\"\"\n",
        "    Compara histogramas de distancia, duraci√≥n y total_amount para datasets originales y limpios.\n",
        "    Omite datasets sin las columnas requeridas.\n",
        "    \"\"\"\n",
        "    for nombre in original_dict:\n",
        "        if nombre not in limpio_dict:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no est√° en el conjunto limpio. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df_orig = original_dict[nombre].copy()\n",
        "        df_limpio = limpio_dict[nombre].copy()\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if \"trip_distance\" not in df_orig.columns or \"trip_distance\" not in df_limpio.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no tiene 'trip_distance'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        if \"trip_duration_min\" not in df_orig.columns:\n",
        "            pickup_cols = [c for c in ['tpep_pickup_datetime', 'lpep_pickup_datetime', 'pickup_datetime'] if c in df_orig.columns]\n",
        "            dropoff_cols = [c for c in ['tpep_dropoff_datetime', 'lpep_dropoff_datetime', 'dropoff_datetime', 'dropOff_datetime'] if c in df_orig.columns]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df_orig[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df_orig[dropoff_cols[0]], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df_orig[pickup_cols[0]], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "\n",
        "        if \"trip_duration_min\" not in df_limpio.columns and \"trip_duration_min\" in df_orig.columns:\n",
        "            df_limpio[\"trip_duration_min\"] = df_orig[\"trip_duration_min\"]\n",
        "\n",
        "        if \"trip_duration_min\" not in df_orig.columns or \"trip_duration_min\" not in df_limpio.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no tiene 'trip_duration_min'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        if \"total_amount\" not in df_orig.columns or \"total_amount\" not in df_limpio.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no tiene 'total_amount'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Filtrado para visualizaci√≥n\n",
        "        df_orig = df_orig[\n",
        "            (df_orig[\"trip_distance\"] < 20) &\n",
        "            (df_orig[\"trip_duration_min\"] < 60) &\n",
        "            (df_orig[\"total_amount\"] < 100)\n",
        "        ]\n",
        "        df_limpio = df_limpio[\n",
        "            (df_limpio[\"trip_distance\"] < 20) &\n",
        "            (df_limpio[\"trip_duration_min\"] < 60) &\n",
        "            (df_limpio[\"total_amount\"] < 100)\n",
        "        ]\n",
        "\n",
        "        # Visualizaci√≥n\n",
        "        fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
        "        fig.suptitle(f\"Distribuci√≥n de trayectos ‚Äì {nombre}\", fontsize=16)\n",
        "\n",
        "        # Distancia\n",
        "        sns.histplot(df_orig[\"trip_distance\"], bins=60, color=\"lightcoral\", label=\"Original\", ax=axs[0], alpha=0.6)\n",
        "        sns.histplot(df_limpio[\"trip_distance\"], bins=60, color=\"seagreen\", label=\"Limpio\", ax=axs[0], alpha=0.6)\n",
        "        axs[0].set_title(\"Histograma de distancias (<20 mi)\")\n",
        "        axs[0].legend()\n",
        "\n",
        "        # Duraci√≥n\n",
        "        sns.histplot(df_orig[\"trip_duration_min\"], bins=60, color=\"lightcoral\", label=\"Original\", ax=axs[1], alpha=0.6)\n",
        "        sns.histplot(df_limpio[\"trip_duration_min\"], bins=60, color=\"seagreen\", label=\"Limpio\", ax=axs[1], alpha=0.6)\n",
        "        axs[1].set_title(\"Histograma de duraci√≥n (<60 min)\")\n",
        "        axs[1].legend()\n",
        "\n",
        "        # Total amount\n",
        "        sns.histplot(df_orig[\"total_amount\"], bins=60, color=\"lightcoral\", label=\"Original\", ax=axs[2], alpha=0.6)\n",
        "        sns.histplot(df_limpio[\"total_amount\"], bins=60, color=\"seagreen\", label=\"Limpio\", ax=axs[2], alpha=0.6)\n",
        "        axs[2].set_title(\"Histograma de importe total (<100 $)\")\n",
        "        axs[2].legend()\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cargar datasets originales desde parquet local (si hiciera falta, antes se liber√≥ la memoria, eliminando datasets originales)\n",
        "datasets = cargar_datasets_parquet(\"datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mostrar si datasets originales y limpios tienen datos\n",
        "for name, df in datasets.items():\n",
        "    print(f\"{name}: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
        "for name, df in normalized_datasets.items():\n",
        "    print(f\"{name}: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra de comparaci√≥n de outliers\n",
        "comparar_outliers_multivariables(datasets, normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Empezamos por visualizar el dataset. Haced un plot en el que, para cada dataset, se muestren el n√∫mero de trayectos entre zonas de forma que en el eje horizontal est√©n las zonas de recogida y en el eje vertical las zonas de llegada. Extrae conclusiones preliminares. ¬øSe aprecian diferencias entre los puntos de los Yellow Cabs y los de los Green Cabs? ¬øSe aprecian diferencias entre un mes y otro?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizaci√≥n de trayectos entre las 20 zonas m√°s activas\n",
        "- Recorre ambos diccionarios de datasets (originales y limpios).\n",
        "- Genera un heatmap para cada uno.\n",
        "- Quita los n√∫meros dentro de las celdas (annot=False) para que se infieran por escala de color.\n",
        "- Usa plotly.express.imshow para interactividad: permite ver el valor al pasar el rat√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comparar_heatmaps_zonas_activas(original_dict, limpio_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Compara visualmente los heatmaps de trayectos entre zonas m√°s activas\n",
        "    entre datasets originales y limpios usando interactividad.\n",
        "\n",
        "    Par√°metros:\n",
        "    - original_dict: diccionario con datasets originales\n",
        "    - limpio_dict: diccionario con datasets limpios\n",
        "    - top_n: n√∫mero de zonas m√°s activas a considerar\n",
        "    \"\"\"\n",
        "    for nombre in original_dict:\n",
        "        if nombre not in limpio_dict:\n",
        "            continue\n",
        "\n",
        "        df_orig = original_dict[nombre]\n",
        "        df_limpio = limpio_dict[nombre]\n",
        "\n",
        "        for df, version in zip([df_orig, df_limpio], [\"Original\", \"Limpio\"]):\n",
        "            # Validar columnas necesarias\n",
        "            if not {'PULocationID', 'DOLocationID'}.issubset(df.columns):\n",
        "                print(f\"‚ö†Ô∏è {nombre} ({version}): columnas de ubicaci√≥n no encontradas. Se omite.\")\n",
        "                continue\n",
        "\n",
        "            # Agrupar trayectos\n",
        "            zonas = df.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
        "\n",
        "            # Top zonas m√°s frecuentes\n",
        "            top_pu = zonas.groupby('PULocationID')['count'].sum().nlargest(top_n).index\n",
        "            top_do = zonas.groupby('DOLocationID')['count'].sum().nlargest(top_n).index\n",
        "\n",
        "            # Filtrar top zonas\n",
        "            zonas_top = zonas[\n",
        "                zonas['PULocationID'].isin(top_pu) & zonas['DOLocationID'].isin(top_do)\n",
        "            ]\n",
        "\n",
        "            # Pivotear para heatmap\n",
        "            pivot = zonas_top.pivot(index='DOLocationID', columns='PULocationID', values='count').fillna(0)\n",
        "\n",
        "            # Plot interactivo con plotly\n",
        "            fig = px.imshow(\n",
        "                pivot.values,\n",
        "                labels=dict(x=\"PULocationID\", y=\"DOLocationID\", color=\"N¬∫ de Trayectos\"),\n",
        "                x=pivot.columns,\n",
        "                y=pivot.index,\n",
        "                color_continuous_scale=\"YlGnBu\",\n",
        "                title=f\"Trayectos entre zonas m√°s activas ‚Äì {nombre} ({version})\"\n",
        "            )\n",
        "            fig.update_layout(height=600, width=700)\n",
        "            fig.show()\n",
        "\n",
        "comparar_heatmaps_zonas_activas(datasets, normalized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para generar un diagrama de Sankey con las 5 zonas m√°s populares\n",
        "\n",
        "def plot_sankey_top5_dict(datasets_dict, zona_nombres):\n",
        "    \"\"\"\n",
        "    Genera un diagrama de Sankey para cada dataset en el diccionario,\n",
        "    mostrando los trayectos entre las 5 zonas m√°s frecuentes (origen/destino).\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    - zona_nombres: diccionario {LocationID: Nombre de zona}\n",
        "\n",
        "    Devuelve:\n",
        "    - lista de figuras Sankey (una por dataset)\n",
        "    \"\"\"\n",
        "    import plotly.graph_objects as go\n",
        "\n",
        "    figuras = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        if \"PULocationID\" not in df.columns or \"DOLocationID\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: no contiene columnas de ubicaci√≥n. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        zonas = df.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
        "\n",
        "        # Top 5 zonas m√°s activas combinando origen y destino\n",
        "        zonas_frecuentes = pd.concat([\n",
        "            zonas.groupby('PULocationID')['count'].sum(),\n",
        "            zonas.groupby('DOLocationID')['count'].sum()\n",
        "        ]).groupby(level=0).sum().nlargest(5).index.tolist()\n",
        "\n",
        "        zonas_top5 = zonas[\n",
        "            zonas['PULocationID'].isin(zonas_frecuentes) &\n",
        "            zonas['DOLocationID'].isin(zonas_frecuentes)\n",
        "        ]\n",
        "\n",
        "        if zonas_top5.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: no hay suficientes trayectos entre las 5 zonas m√°s frecuentes.\")\n",
        "            continue\n",
        "\n",
        "        etiquetas = [f\"{z} - {zona_nombres.get(z, 'Desconocido')}\" for z in zonas_frecuentes]\n",
        "        zona_idx = {zona: i for i, zona in enumerate(zonas_frecuentes)}\n",
        "\n",
        "        source = zonas_top5['PULocationID'].map(zona_idx).tolist()\n",
        "        target = zonas_top5['DOLocationID'].map(zona_idx).tolist()\n",
        "        value = zonas_top5['count'].tolist()\n",
        "\n",
        "        fig = go.Figure(data=[go.Sankey(\n",
        "            node=dict(\n",
        "                pad=15, thickness=20,\n",
        "                line=dict(color=\"black\", width=0.5),\n",
        "                label=etiquetas\n",
        "            ),\n",
        "            link=dict(source=source, target=target, value=value)\n",
        "        )])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text=f\"Trayectos entre las 5 zonas m√°s populares ‚Äì {nombre}\",\n",
        "            font_size=12\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "        figuras.append(fig)\n",
        "\n",
        "    return figuras\n",
        "\n",
        "# Generamos diagramas de Sankey para los datasets de Febrero 2025 y Diciembre 2024 limpios\n",
        "# Asumiendo que zona_nombres ya est√° cargado\n",
        "plot_sankey_top5_dict(normalized_datasets, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4\tMejora la visualizaci√≥n anterior con un heat map. Ay√∫date de los Zone Map‚Ä¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_heatmap_top5_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Genera un heatmap con trayectos entre las 5 zonas m√°s populares para cada dataset del diccionario.\n",
        "\n",
        "    Se espera que las claves del diccionario sigan el formato: tipo_tripdata_aaaa-mm\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    \"\"\"\n",
        "    for key, df in datasets_dict.items():\n",
        "        # Parsear tipo de taxi y fecha desde el nombre del archivo\n",
        "        try:\n",
        "            tipo, _, fecha = key.split(\"_\")\n",
        "            a√±o, mes = fecha.split(\"-\")\n",
        "            mes_label = f\"{mes}/{a√±o}\"\n",
        "            nombre_dataset = tipo.capitalize() + \" Cabs\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå No se pudo interpretar '{key}': {e}\")\n",
        "            continue\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if not all(col in df.columns for col in [\"PULocationID\", \"DOLocationID\"]):\n",
        "            print(f\"‚ö†Ô∏è {key}: columnas de ubicaci√≥n no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Agrupar trayectos\n",
        "        zonas = df.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
        "\n",
        "        # Zonas m√°s activas (top 5 combinando origen y destino)\n",
        "        zonas_frecuentes = pd.concat([\n",
        "            zonas.groupby('PULocationID')['count'].sum(),\n",
        "            zonas.groupby('DOLocationID')['count'].sum()\n",
        "        ]).groupby(level=0).sum().nlargest(5).index.tolist()\n",
        "\n",
        "        # Filtrar trayectos entre esas zonas\n",
        "        zonas_top5 = zonas[\n",
        "            zonas['PULocationID'].isin(zonas_frecuentes) &\n",
        "            zonas['DOLocationID'].isin(zonas_frecuentes)\n",
        "        ].copy()\n",
        "\n",
        "        # Mapeo a nombres de zonas\n",
        "        zonas_top5[\"Zona origen\"] = zonas_top5[\"PULocationID\"].map(zona_nombres)\n",
        "        zonas_top5[\"Zona destino\"] = zonas_top5[\"DOLocationID\"].map(zona_nombres)\n",
        "\n",
        "        # Crear clave de combinaci√≥n\n",
        "        zonas_top5[\"Zona\"] = zonas_top5[\"Zona origen\"] + \" ‚Üí \" + zonas_top5[\"Zona destino\"]\n",
        "        zonas_top5[\"Mes-Tipo\"] = f\"{mes_label} ({nombre_dataset})\"\n",
        "\n",
        "        # Pivot para heatmap\n",
        "        pivot_df = zonas_top5.pivot(index=\"Zona\", columns=\"Mes-Tipo\", values=\"count\").fillna(0)\n",
        "\n",
        "        # Dibujar heatmap\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(pivot_df, annot=True, fmt=\".0f\", cmap=\"YlGnBu\", cbar_kws={'label': 'N√∫mero de trayectos'})\n",
        "        plt.title(f\"Heatmap ‚Äì Top 5 zonas m√°s populares ({mes_label}, {nombre_dataset})\")\n",
        "        plt.ylabel(\"Zona origen ‚Üí destino\")\n",
        "        plt.xlabel(\"Mes y tipo de taxi\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generamos heatmaps para los datasets Limpios\n",
        "plot_heatmap_top5_dict(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Segunda Parte: An√°lisis Cualitativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1\t¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s alta? ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s alta? ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s alta?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Opcional) si hace falta se puede cargar el normalized_dataset original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalized_datasets\n",
        "normalized_datasets = cargar_datasets_parquet(\"datasets_limpios\")\n",
        "# Verificar la carga\n",
        "for name, df in normalized_datasets.items():\n",
        "    print(f\"{name}: {df.shape[0]:,} filas, {df.shape[1]} columnas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identificador de Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identificar_outliers_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Detecta outliers multivariables por IQR en cada dataset de un diccionario.\n",
        "    Eval√∫a columnas como distancia, importe, duraci√≥n y precio/km.\n",
        "\n",
        "    Devuelve:\n",
        "        dict con {nombre_dataset: DataFrame de outliers}\n",
        "    \"\"\"\n",
        "    outliers_dict = {}\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "        print(f\"\\nüìä Procesando outliers en: {nombre}\")\n",
        "\n",
        "        # Calcular duraci√≥n si no existe\n",
        "        if \"trip_duration_min\" not in df.columns:\n",
        "            pickup_cols = [c for c in ['tpep_pickup_datetime', 'lpep_pickup_datetime', 'pickup_datetime'] if c in df.columns]\n",
        "            dropoff_cols = [c for c in ['tpep_dropoff_datetime', 'lpep_dropoff_datetime', 'dropoff_datetime', 'dropOff_datetime'] if c in df.columns]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[dropoff_cols[0]], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df[pickup_cols[0]], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "\n",
        "        # Calcular fare_per_km si procede\n",
        "        if 'fare_amount' in df.columns and 'trip_distance' in df.columns:\n",
        "            df = df[df['trip_distance'] > 0].copy()\n",
        "            df['fare_per_km'] = df['fare_amount'] / df['trip_distance']\n",
        "\n",
        "        # Columnas a evaluar\n",
        "        columnas = ['trip_distance', 'fare_amount', 'trip_duration_min', 'fare_per_km']\n",
        "        columnas = [col for col in columnas if col in df.columns]\n",
        "\n",
        "        outlier_flags = pd.DataFrame(index=df.index)\n",
        "        resumen = []\n",
        "\n",
        "        for col in columnas:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lim_inf = Q1 - 1.5 * IQR\n",
        "            lim_sup = Q3 + 1.5 * IQR\n",
        "\n",
        "            flag = (df[col] < lim_inf) | (df[col] > lim_sup)\n",
        "            outlier_flags[col + '_outlier'] = flag\n",
        "            resumen.append((col, flag.sum(), lim_inf, lim_sup))\n",
        "\n",
        "        outlier_rows = df[outlier_flags.any(axis=1)].copy()\n",
        "        if not outlier_rows.empty:\n",
        "            outlier_rows['Outlier en'] = outlier_flags[outlier_flags.any(axis=1)].apply(\n",
        "                lambda x: ', '.join(x.index[x]), axis=1\n",
        "            )\n",
        "            outliers_dict[nombre] = outlier_rows\n",
        "\n",
        "        # Mostrar resumen\n",
        "        for col, count, low, high in resumen:\n",
        "            print(f\"  ‚ñ™ {col}: {count} outliers (fuera de [{low:.2f}, {high:.2f}])\")\n",
        "\n",
        "    return outliers_dict\n",
        "# Identificar outliers en los datasets limpios\n",
        "outliers_detectados = identificar_outliers_dict(normalized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_datasets['yellow_tripdata_2009-02'].columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Muestra un resumen por DataSet de registros con incoherencias entre distancia y precio. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_incoherencias_distancia_precio(datasets_dict, z_thresh=3.0, min_registros=100):\n",
        "    \"\"\"\n",
        "    Analiza incoherencias entre trip_distance y fare_amount usando regresi√≥n lineal.\n",
        "    No modifica los datasets. Solo informa.\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario {nombre: DataFrame}\n",
        "    - z_thresh: umbral de Z-score para considerar un registro incoherente\n",
        "    - min_registros: m√≠nimo de registros para aplicar regresi√≥n\n",
        "\n",
        "    Muestra un resumen con el n√∫mero de incoherencias detectadas por dataset.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Validar columnas necesarias\n",
        "        if \"trip_distance\" not in df.columns or \"fare_amount\" not in df.columns:\n",
        "            print(f\"‚ùå {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Filtrar registros v√°lidos\n",
        "        df_validos = df[\n",
        "            df[\"trip_distance\"].notnull() &\n",
        "            df[\"fare_amount\"].notnull() &\n",
        "            (df[\"trip_distance\"] > 0) &\n",
        "            (df[\"fare_amount\"] > 0)\n",
        "        ]\n",
        "\n",
        "        if len(df_validos) < min_registros:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: solo {len(df_validos)} registros v√°lidos. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Regresi√≥n lineal\n",
        "        X = df_validos[[\"trip_distance\"]].values\n",
        "        y = df_validos[\"fare_amount\"].values\n",
        "        modelo = LinearRegression()\n",
        "        modelo.fit(X, y)\n",
        "\n",
        "        # Calcular residuos y z-score\n",
        "        y_pred = modelo.predict(X)\n",
        "        residuos = y - y_pred\n",
        "        z_residuos = zscore(residuos)\n",
        "\n",
        "        # Identificar incoherencias\n",
        "        incoherentes = (abs(z_residuos) > z_thresh).sum()\n",
        "        total = len(df_validos)\n",
        "        porcentaje = 100 * incoherentes / total\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Registros v√°lidos\": total,\n",
        "            \"Incoherencias (> z_thresh)\": incoherentes,\n",
        "            \"% Incoherente\": f\"{porcentaje:.2f}%\"\n",
        "        })\n",
        "\n",
        "    # Mostrar resumen\n",
        "    print(\"\\nüìä Resumen de incoherencias entre distancia y precio (regresi√≥n):\")\n",
        "    display(pd.DataFrame(resumen))\n",
        "\n",
        "# Ejemplo de uso\n",
        "analizar_incoherencias_distancia_precio(normalized_datasets, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imputar la media local/global en las incoherencias entre distancia y precio detectadas por regresi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imputaci√≥n de valores an√≥malos de fare_amount por regresi√≥n lineal\n",
        "def imputar_fare_anomalo_por_regresion(datasets_dict, z_thresh=3.0, min_similares=5):\n",
        "    \"\"\"\n",
        "    Imputa valores incoherentes en 'fare_amount' usando regresi√≥n lineal contra 'trip_distance'.\n",
        "    La imputaci√≥n se realiza s√≥lo para registros v√°lidos que sean outliers:\n",
        "    - Primero intenta usar la media local por trayecto (PU-DO).\n",
        "    - Si no hay suficientes trayectos, usa la media global.\n",
        "    \"\"\"\n",
        "\n",
        "    corregidos = {}\n",
        "    resumen = []\n",
        "\n",
        "    for nombre, df_original in datasets_dict.items():\n",
        "        if not all(col in df_original.columns for col in [\"trip_distance\", \"fare_amount\", \"PULocationID\", \"DOLocationID\"]):\n",
        "            print(f\"‚ùå {nombre}: columnas necesarias no encontradas. Se omite imputaci√≥n.\")\n",
        "            corregidos[nombre] = df_original.copy()\n",
        "            continue\n",
        "\n",
        "\n",
        "        print(f\"üîç Procesando: {nombre}\")\n",
        "        df = df_original.copy()\n",
        "        df[\"fare_imputed_by_regression\"] = False\n",
        "\n",
        "        # Filtrar registros v√°lidos\n",
        "        valid_mask = (df[\"trip_distance\"] > 0) & (df[\"fare_amount\"] > 0)\n",
        "        df_valid = df[valid_mask].copy()\n",
        "\n",
        "        if len(df_valid) < 1000:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: muy pocos registros v√°lidos. Se mantiene sin imputaci√≥n.\")\n",
        "            corregidos[nombre] = df\n",
        "            continue\n",
        "\n",
        "        # Entrenar regresi√≥n\n",
        "        X = df_valid[[\"trip_distance\"]].values\n",
        "        y = df_valid[\"fare_amount\"].values\n",
        "        modelo = LinearRegression()\n",
        "        modelo.fit(X, y)\n",
        "        y_pred = modelo.predict(X)\n",
        "\n",
        "        # Detectar outliers por residuos\n",
        "        residuos = y - y_pred\n",
        "        z_residuos = zscore(residuos)\n",
        "        df_valid[\"fare_residuo_z\"] = z_residuos\n",
        "        outliers = df_valid[abs(z_residuos) > z_thresh]\n",
        "\n",
        "        if outliers.empty:\n",
        "            print(f\"‚ÑπÔ∏è {nombre}: sin outliers detectados. No se requiere imputaci√≥n.\")\n",
        "            corregidos[nombre] = df\n",
        "            continue\n",
        "\n",
        "        # Calcular medias locales\n",
        "        normales = df_valid[abs(z_residuos) <= z_thresh]\n",
        "        conteo = normales.groupby([\"PULocationID\", \"DOLocationID\"]).size().reset_index(name=\"count\")\n",
        "        frecuentes = conteo[conteo[\"count\"] >= min_similares][[\"PULocationID\", \"DOLocationID\"]]\n",
        "        medias_locales = (\n",
        "            normales.groupby([\"PULocationID\", \"DOLocationID\"])[\"fare_amount\"]\n",
        "            .mean()\n",
        "            .reset_index()\n",
        "            .rename(columns={\"fare_amount\": \"fare_local\"})\n",
        "            .merge(frecuentes, on=[\"PULocationID\", \"DOLocationID\"])\n",
        "        )\n",
        "\n",
        "        media_global = normales[\"fare_amount\"].mean()\n",
        "\n",
        "        # Reemplazo de valores\n",
        "        imputados = outliers.merge(medias_locales, on=[\"PULocationID\", \"DOLocationID\"], how=\"left\")\n",
        "        imputados[\"fare_amount\"] = imputados[\"fare_local\"].fillna(media_global)\n",
        "        imputados[\"fare_imputed_by_regression\"] = True\n",
        "\n",
        "        # Aplicar imputaciones al dataframe original\n",
        "        df.update(imputados[[\"fare_amount\", \"fare_imputed_by_regression\"]])\n",
        "\n",
        "        # Guardar resultados\n",
        "        corregidos[nombre] = df\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Registros v√°lidos\": len(df_valid),\n",
        "            \"Outliers\": len(imputados),\n",
        "            \"Imputados local\": imputados[\"fare_local\"].notna().sum(),\n",
        "            \"Imputados global\": imputados[\"fare_local\"].isna().sum()\n",
        "        })\n",
        "\n",
        "    if resumen:\n",
        "        resumen_df = pd.DataFrame(resumen)\n",
        "        resumen_df[\"% Imputados\"] = ((resumen_df[\"Outliers\"]) / resumen_df[\"Registros v√°lidos\"] * 100).round(2)\n",
        "        print(\"\\nüìä Resumen de imputaciones (fare vs distance):\")\n",
        "        display(resumen_df)\n",
        "\n",
        "    return corregidos\n",
        "\n",
        "# Ejecutar funci√≥n sobre normalized_datasets\n",
        "normalized_datasets = imputar_fare_anomalo_por_regresion(normalized_datasets,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprobaci√≥n tras Imputaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analizar_incoherencias_distancia_precio(normalized_datasets,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trayecto con mayor precio por km"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_mayor_precio_por_km(datasets_dict):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con la mayor relaci√≥n precio/km para cada dataset\n",
        "    y tambi√©n a nivel global, usando nombres reales de zona.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict (dict): {nombre_dataset: DataFrame}\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame con los trayectos con mayor precio/km por dataset y global.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        if not all(col in df.columns for col in [\"trip_distance\", \"fare_amount\", \"PULocationID\", \"DOLocationID\"]):\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[(df[\"trip_distance\"] > 0) & (df[\"fare_amount\"] > 0)].copy()\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos para calcular precio/km.\")\n",
        "            continue\n",
        "\n",
        "        df[\"fare_per_km\"] = df[\"fare_amount\"] / df[\"trip_distance\"]\n",
        "        top = df.loc[df[\"fare_per_km\"].idxmax()]\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{int(top['PULocationID'])} - {zona_nombres.get(top['PULocationID'], 'Desconocido')}\",\n",
        "            \"Destino\": f\"{int(top['DOLocationID'])} - {zona_nombres.get(top['DOLocationID'], 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Precio ($)\": top[\"fare_amount\"],\n",
        "            \"Precio/km ($)\": top[\"fare_per_km\"]\n",
        "        })\n",
        "\n",
        "        if global_max is None or top[\"fare_per_km\"] > global_max[\"fare_per_km\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_max is not None:\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{int(global_max['PULocationID'])} - {zona_nombres.get(global_max['PULocationID'], 'Desconocido')}\",\n",
        "            \"Destino\": f\"{int(global_max['DOLocationID'])} - {zona_nombres.get(global_max['DOLocationID'], 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": global_max[\"trip_distance\"],\n",
        "            \"Precio ($)\": global_max[\"fare_amount\"],\n",
        "            \"Precio/km ($)\": global_max[\"fare_per_km\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con mayor precio/km\n",
        "trayecto_mayor_precio_por_km(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trayecto con la relaci√≥n tiempo/km m√°s alta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_mayor_duracion_por_km(datasets_dict):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con mayor duraci√≥n por kil√≥metro para cada dataset\n",
        "    y tambi√©n a nivel global, incluyendo nombres de zona.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame resumen con los trayectos m√°s lentos (min/km)\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Detectar columnas de datetime\n",
        "        pickup_cols = [col for col in df.columns if 'pickup' in col and 'datetime' in col]\n",
        "        dropoff_cols = [col for col in df.columns if 'drop' in col and 'datetime' in col]\n",
        "\n",
        "        if not pickup_cols or not dropoff_cols:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas de datetime no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        pickup = pd.to_datetime(df[pickup_cols[0]], errors='coerce')\n",
        "        dropoff = pd.to_datetime(df[dropoff_cols[0]], errors='coerce')\n",
        "        df[\"duration_min\"] = (dropoff - pickup).dt.total_seconds() / 60\n",
        "\n",
        "        if \"trip_distance\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columna 'trip_distance' no encontrada. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[(df[\"trip_distance\"] > 0) & (df[\"duration_min\"] > 0)].copy()\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos tras filtrado. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df[\"min_per_km\"] = df[\"duration_min\"] / df[\"trip_distance\"]\n",
        "\n",
        "        top = df.loc[df[\"min_per_km\"].idxmax()]\n",
        "        origen_id = int(top[\"PULocationID\"])\n",
        "        destino_id = int(top[\"DOLocationID\"])\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": top[\"duration_min\"],\n",
        "            \"Min/km\": top[\"min_per_km\"]\n",
        "        })\n",
        "\n",
        "        if global_max is None or top[\"min_per_km\"] > global_max[\"min_per_km\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_max is not None:\n",
        "        origen_id = int(global_max[\"PULocationID\"])\n",
        "        destino_id = int(global_max[\"DOLocationID\"])\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": global_max[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": global_max[\"duration_min\"],\n",
        "            \"Min/km\": global_max[\"min_per_km\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con mayor duraci√≥n por kil√≥metro\n",
        "trayecto_mayor_duracion_por_km(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trayecto con relaci√≥n precio/tiempo m√°s alta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_mayor_precio_por_minuto(datasets_dict):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con mayor relaci√≥n precio/tiempo para cada dataset\n",
        "    (es decir, el mayor coste por minuto), e identifica tambi√©n el global.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Detectar columnas de tiempo\n",
        "        pickup_cols = [col for col in df.columns if 'pickup' in col and 'datetime' in col]\n",
        "        dropoff_cols = [col for col in df.columns if 'drop' in col and 'datetime' in col]\n",
        "        if not pickup_cols or not dropoff_cols:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas de tiempo no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Calcular duraci√≥n en minutos\n",
        "        pickup = pd.to_datetime(df[pickup_cols[0]], errors='coerce')\n",
        "        dropoff = pd.to_datetime(df[dropoff_cols[0]], errors='coerce')\n",
        "        df[\"duration_min\"] = (dropoff - pickup).dt.total_seconds() / 60\n",
        "\n",
        "        if \"fare_amount\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columna 'fare_amount' no encontrada. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Filtrar registros v√°lidos\n",
        "        df = df[(df[\"duration_min\"] > 0) & (df[\"fare_amount\"] > 0)].copy()\n",
        "\n",
        "        # Calcular $ por minuto\n",
        "        df[\"usd_per_min\"] = df[\"fare_amount\"] / df[\"duration_min\"]\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        top = df.loc[df[\"usd_per_min\"].idxmax()]\n",
        "\n",
        "        # Manejo seguro de columnas de localizaci√≥n\n",
        "        origen_id = int(top[\"PULocationID\"]) if \"PULocationID\" in top else None\n",
        "        destino_id = int(top[\"DOLocationID\"]) if \"DOLocationID\" in top else None\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\" if origen_id else \"N/A\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\" if destino_id else \"N/A\",\n",
        "            \"Duraci√≥n (min)\": top[\"duration_min\"],\n",
        "            \"Precio ($)\": top[\"fare_amount\"],\n",
        "            \"$/min\": top[\"usd_per_min\"]\n",
        "        })\n",
        "\n",
        "        if global_max is None or top[\"usd_per_min\"] > global_max[\"usd_per_min\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "\n",
        "    # Agregar global\n",
        "    if global_max is not None:\n",
        "        origen_id = int(global_max[\"PULocationID\"]) if \"PULocationID\" in global_max else None\n",
        "        destino_id = int(global_max[\"DOLocationID\"]) if \"DOLocationID\" in global_max else None\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\" if origen_id else \"N/A\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\" if destino_id else \"N/A\",\n",
        "            \"Duraci√≥n (min)\": global_max[\"duration_min\"],\n",
        "            \"Precio ($)\": global_max[\"fare_amount\"],\n",
        "            \"$/min\": global_max[\"usd_per_min\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con mayor precio por minuto\n",
        "trayecto_mayor_precio_por_minuto(normalized_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2\t¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s baja? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_precio_por_km(datasets_dict, min_distance=0.1):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con menor relaci√≥n precio/km para cada dataset,\n",
        "    ignorando trayectos con distancia menor al umbral especificado.\n",
        "    Tambi√©n encuentra el trayecto global con la relaci√≥n m√°s baja.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre: DataFrame}\n",
        "        min_distance: distancia m√≠nima (en millas) para considerar un trayecto\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame con los trayectos m√°s baratos por km\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    global_min = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "        if not all(col in df.columns for col in [\"trip_distance\", \"fare_amount\", \"PULocationID\", \"DOLocationID\"]):\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[\n",
        "            (df[\"trip_distance\"] >= min_distance) &\n",
        "            (df[\"fare_amount\"] > 0)\n",
        "        ].copy()\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        df[\"fare_per_km\"] = df[\"fare_amount\"] / df[\"trip_distance\"]\n",
        "\n",
        "        top = df.loc[df[\"fare_per_km\"].idxmin()]\n",
        "        origen_id = int(top[\"PULocationID\"])\n",
        "        destino_id = int(top[\"DOLocationID\"])\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Precio ($)\": top[\"fare_amount\"],\n",
        "            \"Precio/km ($)\": top[\"fare_per_km\"]\n",
        "        })\n",
        "\n",
        "        if global_min is None or top[\"fare_per_km\"] < global_min[\"fare_per_km\"]:\n",
        "            global_min = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_min is not None:\n",
        "        origen_id = int(global_min[\"PULocationID\"])\n",
        "        destino_id = int(global_min[\"DOLocationID\"])\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": global_min[\"trip_distance\"],\n",
        "            \"Precio ($)\": global_min[\"fare_amount\"],\n",
        "            \"Precio/km ($)\": global_min[\"fare_per_km\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor precio/km\n",
        "trayecto_menor_precio_por_km(normalized_datasets, min_distance=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s baja? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_tiempo_por_km(datasets_dict):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con menor relaci√≥n tiempo/km (minutos por milla) para cada dataset,\n",
        "    as√≠ como el trayecto global con la menor relaci√≥n.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame con los trayectos m√°s r√°pidos por km\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    global_min = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        if \"trip_distance\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin columna 'trip_distance'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df[\"trip_distance\"] > 0]\n",
        "\n",
        "        if \"trip_duration_min\" not in df.columns:\n",
        "            pickup_cols = [c for c in df.columns if \"pickup\" in c and \"datetime\" in c]\n",
        "            dropoff_cols = [c for c in df.columns if \"drop\" in c and \"datetime\" in c]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[dropoff_cols[0]], errors='coerce') -\n",
        "                    pd.to_datetime(df[pickup_cols[0]], errors='coerce')\n",
        "                ).dt.total_seconds() / 60\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin columnas de fecha v√°lidas. Se omite.\")\n",
        "                continue\n",
        "\n",
        "        df = df[df[\"trip_duration_min\"] > 0]\n",
        "        df[\"min_per_km\"] = df[\"trip_duration_min\"] / df[\"trip_distance\"]\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        top = df.loc[df[\"min_per_km\"].idxmin()]\n",
        "        origen_id = int(top[\"PULocationID\"])\n",
        "        destino_id = int(top[\"DOLocationID\"])\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Min/km\": top[\"min_per_km\"]\n",
        "        })\n",
        "\n",
        "        if global_min is None or top[\"min_per_km\"] < global_min[\"min_per_km\"]:\n",
        "            global_min = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_min is not None:\n",
        "        origen_id = int(global_min[\"PULocationID\"])\n",
        "        destino_id = int(global_min[\"DOLocationID\"])\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": global_min[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": global_min[\"trip_duration_min\"],\n",
        "            \"Min/km\": global_min[\"min_per_km\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor tiempo/km\n",
        "trayecto_menor_tiempo_por_km(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s baja?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_precio_por_minuto(datasets_dict):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con menor relaci√≥n precio/tiempo (USD por minuto) para cada dataset\n",
        "    y el trayecto global con la menor relaci√≥n.\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    global_min = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        if \"fare_amount\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columna 'fare_amount' no encontrada. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df[\"fare_amount\"] > 0]\n",
        "\n",
        "        # Calcular duraci√≥n si no existe\n",
        "        if \"trip_duration_min\" not in df.columns:\n",
        "            pickup_cols = [col for col in df.columns if \"pickup\" in col and \"datetime\" in col]\n",
        "            dropoff_cols = [col for col in df.columns if \"drop\" in col and \"datetime\" in col]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[dropoff_cols[0]], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df[pickup_cols[0]], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin columnas de fecha v√°lidas. Se omite.\")\n",
        "                continue\n",
        "\n",
        "        df = df[df[\"trip_duration_min\"] > 0]\n",
        "        df[\"usd_per_min\"] = df[\"fare_amount\"] / df[\"trip_duration_min\"]\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        top = df.loc[df[\"usd_per_min\"].idxmin()]\n",
        "\n",
        "        # Manejo seguro de IDs de localizaci√≥n\n",
        "        origen_id = int(top[\"PULocationID\"]) if \"PULocationID\" in top else None\n",
        "        destino_id = int(top[\"DOLocationID\"]) if \"DOLocationID\" in top else None\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\" if origen_id else \"N/A\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\" if destino_id else \"N/A\",\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": top[\"fare_amount\"],\n",
        "            \"Precio/min ($)\": top[\"usd_per_min\"]\n",
        "        })\n",
        "\n",
        "        if global_min is None or top[\"usd_per_min\"] < global_min[\"usd_per_min\"]:\n",
        "            global_min = top\n",
        "            global_name = nombre\n",
        "\n",
        "    # Trayecto global\n",
        "    if global_min is not None:\n",
        "        origen_id = int(global_min[\"PULocationID\"]) if \"PULocationID\" in global_min else None\n",
        "        destino_id = int(global_min[\"DOLocationID\"]) if \"DOLocationID\" in global_min else None\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\" if origen_id else \"N/A\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\" if destino_id else \"N/A\",\n",
        "            \"Duraci√≥n (min)\": global_min[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": global_min[\"fare_amount\"],\n",
        "            \"Precio/min ($)\": global_min[\"usd_per_min\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor precio por minuto\n",
        "trayecto_menor_precio_por_minuto(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3\tMuestra la evoluci√≥n del tiempo medio de trayecto a lo largo del d√≠a. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tiempo_medio_por_hora_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Grafica la duraci√≥n media (en minutos) del trayecto a lo largo del d√≠a para cada dataset del diccionario.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    \"\"\"\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if 'pickup_datetime' in df.columns and 'dropoff_datetime' in df.columns:\n",
        "            df['hora'] = pd.to_datetime(df['pickup_datetime'], errors='coerce').dt.hour\n",
        "            df['duracion_min'] = (\n",
        "                pd.to_datetime(df['dropoff_datetime'], errors='coerce') -\n",
        "                pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
        "            ).dt.total_seconds() / 60\n",
        "        else:\n",
        "            print(f\"‚ùå {nombre}: columnas de fecha no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df['duracion_min'] > 0]\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin datos v√°lidos para calcular duraci√≥n.\")\n",
        "            continue\n",
        "\n",
        "        media_por_hora = df.groupby('hora')['duracion_min'].mean()\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.lineplot(x=media_por_hora.index, y=media_por_hora.values, marker=\"o\", color=\"orange\")\n",
        "        plt.title(f\"Duraci√≥n media del trayecto por hora ‚Äì {nombre}\")\n",
        "        plt.xlabel(\"Hora del d√≠a\")\n",
        "        plt.ylabel(\"Duraci√≥n media (minutos)\")\n",
        "        plt.xticks(range(0, 24))\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generar gr√°ficos de duraci√≥n media por hora para los datasets limpios\n",
        "plot_tiempo_medio_por_hora_dict(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Muestra la evoluci√≥n de la distancia media de trayecto a lo largo del d√≠a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distancia_media_por_hora_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Grafica la distancia media (millas) del trayecto a lo largo del d√≠a para cada dataset del diccionario.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    \"\"\"\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if 'pickup_datetime' in df.columns and 'trip_distance' in df.columns:\n",
        "            df['hora'] = pd.to_datetime(df['pickup_datetime'], errors='coerce').dt.hour\n",
        "        else:\n",
        "            print(f\"‚ùå {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df['trip_distance'] > 0]\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin datos v√°lidos para calcular distancia.\")\n",
        "            continue\n",
        "\n",
        "        media_por_hora = df.groupby('hora')['trip_distance'].mean()\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.lineplot(x=media_por_hora.index, y=media_por_hora.values, marker=\"o\", color=\"steelblue\")\n",
        "        plt.title(f\"Distancia media del trayecto por hora ‚Äì {nombre}\")\n",
        "        plt.xlabel(\"Hora del d√≠a\")\n",
        "        plt.ylabel(\"Distancia media (millas)\")\n",
        "        plt.xticks(range(0, 24))\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generar gr√°ficos de distancia media por hora para los datasets limpios\n",
        "plot_distancia_media_por_hora_dict(normalized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4\tElige dos zonas cualesquiera de la ciudad y calcula la probabilidad de desplazarse de una zona a otra en menos de X minutos. (El valor X, as√≠ como las zonas deben ser f√°cilmente modificables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcular_probabilidad_desplazamiento_rapido_dict(datasets_dict, origen, destino, umbral_min=10):\n",
        "    \"\"\"\n",
        "    Calcula la probabilidad de que un trayecto entre dos zonas espec√≠ficas dure menos de X minutos\n",
        "    para cada dataset del diccionario.\n",
        "\n",
        "    Par√°metros:\n",
        "        - datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        - origen: ID de la zona de origen (int)\n",
        "        - destino: ID de la zona de destino (int)\n",
        "        - umbral_min: duraci√≥n m√°xima para considerar un trayecto como r√°pido (por defecto 10 minutos)\n",
        "\n",
        "    Muestra resultados por consola para cada dataset.\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        if \"pickup_datetime\" not in df.columns or \"dropoff_datetime\" not in df.columns:\n",
        "            print(f\"‚ùå {nombre}: columnas de fecha no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        if \"PULocationID\" not in df.columns or \"DOLocationID\" not in df.columns:\n",
        "            print(f\"‚ùå {nombre}: columnas de zona no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Calcular duraci√≥n en minutos\n",
        "        pickup = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
        "        dropoff = pd.to_datetime(df[\"dropoff_datetime\"], errors=\"coerce\")\n",
        "        df[\"duracion_min\"] = (dropoff - pickup).dt.total_seconds() / 60\n",
        "\n",
        "        # Filtrar trayectos v√°lidos\n",
        "        trayectos = df[\n",
        "            (df[\"PULocationID\"] == origen) &\n",
        "            (df[\"DOLocationID\"] == destino) &\n",
        "            (df[\"duracion_min\"].notnull())\n",
        "        ]\n",
        "\n",
        "        total = len(trayectos)\n",
        "        if total == 0:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin trayectos entre {origen} ‚Üí {destino}.\")\n",
        "            continue\n",
        "\n",
        "        rapidos = trayectos[trayectos[\"duracion_min\"] < umbral_min]\n",
        "        probabilidad = len(rapidos) / total\n",
        "\n",
        "        print(f\"üìä {nombre}:\")\n",
        "        print(f\"- Total trayectos {origen} ‚Üí {destino}: {total}\")\n",
        "        print(f\"- < {umbral_min} min: {len(rapidos)} ({probabilidad:.2%})\")\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Trayectos\": total,\n",
        "            \"R√°pidos (< X min)\": len(rapidos),\n",
        "            \"Probabilidad (%)\": round(probabilidad * 100, 2)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "# Ejemplo: ¬øQu√© probabilidad hay de ir de Midtown Center (161) a West Chelsea (246) en menos de 8 minutos?\n",
        "calcular_probabilidad_desplazamiento_rapido_dict(\n",
        "    normalized_datasets,\n",
        "   origen=161,\n",
        "    destino=246,\n",
        "    umbral_min=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5\tRepite los apartados 1.1, 1.2, 1.3, 1.4, 2.1, 2.2, 2.3 y 2.4 con un dataset del mismo mes, pero del a√±o 2009. Comenta todas las diferencias que vayas encontrando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Febrero de 2009\n",
        "Se realizar√° en las funciones anteriores, a√±adiendo el nuevo DataSet yellow_tripdata_2009-02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tercera Parte: An√°lisis Predictivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1\t¬øCu√°les son las zonas donde es m√°s probable coger un taxi en funci√≥n de la hora del d√≠a?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_probabilidad_pickup_por_zona(datasets_dict):\n",
        "    \"\"\"\n",
        "    Calcula y visualiza las zonas con m√°s probabilidad de coger un taxi seg√∫n la hora del d√≠a.\n",
        "    Incluye:\n",
        "    - Heatmap general de todas las zonas.\n",
        "    - Gr√°fico de l√≠neas para el top 10 zonas.\n",
        "    - Heatmap restringido al top 20 zonas.\n",
        "    \"\"\"\n",
        "    # Concatenar todos los datasets que tengan pickup_datetime y PULocationID\n",
        "    data = []\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        if 'pickup_datetime' in df.columns and 'PULocationID' in df.columns:\n",
        "            temp = df[['pickup_datetime', 'PULocationID']].copy()\n",
        "            temp[\"dataset\"] = nombre\n",
        "            data.append(temp)\n",
        "    if not data:\n",
        "        print(\"‚ùå Ning√∫n dataset tiene las columnas requeridas.\")\n",
        "        return\n",
        "    \n",
        "    df_all = pd.concat(data)\n",
        "    df_all['pickup_datetime'] = pd.to_datetime(df_all['pickup_datetime'], errors='coerce')\n",
        "    df_all = df_all.dropna(subset=['pickup_datetime', 'PULocationID'])\n",
        "    df_all['hour'] = df_all['pickup_datetime'].dt.hour\n",
        "\n",
        "    # Calcular n√∫mero de pickups por zona y hora\n",
        "    pickup_counts = df_all.groupby(['hour', 'PULocationID']).size().reset_index(name='count')\n",
        "\n",
        "    # Normalizar por hora (para obtener probabilidad relativa)\n",
        "    total_por_hora = pickup_counts.groupby('hour')['count'].transform('sum')\n",
        "    pickup_counts['probabilidad'] = pickup_counts['count'] / total_por_hora\n",
        "\n",
        "    # Pivot para heatmap completo\n",
        "    pivot = pickup_counts.pivot(index='PULocationID', columns='hour', values='probabilidad').fillna(0)\n",
        "\n",
        "    # Visualizaci√≥n 1: Gr√°fico de l√≠neas para top 10 zonas\n",
        "    top_zonas = df_all[\"PULocationID\"].value_counts().head(10).index\n",
        "    df_top = df_all[df_all[\"PULocationID\"].isin(top_zonas)]\n",
        "    top_line_df = df_top.groupby([\"hour\", \"PULocationID\"]).size().unstack().fillna(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_line_df.plot(marker=\"o\")\n",
        "    plt.title(\"üìà Evoluci√≥n horaria de los 10 PULocationID m√°s activos\")\n",
        "    plt.xlabel(\"Hora del d√≠a\")\n",
        "    plt.ylabel(\"N√∫mero de pickups\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(title=\"Zona\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualizaci√≥n 2: Heatmap de top 20 zonas\n",
        "    top_20 = df_all[\"PULocationID\"].value_counts().head(20).index\n",
        "    filtered = df_all[df_all[\"PULocationID\"].isin(top_20)].copy()\n",
        "    filtered[\"hour\"] = pd.to_datetime(filtered[\"pickup_datetime\"]).dt.hour\n",
        "    heat_df = filtered.groupby([\"PULocationID\", \"hour\"]).size().unstack(fill_value=0)\n",
        "    heat_df = heat_df.div(heat_df.sum(axis=1), axis=0)  # Normalizar por zona\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(heat_df, cmap=\"YlOrBr\", annot=True, fmt=\".2f\")\n",
        "    plt.title(\"üî• Probabilidad normalizada por zona (Top 20)\")\n",
        "    plt.xlabel(\"Hora del d√≠a\")\n",
        "    plt.ylabel(\"Zona\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar la funci√≥n para analizar la probabilidad de recoger un taxi por hora y zona\n",
        "analizar_probabilidad_pickup_por_zona(normalized_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2\t¬øCu√°l es la mejor hora del d√≠a para ir al aeropuerto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objetivo: Saber a qu√© hora los trayectos al aeropuerto son m√°s r√°pidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_horas_para_ir_aeropuerto(normalized_datasets, zona_nombres):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "\n",
        "    # 1. Detectar zonas de aeropuerto autom√°ticamente\n",
        "    aeropuerto_ids = [\n",
        "        zona_id for zona_id, nombre in zona_nombres.items()\n",
        "        if isinstance(nombre, str) and (\n",
        "            'airport' in nombre.lower() or 'jfk' in nombre.lower() or 'laguardia' in nombre.lower()\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if not aeropuerto_ids:\n",
        "        print(\"‚ùå No se detectaron zonas de aeropuerto en zona_nombres.\")\n",
        "        return\n",
        "\n",
        "    print(\"üõ´ IDs de aeropuerto detectados:\", aeropuerto_ids)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # 2. Recolectar trayectos a aeropuerto\n",
        "    for nombre, df in normalized_datasets.items():\n",
        "        if all(col in df.columns for col in ['pickup_datetime', 'DOLocationID', 'trip_duration_min']):\n",
        "            temp = df[['pickup_datetime', 'DOLocationID', 'trip_duration_min']].copy()\n",
        "            temp = temp[temp['DOLocationID'].isin(aeropuerto_ids)]\n",
        "            temp['hour'] = pd.to_datetime(temp['pickup_datetime'], errors='coerce').dt.hour\n",
        "            temp.dropna(subset=['hour', 'trip_duration_min'], inplace=True)\n",
        "            data.append(temp)\n",
        "\n",
        "    if not data:\n",
        "        print(\"‚ùå No hay trayectos con destino a aeropuerto.\")\n",
        "        return\n",
        "\n",
        "    df_aeropuerto = pd.concat(data)\n",
        "\n",
        "    # 3. Agrupaciones\n",
        "    duracion_media = df_aeropuerto.groupby('hour')['trip_duration_min'].mean().reset_index(name='duraci√≥n_media_min')\n",
        "    frecuencia = df_aeropuerto.groupby('hour').size().reset_index(name='n_trayectos')\n",
        "\n",
        "    resumen = pd.merge(duracion_media, frecuencia, on='hour')\n",
        "\n",
        "    # 4. Selecci√≥n de mejor hora\n",
        "    top_frecuencia = resumen['n_trayectos'].quantile(0.7)\n",
        "    candidatos = resumen[resumen['n_trayectos'] >= top_frecuencia]\n",
        "    mejor_hora = candidatos.sort_values('duraci√≥n_media_min').iloc[0]\n",
        "\n",
        "    # 5. Visualizaci√≥n\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    fig.suptitle(\"üõ´ An√°lisis de la mejor hora para ir al aeropuerto\", fontsize=16)\n",
        "\n",
        "    sns.lineplot(x='hour', y='duraci√≥n_media_min', data=resumen, marker='o', ax=axs[0], color='orange')\n",
        "    axs[0].axvline(mejor_hora['hour'], color='green', linestyle='--', label=f'Mejor hora: {int(mejor_hora[\"hour\"])}h')\n",
        "    axs[0].set_title(\"Duraci√≥n media del trayecto por hora\")\n",
        "    axs[0].set_ylabel(\"Duraci√≥n media (min)\")\n",
        "    axs[0].set_xlabel(\"Hora\")\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    sns.barplot(x='hour', y='n_trayectos', data=resumen, ax=axs[1], palette='Blues_d')\n",
        "    axs[1].axvline(mejor_hora['hour'], color='green', linestyle='--')\n",
        "    axs[1].set_title(\"Frecuencia de trayectos al aeropuerto por hora\")\n",
        "    axs[1].set_ylabel(\"N¬∫ de trayectos\")\n",
        "    axs[1].set_xlabel(\"Hora\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # 6. Imprimir resumen\n",
        "    print(\"\\nüìù Recomendaci√≥n:\")\n",
        "    print(f\"La mejor hora para ir al aeropuerto es las **{int(mejor_hora['hour'])}:00 h**.\")\n",
        "    print(f\"- Duraci√≥n media estimada: {mejor_hora['duraci√≥n_media_min']:.1f} minutos.\")\n",
        "    print(f\"- N√∫mero de trayectos registrados en esa hora: {mejor_hora['n_trayectos']}.\")\n",
        "\n",
        "    return resumen\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resumen_aeropuerto = analizar_horas_para_ir_aeropuerto(normalized_datasets, zona_nombres)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3\tDise√±a un modelo que, dada una hora, una zona origen, y una zona destino, predice la duraci√≥n del trayecto y su coste. Muestra la relevancia de los atributos del dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicci√≥n de duraci√≥n y coste del trayecto dada la hora, la zona de origen y la zona de destino.\n",
        "- Dado:\n",
        "    - hora (extra√≠da de pickup_datetime)\n",
        "    - zona origen (PULocationID)\n",
        "    - zona destino (DOLocationID)\n",
        "\n",
        "- Predecir:\n",
        "    - trip_duration_min\n",
        "    - total_amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preparar_dataset_modelo_total_amount_log(datasets_dict):\n",
        "    \"\"\"\n",
        "    Prepara los datos a partir de un diccionario de datasets para predecir duraci√≥n y total_amount.\n",
        "    - Combina todos los DataFrames en uno.\n",
        "    - Filtra y limpia outliers.\n",
        "    - Crea variables: hora, d√≠a_semana, log_duraci√≥n.\n",
        "    - Devuelve X, y_duracion_log, y_total, df final.\n",
        "    \"\"\"\n",
        "    # Unir todos los datasets v√°lidos en uno\n",
        "    dfs = []\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        if {\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\", \"total_amount\"}.issubset(df.columns):\n",
        "            dfs.append(df[[\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\", \"total_amount\", \"trip_duration_min\"]].copy())\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No hay datasets v√°lidos con las columnas requeridas.\")\n",
        "    \n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    \n",
        "    # Limpiar registros inv√°lidos\n",
        "    df = df.dropna(subset=[\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\", \"total_amount\", \"trip_duration_min\"])\n",
        "    df = df[\n",
        "        (df[\"trip_distance\"] > 0) & \n",
        "        (df[\"total_amount\"] > 0) & \n",
        "        (df[\"trip_duration_min\"] > 0)\n",
        "    ].copy()\n",
        "    \n",
        "    # Transformaciones\n",
        "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
        "    df[\"hour\"] = df[\"pickup_datetime\"].dt.hour\n",
        "    df[\"day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
        "    df[\"log_duration\"] = np.log1p(df[\"trip_duration_min\"])\n",
        "    \n",
        "    # Preparar features y targets\n",
        "    X = df[[\"hour\", \"day_of_week\", \"PULocationID\", \"DOLocationID\", \"trip_distance\"]]\n",
        "    y_duracion_log = df[\"log_duration\"]\n",
        "    y_total = df[\"total_amount\"]\n",
        "    \n",
        "    return X, y_duracion_log, y_total, df\n",
        "\n",
        "# Probamos si carga correctamente\n",
        "X, y_duracion_log, y_total, df_modelo = preparar_dataset_modelo_total_amount_log(normalized_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenar y evaluar los modelos de RandomForest: duraci√≥n y coste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "def entrenar_y_evaluar_modelos(df_modelo):\n",
        "    \"\"\"\n",
        "    Entrena y eval√∫a dos modelos de RandomForest: duraci√≥n y coste.\n",
        "    \"\"\"\n",
        "    # Variables predictoras y objetivo\n",
        "    X = df_modelo[[\"hour\", \"PULocationID\", \"DOLocationID\"]]\n",
        "    y_duracion = df_modelo[\"trip_duration_min\"]\n",
        "    y_precio = df_modelo[\"total_amount\"]\n",
        "\n",
        "    # Codificaci√≥n categ√≥rica con OneHot\n",
        "    preprocesador = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), [\"PULocationID\", \"DOLocationID\"])\n",
        "        ],\n",
        "        remainder='passthrough'  # deja pasar la hora\n",
        "    )\n",
        "\n",
        "    # Modelo para duraci√≥n\n",
        "    modelo_duracion = Pipeline([\n",
        "        (\"preprocesado\", preprocesador),\n",
        "        (\"rf\", RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    # Modelo para precio\n",
        "    modelo_precio = Pipeline([\n",
        "        (\"preprocesado\", preprocesador),\n",
        "        (\"rf\", RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    # Divisi√≥n train-test\n",
        "    X_train, X_test, y_train_d, y_test_d = train_test_split(X, y_duracion, test_size=0.2, random_state=42)\n",
        "    _, _, y_train_p, y_test_p = train_test_split(X, y_precio, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"üîÅ Entrenando modelo de duraci√≥n...\")\n",
        "    modelo_duracion.fit(X_train, y_train_d)\n",
        "\n",
        "    print(\"üîÅ Entrenando modelo de precio total...\")\n",
        "    modelo_precio.fit(X_train, y_train_p)\n",
        "\n",
        "    # Predicciones\n",
        "    y_pred_d = modelo_duracion.predict(X_test)\n",
        "    y_pred_p = modelo_precio.predict(X_test)\n",
        "\n",
        "    # Evaluaci√≥n\n",
        "    print(\"\\nüìä Evaluaci√≥n modelo de duraci√≥n:\")\n",
        "    print(f\"- MAE: {mean_absolute_error(y_test_d, y_pred_d):.2f} minutos\")\n",
        "    print(f\"- RMSE: {np.sqrt(mean_squared_error(y_test_d, y_pred_d)):.2f}\")\n",
        "    print(f\"- R¬≤: {r2_score(y_test_d, y_pred_d):.2f}\")\n",
        "\n",
        "    print(\"\\nüìä Evaluaci√≥n modelo de precio total:\")\n",
        "    print(f\"- MAE: {mean_absolute_error(y_test_p, y_pred_p):.2f} $\")\n",
        "    print(f\"- RMSE: {np.sqrt(mean_squared_error(y_test_p, y_pred_p)):.2f}\")\n",
        "    print(f\"- R¬≤: {r2_score(y_test_p, y_pred_p):.2f}\")\n",
        "\n",
        "    return modelo_duracion, modelo_precio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar y evaluar los modelos (duraci√≥n y precio)\n",
        "# 12minutos\n",
        "modelo_duracion, modelo_precio = entrenar_y_evaluar_modelos(df_modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analizar_valores_invalidos_dict(normalized_datasets)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
