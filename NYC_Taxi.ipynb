{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Primera Parte: An√°lisis Cuantitativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1\tPrimer examen preliminar del dataset.  ¬øEn qu√© formato est√° el dataset y qu√© tiene que ver este formato con Big Data? ¬øQu√© par√°metros hay en el dataset? ¬øCu√°l es su significado? ¬øExisten valores aparentemente incorrectos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalar los paquetes necesarios, ejecutar solo al principio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Instalar paquetes necesarios\n",
        "%pip install pandas numpy pyarrow matplotlib seaborn plotly nbformat scipy scikit-learn pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importar librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GPDQVVaGW7A"
      },
      "outputs": [],
      "source": [
        "# üì¶ Imports principales para an√°lisis de datos y visualizaci√≥n\n",
        "\n",
        "# Carga y manipulaci√≥n de datos\n",
        "import pandas as pd       # type: ignore # Manipulaci√≥n y an√°lisis de datos tabulares\n",
        "import numpy as np        # type: ignore # Operaciones num√©ricas (opcional, pero √∫til)\n",
        "\n",
        "# Lectura de archivos Parquet\n",
        "import pyarrow.parquet as pq  # type: ignore # Backend recomendado para parquet (alternativa: fastparquet)\n",
        "\n",
        "# Visualizaci√≥n b√°sica y exploratoria\n",
        "import matplotlib.pyplot as plt      # type: ignore # Visualizaciones est√°ndar (histogramas, scatter, etc.)\n",
        "import seaborn as sns                # type: ignore # Visualizaci√≥n estad√≠stica avanzada (heatmaps, countplots)\n",
        "\n",
        "# Visualizaci√≥n avanzada \n",
        "import plotly.express as px         # type: ignore # Visualizaciones interactivas (incl. Sankey, mapas, etc.)\n",
        "import plotly.graph_objects as go # type: ignore # Gr√°ficos m√°s complejos y personalizados\n",
        "\n",
        "# Configuraci√≥n general de visualizaciones\n",
        "plt.style.use('seaborn-v0_8')        # Estilo visual est√°ndar\n",
        "# %matplotlib inline                   # Para visualizar directamente en el notebook\n",
        "\n",
        "# Para calcular el z-score de las columnas num√©ricas\n",
        "from scipy.stats import zscore # type: ignore\n",
        "\n",
        "# Para manejar rutas de archivos y directorios\n",
        "import os\n",
        "\n",
        "# linear models para regresi√≥n\n",
        "from sklearn.linear_model import RANSACRegressor, LinearRegression # type: ignore\n",
        "\n",
        "# Para manejar el entorno de Spark\n",
        "from pyspark.sql import SparkSession # type: ignore\n",
        "from pyspark.sql.functions import col, when, isnull, lit, mean, median, count, udf  # type: ignore\n",
        "from pyspark.sql.types import BooleanType, FloatType # type: ignore\n",
        "from pyspark.sql.window import Window # type: ignore\n",
        "import pyspark.sql.functions as F # type: ignore\n",
        "from datetime import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carga los nombres de zona desde taxi_zone_lookup.csv en zona_nombres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_zona_nombres():\n",
        "    \"\"\"\n",
        "    Devuelve un diccionario {LocationID: Zone} con los nombres de zona de NYC.\n",
        "    1. Si el archivo local 'original_parquets/taxi_zone_lookup.csv' existe, lo carga desde ah√≠.\n",
        "    2. Si no existe, lo descarga de la URL y lo guarda localmente para futuras ejecuciones.\n",
        "    \"\"\"\n",
        "    local_path = \"original_parquets/taxi_zone_lookup.csv\"\n",
        "    url_lookup = \"https://raw.githubusercontent.com/renerubio/NYC-TAXI/refs/heads/main/original_parquets/taxi_zone_lookup.csv\"\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        taxi_zones = pd.read_csv(local_path)\n",
        "        print(\"Cargado desde archivo local.\")\n",
        "    else:\n",
        "        taxi_zones = pd.read_csv(url_lookup)\n",
        "        print(\"Cargado desde URL.\")\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "        taxi_zones.to_csv(local_path, index=False)\n",
        "        print(f\"Guardado en {local_path}.\")\n",
        "\n",
        "    return taxi_zones.set_index(\"LocationID\")[\"Zone\"].to_dict()\n",
        "\n",
        "# Cargar diccionario\n",
        "zona_nombres = get_zona_nombres()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qUVDhbOJ4C-"
      },
      "source": [
        "### Importar y cargar ficheros parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tTdCSulJBxm"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n reutilizable para cargar Parquet desde local o URL\n",
        "def cargar_parquet_local_o_url(local_path: str, url: str) -> pd.DataFrame:\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"‚úÖ Cargado desde archivo local: {local_path}\")\n",
        "    else:\n",
        "        print(f\"üåê Descargando desde URL: {url}\")\n",
        "        df = pd.read_parquet(url, engine=\"pyarrow\")\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "        df.to_parquet(local_path, engine=\"pyarrow\", index=False)\n",
        "        print(f\"üíæ Guardado en {local_path}\")\n",
        "        return df\n",
        "\n",
        "    return pd.read_parquet(local_path, engine=\"pyarrow\")\n",
        "\n",
        "# Ruta base de trip-data\n",
        "data_path = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
        "\n",
        "# Diccionario de datasets y sus rutas\n",
        "datasets_routes = {\n",
        "    \"yellow_df_2024_02\": (\"original_parquets/yellow_tripdata_2024-02.parquet\", data_path + \"yellow_tripdata_2024-02.parquet\"),\n",
        "    \"green_df_2024_02\":  (\"original_parquets/green_tripdata_2024-02.parquet\",  data_path + \"green_tripdata_2024-02.parquet\"),\n",
        "    \"yellow_df_2024_12\": (\"original_parquets/yellow_tripdata_2024-12.parquet\", data_path + \"yellow_tripdata_2024-12.parquet\"),\n",
        "    \"green_df_2024_12\":  (\"original_parquets/green_tripdata_2024-12.parquet\",  data_path + \"green_tripdata_2024-12.parquet\"),\n",
        "}\n",
        "\n",
        "# Cargar todos los datasets\n",
        "for nombre, (ruta_local, url_remota) in datasets_routes.items():\n",
        "    globals()[nombre] = cargar_parquet_local_o_url(ruta_local, url_remota)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objeto global con los datasets cargados y listos para usar "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos todos los datasets con sus nombres\n",
        "original_datasets = {\n",
        "    \"yellow_tripdata_2024-02\": yellow_df_2024_02, # type: ignore\n",
        "    \"green_tripdata_2024-02\": green_df_2024_02, # type: ignore\n",
        "    \"yellow_tripdata_2024-12\": yellow_df_2024_12, # type: ignore\n",
        "    \"green_tripdata_2024-12\": green_df_2024_12, # type: ignore\n",
        "}\n",
        "\n",
        "# Liberar memoria de las variables ya cargadas\n",
        "del yellow_df_2024_02 # type: ignore\n",
        "del green_df_2024_02 # type: ignore\n",
        "del yellow_df_2024_12 # type: ignore\n",
        "del green_df_2024_12 # type: ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An√°lisis de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtener todas las columnas y su frecuencia\n",
        "Al haber diferentes nombres en las columnas las visualizamos en un heatmap\n",
        "0.8seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_column_presence_heatmap(datasets):\n",
        "    \"\"\"\n",
        "    Genera un heatmap de presencia de columnas a partir de un diccionario de datasets.\n",
        "\n",
        "    :param datasets: Diccionario con claves como nombres de dataset y valores como DataFrames\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Obtener todas las columnas y su frecuencia\n",
        "    all_columns = set()\n",
        "    for df in datasets.values():\n",
        "        all_columns.update(df.columns)\n",
        "    column_freq = {col: sum(col in df.columns for df in datasets.values()) for col in all_columns}\n",
        "\n",
        "    # 2. Ordenar columnas por frecuencia (de mayor a menor)\n",
        "    sorted_columns = sorted(all_columns, key=lambda x: (-column_freq[x], x))\n",
        "\n",
        "    # 3. Crear matriz de presencia (S√≠/No)\n",
        "    presence_matrix = []\n",
        "    for name, df in datasets.items():\n",
        "        presences = [1 if col in df.columns else 0 for col in sorted_columns]\n",
        "        presences = [\"S√≠\" if p == 1 else \"No\" for p in presences]\n",
        "        presence_matrix.append(presences)\n",
        "\n",
        "    # 4. Crear DataFrame para el heatmap\n",
        "    heatmap_df = pd.DataFrame(\n",
        "        presence_matrix,\n",
        "        index=datasets.keys(),\n",
        "        columns=sorted_columns\n",
        "    )\n",
        "\n",
        "    # 5. Configurar el gr√°fico\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.heatmap(\n",
        "        heatmap_df.replace({\"S√≠\": 1, \"No\": 0}),\n",
        "        cmap=[\"#FF6B6B\", \"#51CF66\"],\n",
        "        linewidths=0.5,\n",
        "        linecolor=\"lightgray\",\n",
        "        annot=heatmap_df.values,\n",
        "        fmt=\"\",\n",
        "        cbar=False\n",
        "    )\n",
        "\n",
        "    # 6. Personalizar el gr√°fico\n",
        "    plt.title(\"Presencia de Columnas en los Datasets\", fontsize=14, pad=20)\n",
        "    plt.xlabel(\"Columnas (ordenadas por frecuencia)\", fontsize=10)\n",
        "    plt.ylabel(\"Dataset\", fontsize=10)\n",
        "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_column_presence_heatmap(original_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Renombrar columnas/atributos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objeto global `column_groups` son grupos de columnas equivalentes el renombrado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grupos de columnas equivalentes para el renombrado\n",
        "column_groups = {\n",
        "    # Columnas de ubicaci√≥n (zonas)\n",
        "    'PULocationID': ['PUlocationID', 'PULocationID', 'pickup_zone', 'PU_zone'],\n",
        "    'DOLocationID': ['DOlocationID', 'DOLocationID', 'dropoff_zone', 'DO_zone'],\n",
        "\n",
        "    # Columnas de ubicaci√≥n (coordenadas)\n",
        "    'Start_Lon': ['Start_Lon', 'pickup_longitude', 'start_lon', 'pickup_lon'],\n",
        "    'Start_Lat': ['Start_Lat', 'pickup_latitude', 'start_lat', 'pickup_lat'],\n",
        "    'End_Lon': ['End_Lon', 'dropoff_longitude', 'end_lon', 'dropoff_lon'],\n",
        "    'End_Lat': ['End_Lat', 'dropoff_latitude', 'end_lat', 'dropoff_lat'],\n",
        "\n",
        "    # Columnas temporales\n",
        "    'pickup_datetime': [\n",
        "        'tpep_pickup_datetime', 'lpep_pickup_datetime',\n",
        "        'pickup_datetime', 'Trip_Pickup_DateTime',\n",
        "        'request_datetime'\n",
        "    ],\n",
        "    'dropoff_datetime': [\n",
        "        'tpep_dropoff_datetime', 'lpep_dropoff_datetime',\n",
        "        'dropoff_datetime', 'dropOff_datetime', 'on_scene_datetime',\n",
        "        'Trip_Dropoff_DateTime'\n",
        "    ],\n",
        "\n",
        "    # Columnas de distancia\n",
        "    'trip_distance': [\n",
        "        'trip_distance', 'Trip_distance', 'Trip_Distance',\n",
        "        'distance', 'trip_miles', 'Distance'\n",
        "    ],\n",
        "\n",
        "    # Columnas de importe total\n",
        "    'total_amount': ['total_amount', 'Total_Amt', 'Total_amount', 'total_amt', 'total fare', 'fare'],\n",
        "\n",
        "    # Columnas de tarifa base\n",
        "    'fare_amount': ['fare_amount', 'Fare_Amt', 'fare_amt', 'Fare', 'fare'],\n",
        "\n",
        "    # Columnas de tarifas\n",
        "    'airport_fee': ['airport_fee', 'Airport_fee', 'airportFee'],\n",
        "    'extra': ['extra', 'surcharge', 'Extra', 'extra_charge'],\n",
        "\n",
        "    # Columnas de pago\n",
        "    'tolls': ['tolls_amount', 'tolls', 'Tolls_Amt', 'toll', 'Tolls'],\n",
        "    'tips': ['tip_amount', 'tips', 'Tip_Amt', 'Tip', 'Tips'],\n",
        "    'payment_type': ['payment_type', 'Payment_Type', 'paymenttype', 'pay_type'],\n",
        "\n",
        "    # Columnas de c√≥digo de tarifa\n",
        "    'RatecodeID': ['RatecodeID', 'Rate_Code', 'rate_code', 'Rate_Code_ID'],\n",
        "\n",
        "    # Columnas de ID de proveedor\n",
        "    'VendorID': ['VendorID', 'vendor_name', 'vendorid', 'Vendor_Id'],\n",
        "\n",
        "    # Columnas de n√∫mero de pasajeros\n",
        "    'passenger_count': ['passenger_count', 'Passenger_Count', 'passengers', 'passenger', 'num_passengers'],\n",
        "\n",
        "    # Columnas de store and forward\n",
        "    'store_and_fwd_flag': ['store_and_fwd_flag', 'store_and_forward', 'store_fwd_flag', 'store_flag'],\n",
        "\n",
        "    # Columnas especiales de FHV y FHVHV\n",
        "    'base_number': [\n",
        "        'dispatching_base_num', 'Affiliated_base_number',\n",
        "        'originating_base_num', 'hvfhs_license_num', 'base_number'\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Renombrar los par√°metros para tener uniformidad para los futuros an√°lisis (50seg ~ 2min)\n",
        "- Renombra columnas en cada dataset seg√∫n el diccionario column_groups.\n",
        "- A√±ade same_location_flag cuando el punto de recogida y destino son iguales.\n",
        "- A√±ade suspicious_location_flag si adem√°s la distancia es 0 o el total_amount es muy alto (umbral > 1000$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_column_names_with_flags(datasets_dict, column_mapping):\n",
        "    \"\"\"\n",
        "    Renombra columnas a su est√°ndar y agrega flags de localizaci√≥n.\n",
        "    Es compatible con datasets con zonas (PULocationID/DOLocationID) y solo coordenadas.\n",
        "\n",
        "    - same_location_flag: True si PULocationID == DOLocationID o si las coordenadas de origen y destino son iguales.\n",
        "    - suspicious_location_flag: True si misma localizaci√≥n y (trip_distance == 0 o total_amount > umbral)\n",
        "    - Ordena columnas seg√∫n el orden de column_mapping + flags al final\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        column_mapping: mapeo de nombres est√°ndar a variantes\n",
        "\n",
        "    Retorna:\n",
        "        diccionario con DataFrames normalizados y enriquecidos con flags\n",
        "    \"\"\"\n",
        "    datasets_renombrados = {}\n",
        "\n",
        "    # Orden est√°ndar: todos los keys del mapping en orden + flags al final\n",
        "    orden_base = list(column_mapping.keys()) + [\"same_location_flag\", \"suspicious_location_flag\"]\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # 1. Renombra columnas\n",
        "        for standard_name, variants in column_mapping.items():\n",
        "            for variant in variants:\n",
        "                if variant in df.columns:\n",
        "                    if standard_name in df.columns and variant != standard_name:\n",
        "                        df.drop(columns=[variant], inplace=True)\n",
        "                    else:\n",
        "                        df.rename(columns={variant: standard_name}, inplace=True)\n",
        "\n",
        "        # 2. Elimina duplicadas\n",
        "        df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "        # 3. Flags de localizaci√≥n\n",
        "        if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "            df[\"same_location_flag\"] = df[\"PULocationID\"] == df[\"DOLocationID\"]\n",
        "\n",
        "            if \"trip_distance\" in df.columns and \"total_amount\" in df.columns:\n",
        "                df[\"suspicious_location_flag\"] = (\n",
        "                    (df[\"same_location_flag\"]) &\n",
        "                    ((df[\"trip_distance\"] == 0) | (df[\"total_amount\"] > 1000))\n",
        "                )\n",
        "            else:\n",
        "                df[\"suspicious_location_flag\"] = False\n",
        "\n",
        "        # Si no hay zonas pero s√≠ coordenadas (soporta datasets antiguos tipo 2009)\n",
        "        elif all(col in df.columns for col in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            df[\"same_location_flag\"] = (\n",
        "                (df[\"Start_Lon\"] == df[\"End_Lon\"]) &\n",
        "                (df[\"Start_Lat\"] == df[\"End_Lat\"])\n",
        "            )\n",
        "            if \"trip_distance\" in df.columns and \"total_amount\" in df.columns:\n",
        "                df[\"suspicious_location_flag\"] = (\n",
        "                    (df[\"same_location_flag\"]) &\n",
        "                    ((df[\"trip_distance\"] == 0) | (df[\"total_amount\"] > 1000))\n",
        "                )\n",
        "            else:\n",
        "                df[\"suspicious_location_flag\"] = False\n",
        "        else:\n",
        "            df[\"same_location_flag\"] = False\n",
        "            df[\"suspicious_location_flag\"] = False\n",
        "\n",
        "        # 4. Ordena columnas seg√∫n orden_base\n",
        "        columnas_existentes = [col for col in orden_base if col in df.columns]\n",
        "        columnas_extra = [col for col in df.columns if col not in columnas_existentes]\n",
        "        df = df[columnas_existentes + columnas_extra]\n",
        "\n",
        "        datasets_renombrados[nombre] = df\n",
        "\n",
        "    return datasets_renombrados\n",
        "\n",
        "\n",
        "renamed_datasets = rename_column_names_with_flags(original_datasets, column_groups)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap antes y despu√©s de renombrar columnas\n",
        "0.9seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# heatmap de columnas antes de renombrar columnas\n",
        "plot_column_presence_heatmap(original_datasets)\n",
        "# heatmap de columnas despu√©s de renombrar columnas\n",
        "plot_column_presence_heatmap(renamed_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardado en parquet unificado "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar datasets renombrados\n",
        "def guardar_parquets_unificados(datasets_dict, carpeta_salida=\"unified_parquets\"):\n",
        "    \"\"\"\n",
        "    Une los datasets yellow y green por mes (ignorando 2009) y guarda en formato Parquet.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        carpeta_salida: carpeta donde guardar los Parquet\n",
        "    \"\"\"\n",
        "    os.makedirs(carpeta_salida, exist_ok=True)\n",
        "\n",
        "    # Detecci√≥n autom√°tica de los pares yellow/green\n",
        "    meses = [\"2024-02\", \"2024-12\"]\n",
        "\n",
        "    for mes in meses:\n",
        "        yellow_key = f\"yellow_tripdata_{mes}\"\n",
        "        green_key = f\"green_tripdata_{mes}\"\n",
        "\n",
        "        if yellow_key in datasets_dict and green_key in datasets_dict:\n",
        "            df_yellow = datasets_dict[yellow_key]\n",
        "            df_green = datasets_dict[green_key]\n",
        "\n",
        "            # Uni√≥n ignorando columnas que falten (en pandas)\n",
        "            df_unificado = pd.concat([df_yellow, df_green], ignore_index=True, sort=False)\n",
        "\n",
        "            # Ruta de salida\n",
        "            ruta_salida = os.path.join(carpeta_salida, f\"taxis_tripdata_{mes}.parquet\")\n",
        "\n",
        "            # Guardado\n",
        "            df_unificado.to_parquet(ruta_salida, index=False)\n",
        "            print(f\"‚úÖ Guardado: {ruta_salida}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  No se encontraron ambos datasets para {mes}, se omite.\")\n",
        "\n",
        "guardar_parquets_unificados(renamed_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unificaci√≥n de datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "taxis_df_2024_02 = pd.concat([\n",
        "    renamed_datasets[\"yellow_tripdata_2024-02\"],\n",
        "    renamed_datasets[\"green_tripdata_2024-02\"]\n",
        "], ignore_index=True, sort=False)\n",
        "\n",
        "taxis_df_2024_12 = pd.concat([\n",
        "    renamed_datasets[\"yellow_tripdata_2024-12\"],\n",
        "    renamed_datasets[\"green_tripdata_2024-12\"]\n",
        "], ignore_index=True, sort=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limpieza de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpieza de datos con pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def limpiar_dataset_integrado(datasets_dict, zona_nombres=None, min_freq=5):\n",
        "    \"\"\"\n",
        "    Limpieza y validaci√≥n exhaustiva de los datasets de trayectos NYC Taxi, robusta ante diferentes formatos y a√±os.\n",
        "\n",
        "    Incorpora imputaci√≥n, filtrado de outliers, validaciones de coherencia,\n",
        "    y enriquecimiento de variables temporales y tarifarias.\n",
        "    Soporta datasets con columnas de zonas (PULocationID/DOLocationID) o solo coordenadas (Start_Lon, Start_Lat, End_Lon, End_Lat).\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        zona_nombres: diccionario {zone_id: nombre_zona} (opcional)\n",
        "        min_freq: frecuencia m√≠nima para imputaci√≥n local\n",
        "\n",
        "    Retorna:\n",
        "        Diccionario con los DataFrames corregidos y enriquecidos.\n",
        "    \"\"\"\n",
        "    datasets_corregidos = {}\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        try:\n",
        "            print(f\"\\nüîÑ Procesando: {nombre}\")\n",
        "            df = df.copy()\n",
        "            flags = []\n",
        "\n",
        "            # --- 1. Detectar columnas est√°ndar o alternativas con column_groups ---\n",
        "            col = lambda grupo: next((c for c in column_groups[grupo] if c in df.columns), grupo if grupo in df.columns else None)\n",
        "            col_pickup = col('pickup_datetime')\n",
        "            col_dropoff = col('dropoff_datetime')\n",
        "            col_total = col('total_amount')\n",
        "            col_dist = col('trip_distance')\n",
        "            col_pu = col('PULocationID')\n",
        "            col_do = col('DOLocationID')\n",
        "            col_start_lon = col('Start_Lon')\n",
        "            col_start_lat = col('Start_Lat')\n",
        "            col_end_lon = col('End_Lon')\n",
        "            col_end_lat = col('End_Lat')\n",
        "            col_passengers = col('passenger_count')\n",
        "\n",
        "            # --- 2. Validaci√≥n de columnas requeridas ---\n",
        "            required_cols = [col_pickup, col_dropoff, col_total, col_dist]\n",
        "            if any(c is None for c in required_cols):\n",
        "                print(f\"‚ö†Ô∏è {nombre}: faltan columnas m√≠nimas requeridas. Se omite.\")\n",
        "                continue\n",
        "\n",
        "            # --- 3. Convertir fechas ---\n",
        "            df[col_pickup] = pd.to_datetime(df[col_pickup], errors=\"coerce\")\n",
        "            df[col_dropoff] = pd.to_datetime(df[col_dropoff], errors=\"coerce\")\n",
        "\n",
        "            # --- 4. Validaci√≥n geogr√°fica flexible ---\n",
        "            tiene_zonas = col_pu is not None and col_do is not None and zona_nombres is not None\n",
        "            tiene_coords = all(c is not None for c in [col_start_lon, col_start_lat, col_end_lon, col_end_lat])\n",
        "\n",
        "            if tiene_zonas:\n",
        "                mask_zonas_validas = (\n",
        "                    df[col_pu].notnull() & df[col_do].notnull() &\n",
        "                    df[col_pu].astype(\"Int64\").isin(zona_nombres.keys()) &\n",
        "                    df[col_do].astype(\"Int64\").isin(zona_nombres.keys())\n",
        "                )\n",
        "                flags.append(mask_zonas_validas)\n",
        "                df[\"key\"] = list(zip(df[col_pu], df[col_do]))\n",
        "            elif tiene_coords:\n",
        "                mask_coords_validas = (\n",
        "                    df[col_start_lon].notnull() & df[col_start_lat].notnull() &\n",
        "                    df[col_end_lon].notnull() & df[col_end_lat].notnull() &\n",
        "                    ~df.get(\"suspicious_location_flag\", False)\n",
        "                )\n",
        "                flags.append(mask_coords_validas)\n",
        "                df[\"key\"] = list(zip(df[col_start_lon], df[col_start_lat], df[col_end_lon], df[col_end_lat]))\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: No se detectan ni columnas de zona ni de coordenadas. Se omite.\")\n",
        "                continue\n",
        "\n",
        "            # --- 5. Imputaci√≥n local de distancias ---\n",
        "            mask_td = df[col_dist].isnull() | (df[col_dist] <= 0)\n",
        "            valid_td = df.loc[~mask_td].copy()\n",
        "            freq = valid_td.groupby(\"key\").size()\n",
        "            comunes = freq[freq >= min_freq].index\n",
        "            medianas_td = valid_td.groupby(\"key\")[col_dist].median()\n",
        "            def imputar_td(row):\n",
        "                if row[\"key\"] in comunes:\n",
        "                    return medianas_td[row[\"key\"]]\n",
        "                return None\n",
        "            df.loc[mask_td, col_dist] = df.loc[mask_td].apply(imputar_td, axis=1)\n",
        "            flags.append(~df[col_dist].isnull())\n",
        "\n",
        "            # --- 6. Filtro: Distancia realista ---\n",
        "            flags.append((df[col_dist] >= 0.3) & (df[col_dist] <= 50))\n",
        "\n",
        "            # --- 7. C√°lculo de duraci√≥n de trayecto en minutos ---\n",
        "            if \"trip_duration_min\" not in df.columns:\n",
        "                df[\"trip_duration_min\"] = (df[col_dropoff] - df[col_pickup]).dt.total_seconds() / 60\n",
        "\n",
        "            # --- 8. Filtro: Duraci√≥n realista ---\n",
        "            mask_dur = df[\"trip_duration_min\"].notnull() & (df[\"trip_duration_min\"] > 0) & (df[\"trip_duration_min\"] <= 180)\n",
        "            flags.append(mask_dur)\n",
        "\n",
        "            # --- 9. Filtro: Precio total positivo ---\n",
        "            mask_amt = df[col_total].notnull() & (df[col_total] > 0)\n",
        "            flags.append(mask_amt)\n",
        "\n",
        "            # --- 10. Filtro: Velocidad media realista (2 < v < 100 km/h) ---\n",
        "            df[\"velocidad_media_kmh\"] = (df[col_dist] * 1.60934) / (df[\"trip_duration_min\"] / 60)\n",
        "            flags.append((df[\"velocidad_media_kmh\"] >= 2) & (df[\"velocidad_media_kmh\"] <= 100))\n",
        "\n",
        "            # --- 11. Filtro: Precio real/te√≥rico coherente ---\n",
        "            tarifa_inicial = 3.00\n",
        "            tarifa_milla = 2.50\n",
        "            tarifa_minuto = 0.70\n",
        "            df[\"precio_teorico_estimado\"] = (\n",
        "                tarifa_inicial +\n",
        "                df[col_dist] * tarifa_milla +\n",
        "                df[\"trip_duration_min\"] * tarifa_minuto\n",
        "            )\n",
        "            df[\"precio_ratio\"] = df[col_total] / df[\"precio_teorico_estimado\"]\n",
        "            flags.append((df[\"precio_ratio\"] < 3) & (df[\"precio_ratio\"] > 0.3))\n",
        "\n",
        "            # --- 12. Filtro: Origen y destino distintos ---\n",
        "            if tiene_zonas:\n",
        "                flags.append(df[col_pu] != df[col_do])\n",
        "            elif tiene_coords:\n",
        "                flags.append(\n",
        "                    (df[col_start_lon] != df[col_end_lon]) | (df[col_start_lat] != df[col_end_lat])\n",
        "                )\n",
        "\n",
        "            # --- 13. Imputaci√≥n local de pasajeros (si existe) ---\n",
        "            if col_passengers is not None:\n",
        "                mask_pc = df[col_passengers].isnull() | (df[col_passengers] < 0)\n",
        "                if mask_pc.any():\n",
        "                    medianas_pc = df.loc[~mask_pc].groupby(\"key\")[col_passengers].median()\n",
        "                    df.loc[mask_pc, col_passengers] = df.loc[mask_pc, \"key\"].map(medianas_pc)\n",
        "                flags.append(~df[col_passengers].isnull())\n",
        "\n",
        "            # --- 14. Filtro cr√≠tico: minutos por kil√≥metro realistas ---\n",
        "            df[\"min_per_km\"] = df[\"trip_duration_min\"] / (df[col_dist] * 1.60934)\n",
        "            flags.append(df[\"min_per_km\"] <= 20)\n",
        "\n",
        "            # --- 15. Enriquecimiento temporal: hora, d√≠a y hora pico ---\n",
        "            df[\"Hora\"] = df[col_pickup].dt.hour\n",
        "            df[\"Dia\"] = df[col_pickup].dt.day_name()\n",
        "            df['Hora_Pico'] = df['Hora'].apply(lambda x: 1 if (7 <= x < 10) or (16 <= x < 19) else 0)\n",
        "\n",
        "            # --- 16. Tarifa m√≠nima NYC: ---\n",
        "            df[\"Precio ($)\"] = df[col_total]\n",
        "            df.loc[df[\"trip_duration_min\"] < 2, \"Precio ($)\"] = 7.00\n",
        "\n",
        "            # --- 17. Filtro adicional: precio total m√≠nimo ---\n",
        "            df = df[df[\"Precio ($)\"] >= 2.00]\n",
        "\n",
        "            # --- 18. Filtro de outliers por precio/minuto ---\n",
        "            df[\"$/min\"] = df[\"Precio ($)\"] / df[\"trip_duration_min\"]\n",
        "            df = df[df[\"$/min\"] <= 5]\n",
        "\n",
        "            # --- 19. Filtro reforzado: Precio por km realista ---\n",
        "            df[\"fare_per_km\"] = df[\"Precio ($)\"] / (df[col_dist] * 1.60934)\n",
        "            df = df[(df[\"fare_per_km\"] >= 0.2) & (df[\"fare_per_km\"] <= 100)]\n",
        "\n",
        "            # --- 20. (Opcional) Validaci√≥n de pagos, tips, tolls, mta_tax, etc. ---\n",
        "            if \"payment_type\" in df.columns:\n",
        "                df = df[df[\"payment_type\"].notnull()]\n",
        "            if \"tips\" in df.columns:\n",
        "                df[\"tips\"] = df[\"tips\"].fillna(0)\n",
        "            if \"tolls\" in df.columns:\n",
        "                df[\"tolls\"] = df[\"tolls\"].fillna(0)\n",
        "            if \"mta_tax\" in df.columns:\n",
        "                df[\"mta_tax\"] = df[\"mta_tax\"].fillna(0)\n",
        "\n",
        "            # --- Aplicaci√≥n final de los flags ---\n",
        "            df[\"flag_limpieza\"] = flags[0]\n",
        "            for f in flags[1:]:\n",
        "                df[\"flag_limpieza\"] &= f\n",
        "\n",
        "            df_final = df[df[\"flag_limpieza\"]].copy()\n",
        "            df_final.drop(\n",
        "                columns=[\"key\", \"flag_limpieza\", \"velocidad_media_kmh\", \"min_per_km\", \"$/min\"],\n",
        "                inplace=True,\n",
        "                errors=\"ignore\"\n",
        "            )\n",
        "\n",
        "            datasets_corregidos[nombre] = df_final\n",
        "            print(f\"‚úÖ {nombre}: limpieza completada con {len(df_final)} registros v√°lidos.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error procesando {nombre}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return datasets_corregidos\n",
        "\n",
        "\n",
        "# aplicar limpieza a los datasets renombrados por separado\n",
        "cleaned_separated_datasets = limpiar_dataset_integrado(renamed_datasets, zona_nombres)\n",
        "\n",
        "# Aplicar limpieza a los datasets renombrados\n",
        "unified_datasets = {\n",
        "    \"taxis_tripdata_2024-02\": taxis_df_2024_02,\n",
        "    \"taxis_tripdata_2024-12\": taxis_df_2024_12\n",
        "}\n",
        "# Aplicar limpieza a los datasets renombrados\n",
        "cleaned_datasets = limpiar_dataset_integrado(unified_datasets, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar los datasets limpios en parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar datasets renombrados\n",
        "def guardar_datasets_limpios(datasets_dict, carpeta_salida=\"cleaned_parquets\", guardar_csv=False):\n",
        "    \"\"\"\n",
        "    Guarda cada DataFrame del diccionario en formato Parquet dentro de la carpeta especificada.\n",
        "    \n",
        "    Opcionalmente, tambi√©n guarda cada DataFrame como CSV.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: dict[str, pd.DataFrame] - Diccionario {nombre_dataset: DataFrame}\n",
        "        carpeta_salida: str - Carpeta donde guardar los archivos .parquet\n",
        "        guardar_csv: bool - Si True, tambi√©n guarda cada archivo como .csv\n",
        "    \"\"\"\n",
        "    os.makedirs(carpeta_salida, exist_ok=True)\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            print(f\"‚ùå El valor asociado a '{nombre}' no es un DataFrame. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            nombre_archivo = f\"{nombre}.parquet\"\n",
        "            ruta_parquet = os.path.join(carpeta_salida, nombre_archivo)\n",
        "            df.to_parquet(ruta_parquet, index=False)\n",
        "            print(f\"‚úÖ Guardado Parquet: {ruta_parquet}\")\n",
        "\n",
        "            if guardar_csv:\n",
        "                nombre_csv = f\"{nombre}.csv\"\n",
        "                ruta_csv = os.path.join(carpeta_salida, nombre_csv)\n",
        "                df.to_csv(ruta_csv, index=False)\n",
        "                print(f\"üìÑ Guardado CSV: {ruta_csv}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al guardar '{nombre}': {e}\")\n",
        "\n",
        "guardar_datasets_limpios(cleaned_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Opcional) - Cargar todos los datasets *.parquet limpios en un diccionario \n",
        "Es opcional si ya los tenemos creados, podr√≠amos cargarlos directamente en vez de pasar todo el proceso de renombrado, limpieza, unificaci√≥n y guardado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cargar_datasets_parquet(carpeta=\"cleaned_parquets\"):\n",
        "    \"\"\"\n",
        "    Carga los archivos .parquet uno a uno desde la carpeta especificada\n",
        "    y los guarda en un diccionario con nombres legibles.\n",
        "\n",
        "    Evita problemas de memoria al no cargar todos simult√°neamente.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    for archivo in os.listdir(carpeta):\n",
        "        if archivo.endswith(\".parquet\"):\n",
        "            nombre_dataset = archivo.replace(\".parquet\", \"\")\n",
        "            ruta = os.path.join(carpeta, archivo)\n",
        "            try:\n",
        "                print(f\"üì• Cargando: {nombre_dataset}\")\n",
        "                df = pd.read_parquet(ruta)\n",
        "                datasets[nombre_dataset] = df\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error al cargar {nombre_dataset}: {e}\")\n",
        "    return datasets\n",
        "\n",
        "# Cargar los datasets renombrados desde parquet\n",
        "\"\"\" renamed_datasets = cargar_datasets_parquet(\"datasets_limpios\") \"\"\"\n",
        "# Verificar la carga\n",
        "\"\"\" for name, df in renamed_datasets.items():\n",
        "    print(f\"{name}: {df.shape[0]:,} filas, {df.shape[1]} columnas\") \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øExisten valores aparentemente incorrectos?\n",
        "Analiza todos los DataFrames en un diccionario (como cleaned_datasets). La funci√≥n eval√∫a los mismos valores inv√°lidos y genera un resumen por dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificaci√≥n de calidad de columnas\n",
        "def analizar_valores_invalidos_dict(datasets_dict, zona_nombres=None):\n",
        "    \"\"\"\n",
        "    Analiza valores inv√°lidos en un diccionario de datasets (DataFrames).\n",
        "\n",
        "    Eval√∫a:\n",
        "    - Valores nulos o incoherentes en pasajeros, distancia, importe, duraci√≥n, zonas o coordenadas.\n",
        "    - Detecta autom√°ticamente si el dataset usa zonas o solo coordenadas (compatibilidad total).\n",
        "    - Si no hay zonas, valida coordenadas y flags de localizaci√≥n sospechosa si existen.\n",
        "\n",
        "    Devuelve:\n",
        "        Un DataFrame resumen con el conteo de errores por tipo y dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    resumen = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        resultados = {}\n",
        "        print(f\"üîç Analizando: {nombre}\")\n",
        "\n",
        "        # Helper para encontrar la primera columna disponible seg√∫n el mapping\n",
        "        def col(grupo):\n",
        "            return next((c for c in column_groups[grupo] if c in df.columns), grupo if grupo in df.columns else None)\n",
        "\n",
        "        col_passengers = col('passenger_count')\n",
        "        col_distance = col('trip_distance')\n",
        "        col_total = col('total_amount')\n",
        "        col_pickup = col('pickup_datetime')\n",
        "        col_dropoff = col('dropoff_datetime')\n",
        "        col_pu = col('PULocationID')\n",
        "        col_do = col('DOLocationID')\n",
        "        col_start_lon = col('Start_Lon')\n",
        "        col_start_lat = col('Start_Lat')\n",
        "        col_end_lon = col('End_Lon')\n",
        "        col_end_lat = col('End_Lat')\n",
        "\n",
        "        # 1. Pasajeros\n",
        "        if col_passengers:\n",
        "            resultados['Pasajeros = nulos'] = df[col_passengers].isna().sum()\n",
        "            resultados['Pasajeros < 0'] = (df[col_passengers] < 0).sum()\n",
        "\n",
        "        # 2. Distancia\n",
        "        if col_distance:\n",
        "            resultados['Distancia = nula'] = df[col_distance].isna().sum()\n",
        "            resultados['Distancia <= 0'] = (df[col_distance] <= 0).sum()\n",
        "\n",
        "        # 3 y 4. Duraci√≥n inv√°lida\n",
        "        if col_pickup and col_dropoff:\n",
        "            pickup = pd.to_datetime(df[col_pickup], errors='coerce')\n",
        "            dropoff = pd.to_datetime(df[col_dropoff], errors='coerce')\n",
        "            resultados['Pickup/dropoff = nulos'] = pickup.isna().sum() + dropoff.isna().sum()\n",
        "            resultados['Dropoff < Pickup'] = (dropoff < pickup).sum()\n",
        "            resultados['Duraci√≥n = 0 min'] = (dropoff == pickup).sum()\n",
        "\n",
        "        # 5. Total\n",
        "        if col_total:\n",
        "            resultados['Total_amount = nulo'] = df[col_total].isna().sum()\n",
        "            resultados['Total_amount <= 0'] = (df[col_total] <= 0).sum()\n",
        "\n",
        "        # 6. Ubicaciones (zonas o coordenadas)\n",
        "        if col_pu and zona_nombres:\n",
        "            resultados['PULocationID = nulo'] = df[col_pu].isna().sum()\n",
        "            resultados['PULocationID fuera de rango'] = df[~df[col_pu].isin(zona_nombres.keys())].shape[0]\n",
        "        if col_do and zona_nombres:\n",
        "            resultados['DOLocationID = nulo'] = df[col_do].isna().sum()\n",
        "            resultados['DOLocationID fuera de rango'] = df[~df[col_do].isin(zona_nombres.keys())].shape[0]\n",
        "        # Si NO hay zonas pero s√≠ coordenadas, valida coordenadas nulas o sospechosas\n",
        "        if col_start_lon and col_start_lat and col_end_lon and col_end_lat:\n",
        "            resultados['Start_Lon = nulo'] = df[col_start_lon].isna().sum()\n",
        "            resultados['Start_Lat = nulo'] = df[col_start_lat].isna().sum()\n",
        "            resultados['End_Lon = nulo'] = df[col_end_lon].isna().sum()\n",
        "            resultados['End_Lat = nulo'] = df[col_end_lat].isna().sum()\n",
        "            # Flags de localizaci√≥n sospechosa si existen\n",
        "            if 'suspicious_location_flag' in df.columns:\n",
        "                resultados['suspicious_location_flag = True'] = df['suspicious_location_flag'].sum()\n",
        "            if 'same_location_flag' in df.columns:\n",
        "                resultados['same_location_flag = True'] = df['same_location_flag'].sum()\n",
        "\n",
        "        # Agregar los resultados al resumen\n",
        "        for k, v in resultados.items():\n",
        "            resumen.append({\"Dataset\": nombre, \"Tipo de error\": k, \"N¬∫ de registros\": v})\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Verificaci√≥n de calidad de columnas\n",
        "df_errores = analizar_valores_invalidos_dict(cleaned_datasets)\n",
        "display(df_errores)\n",
        "\n",
        "# Verificaci√≥n de calidad de columnas\n",
        "\"\"\" df_original_errores = analizar_valores_invalidos_dict(datasets)\n",
        "display(df_original_errores) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gr√°fico combinado para representar de forma clara y visual los outliers en las variables m√°s propensas a errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comparar_outliers_multivariables(unified_datasets, limpio_dict):\n",
        "    \"\"\"\n",
        "    Compara histogramas de distancia, duraci√≥n y total_amount para datasets originales y limpios.\n",
        "    Omite datasets sin las columnas requeridas.\n",
        "    \"\"\"\n",
        "    for nombre in unified_datasets:\n",
        "        if nombre not in limpio_dict:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no est√° en el conjunto limpio. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df_orig = unified_datasets[nombre].copy()\n",
        "        df_limpio = limpio_dict[nombre].copy()\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if \"trip_distance\" not in df_orig.columns or \"trip_distance\" not in df_limpio.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no tiene 'trip_distance'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        if \"trip_duration_min\" not in df_orig.columns:\n",
        "            pickup_cols = [c for c in ['tpep_pickup_datetime', 'lpep_pickup_datetime', 'pickup_datetime'] if c in df_orig.columns]\n",
        "            dropoff_cols = [c for c in ['tpep_dropoff_datetime', 'lpep_dropoff_datetime', 'dropoff_datetime', 'dropOff_datetime'] if c in df_orig.columns]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df_orig[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df_orig[dropoff_cols[0]], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df_orig[pickup_cols[0]], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "\n",
        "        if \"trip_duration_min\" not in df_limpio.columns and \"trip_duration_min\" in df_orig.columns:\n",
        "            df_limpio[\"trip_duration_min\"] = df_orig[\"trip_duration_min\"]\n",
        "\n",
        "        if \"trip_duration_min\" not in df_orig.columns or \"trip_duration_min\" not in df_limpio.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no tiene 'trip_duration_min'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        if \"total_amount\" not in df_orig.columns or \"total_amount\" not in df_limpio.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre} no tiene 'total_amount'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Filtrado para visualizaci√≥n\n",
        "        df_orig = df_orig[\n",
        "            (df_orig[\"trip_distance\"] < 20) &\n",
        "            (df_orig[\"trip_duration_min\"] < 60) &\n",
        "            (df_orig[\"total_amount\"] < 100)\n",
        "        ]\n",
        "        df_limpio = df_limpio[\n",
        "            (df_limpio[\"trip_distance\"] < 20) &\n",
        "            (df_limpio[\"trip_duration_min\"] < 60) &\n",
        "            (df_limpio[\"total_amount\"] < 100)\n",
        "        ]\n",
        "\n",
        "        # Visualizaci√≥n\n",
        "        fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
        "        fig.suptitle(f\"Distribuci√≥n de trayectos ‚Äì {nombre}\", fontsize=16)\n",
        "\n",
        "        # Distancia\n",
        "        sns.histplot(df_orig[\"trip_distance\"], bins=60, color=\"lightcoral\", label=\"Original\", ax=axs[0], alpha=0.6)\n",
        "        sns.histplot(df_limpio[\"trip_distance\"], bins=60, color=\"seagreen\", label=\"Limpio\", ax=axs[0], alpha=0.6)\n",
        "        axs[0].set_title(\"Histograma de distancias (<20 mi)\")\n",
        "        axs[0].legend()\n",
        "\n",
        "        # Duraci√≥n\n",
        "        sns.histplot(df_orig[\"trip_duration_min\"], bins=60, color=\"lightcoral\", label=\"Original\", ax=axs[1], alpha=0.6)\n",
        "        sns.histplot(df_limpio[\"trip_duration_min\"], bins=60, color=\"seagreen\", label=\"Limpio\", ax=axs[1], alpha=0.6)\n",
        "        axs[1].set_title(\"Histograma de duraci√≥n (<60 min)\")\n",
        "        axs[1].legend()\n",
        "\n",
        "        # Total amount\n",
        "        sns.histplot(df_orig[\"total_amount\"], bins=60, color=\"lightcoral\", label=\"Original\", ax=axs[2], alpha=0.6)\n",
        "        sns.histplot(df_limpio[\"total_amount\"], bins=60, color=\"seagreen\", label=\"Limpio\", ax=axs[2], alpha=0.6)\n",
        "        axs[2].set_title(\"Histograma de importe total (<100 $)\")\n",
        "        axs[2].legend()\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        plt.show()\n",
        "# Muestra de comparaci√≥n de outliers\n",
        "comparar_outliers_multivariables(unified_datasets, cleaned_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Empezamos por visualizar el dataset. Haced un plot en el que, para cada dataset, se muestren el n√∫mero de trayectos entre zonas de forma que en el eje horizontal est√©n las zonas de recogida y en el eje vertical las zonas de llegada. Extrae conclusiones preliminares. ¬øSe aprecian diferencias entre los puntos de los Yellow Cabs y los de los Green Cabs? ¬øSe aprecian diferencias entre un mes y otro?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizaci√≥n de trayectos entre las 20 zonas m√°s activas\n",
        "- Recorre ambos diccionarios de datasets (originales y limpios).\n",
        "- Genera un heatmap para cada uno.\n",
        "- Quita los n√∫meros dentro de las celdas (annot=False) para que se infieran por escala de color.\n",
        "- Usa plotly.express.imshow para interactividad: permite ver el valor al pasar el rat√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def heatmap_zonas_o_coordenadas(datasets_dict, top_n=20, decimals=3):\n",
        "    \"\"\"\n",
        "    Visualiza heatmaps de trayectos entre zonas m√°s activas.\n",
        "    Usa PULocationID/DOLocationID si existen, si no agrupa por coordenadas redondeadas.\n",
        "    Si no tiene ninguno, muestra un aviso.\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario con DataFrames\n",
        "    - top_n: n√∫mero de zonas/coordenadas m√°s activas a considerar\n",
        "    - decimals: decimales para redondear coordenadas (solo si aplica)\n",
        "    \"\"\"\n",
        "    import plotly.express as px\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        # --- Zonas oficiales (PULocationID/DOLocationID) ---\n",
        "        if {'PULocationID', 'DOLocationID'}.issubset(df.columns):\n",
        "            zonas = df.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
        "            top_pu = zonas.groupby('PULocationID')['count'].sum().nlargest(top_n).index\n",
        "            top_do = zonas.groupby('DOLocationID')['count'].sum().nlargest(top_n).index\n",
        "            zonas_top = zonas[\n",
        "                zonas['PULocationID'].isin(top_pu) & zonas['DOLocationID'].isin(top_do)\n",
        "            ]\n",
        "            pivot = zonas_top.pivot(index='DOLocationID', columns='PULocationID', values='count').fillna(0)\n",
        "\n",
        "            fig = px.imshow(\n",
        "                pivot.values,\n",
        "                labels=dict(x=\"PULocationID\", y=\"DOLocationID\", color=\"N¬∫ de Trayectos\"),\n",
        "                x=pivot.columns,\n",
        "                y=pivot.index,\n",
        "                color_continuous_scale=\"YlGnBu\",\n",
        "                title=f\"Trayectos entre zonas m√°s activas ‚Äì {nombre}\"\n",
        "            )\n",
        "            fig.update_layout(height=600, width=700)\n",
        "            fig.show()\n",
        "            continue\n",
        "\n",
        "        # --- Coordenadas (Start_Lon, Start_Lat, End_Lon, End_Lat) ---\n",
        "        coord_cols = {\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"}\n",
        "        if coord_cols.issubset(df.columns):\n",
        "            # Redondear coordenadas para zonas sint√©ticas\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "            zonas = df.groupby(['PU_zone', 'DO_zone']).size().reset_index(name='count')\n",
        "\n",
        "            top_pu = zonas.groupby('PU_zone')['count'].sum().nlargest(top_n).index\n",
        "            top_do = zonas.groupby('DO_zone')['count'].sum().nlargest(top_n).index\n",
        "            zonas_top = zonas[\n",
        "                zonas['PU_zone'].isin(top_pu) & zonas['DO_zone'].isin(top_do)\n",
        "            ]\n",
        "            zonas_top['PU_zone_str'] = zonas_top['PU_zone'].astype(str)\n",
        "            zonas_top['DO_zone_str'] = zonas_top['DO_zone'].astype(str)\n",
        "            pivot = zonas_top.pivot(index='DO_zone_str', columns='PU_zone_str', values='count').fillna(0)\n",
        "\n",
        "            fig = px.imshow(\n",
        "                pivot.values,\n",
        "                labels=dict(x=\"Zona Origen (lon, lat)\", y=\"Zona Destino (lon, lat)\", color=\"N¬∫ de Trayectos\"),\n",
        "                x=pivot.columns,\n",
        "                y=pivot.index,\n",
        "                color_continuous_scale=\"YlGnBu\",\n",
        "                title=f\"Trayectos entre 'zonas sint√©ticas' m√°s activas ‚Äì {nombre}\"\n",
        "            )\n",
        "            fig.update_layout(height=800, width=1000)\n",
        "            fig.show()\n",
        "            continue\n",
        "\n",
        "        # --- Si no hay columnas v√°lidas ---\n",
        "        print(f\"‚ö†Ô∏è {nombre}: No tiene ni columnas de zonas ni de coordenadas. No se puede generar heatmap.\")\n",
        "\n",
        "# Visualizar heatmaps de datasets unificados y limpios\n",
        "heatmap_zonas_o_coordenadas(cleaned_datasets)\n",
        "\n",
        "# Visualizar heatmaps de datasets separados y limpios\n",
        "heatmap_zonas_o_coordenadas(cleaned_separated_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para generar un diagrama de Sankey con las 5 zonas m√°s populares\n",
        "def plot_sankey_top5_dict(datasets_dict, zona_nombres=None, top_n=5, decimals=3):\n",
        "    \"\"\"\n",
        "    Genera un diagrama de Sankey para cada dataset.\n",
        "    - Si hay columnas de zona, usa LocationID y nombres.\n",
        "    - Si no, usa coordenadas redondeadas como zonas sint√©ticas.\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    - zona_nombres: diccionario {LocationID: Nombre de zona} (opcional)\n",
        "    - top_n: cu√°ntas zonas mostrar (por defecto 5)\n",
        "    - decimals: decimales para redondear coordenadas (solo para datasets sin zonas)\n",
        "\n",
        "    Devuelve:\n",
        "    - lista de figuras Sankey (una por dataset)\n",
        "    \"\"\"\n",
        "    import plotly.graph_objects as go\n",
        "\n",
        "    figuras = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        # --- CASO 1: Dataset con zonas oficiales ---\n",
        "        if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "            zonas = df.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
        "\n",
        "            # Top N zonas m√°s activas (origen y destino combinados)\n",
        "            zonas_frecuentes = pd.concat([\n",
        "                zonas.groupby('PULocationID')['count'].sum(),\n",
        "                zonas.groupby('DOLocationID')['count'].sum()\n",
        "            ]).groupby(level=0).sum().nlargest(top_n).index.tolist()\n",
        "\n",
        "            zonas_top = zonas[\n",
        "                zonas['PULocationID'].isin(zonas_frecuentes) &\n",
        "                zonas['DOLocationID'].isin(zonas_frecuentes)\n",
        "            ]\n",
        "\n",
        "            if zonas_top.empty:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: no hay suficientes trayectos entre las {top_n} zonas m√°s frecuentes.\")\n",
        "                continue\n",
        "\n",
        "            etiquetas = [f\"{z} - {zona_nombres.get(z, 'Desconocido') if zona_nombres else z}\" for z in zonas_frecuentes]\n",
        "            zona_idx = {zona: i for i, zona in enumerate(zonas_frecuentes)}\n",
        "\n",
        "            source = zonas_top['PULocationID'].map(zona_idx).tolist()\n",
        "            target = zonas_top['DOLocationID'].map(zona_idx).tolist()\n",
        "            value = zonas_top['count'].tolist()\n",
        "\n",
        "        # --- CASO 2: Dataset con coordenadas (tipo 2009) ---\n",
        "        elif all(c in df.columns for c in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            # Zonas sint√©ticas redondeadas\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "\n",
        "            zonas = df.groupby(['PU_zone', 'DO_zone']).size().reset_index(name='count')\n",
        "\n",
        "            zonas_frecuentes = pd.concat([\n",
        "                zonas.groupby('PU_zone')['count'].sum(),\n",
        "                zonas.groupby('DO_zone')['count'].sum()\n",
        "            ]).groupby(level=0).sum().nlargest(top_n).index.tolist()\n",
        "\n",
        "            zonas_top = zonas[\n",
        "                zonas['PU_zone'].isin(zonas_frecuentes) &\n",
        "                zonas['DO_zone'].isin(zonas_frecuentes)\n",
        "            ]\n",
        "\n",
        "            if zonas_top.empty:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: no hay suficientes trayectos entre las {top_n} zonas sint√©ticas m√°s frecuentes.\")\n",
        "                continue\n",
        "\n",
        "            etiquetas = [str(z) for z in zonas_frecuentes]\n",
        "            zona_idx = {zona: i for i, zona in enumerate(zonas_frecuentes)}\n",
        "\n",
        "            source = zonas_top['PU_zone'].map(zona_idx).tolist()\n",
        "            target = zonas_top['DO_zone'].map(zona_idx).tolist()\n",
        "            value = zonas_top['count'].tolist()\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: no tiene columnas de zonas ni de coordenadas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        fig = go.Figure(data=[go.Sankey(\n",
        "            node=dict(\n",
        "                pad=15, thickness=20,\n",
        "                line=dict(color=\"black\", width=0.5),\n",
        "                label=etiquetas\n",
        "            ),\n",
        "            link=dict(source=source, target=target, value=value)\n",
        "        )])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text=f\"Trayectos entre las {top_n} zonas m√°s populares ‚Äì {nombre}\",\n",
        "            font_size=12\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "        figuras.append(fig)\n",
        "\n",
        "    return figuras\n",
        "\n",
        "\n",
        "# Generamos diagramas de Sankey para los datasets de Febrero 2024 y Diciembre 2024 limpios\n",
        "# Asumiendo que zona_nombres ya est√° cargado\n",
        "plot_sankey_top5_dict(cleaned_datasets, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4\tMejora la visualizaci√≥n anterior con un heat map. Ay√∫date de los Zone Map‚Ä¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_heatmap_top5_dict(datasets_dict, zona_nombres=None, decimals=3):\n",
        "    \"\"\"\n",
        "    Genera un heatmap con trayectos entre las 5 zonas m√°s populares para cada dataset del diccionario.\n",
        "\n",
        "    Si no existen columnas de zonas, agrupa por coordenadas redondeadas (zonas sint√©ticas).\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    - zona_nombres: diccionario opcional para mapear LocationID a nombre de zona\n",
        "    - decimals: decimales para redondear coordenadas en datasets antiguos\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    for key, df in datasets_dict.items():\n",
        "        # Parsear tipo de taxi y fecha desde el nombre del archivo\n",
        "        try:\n",
        "            tipo, _, fecha = key.split(\"_\")\n",
        "            a√±o, mes = fecha.split(\"-\")\n",
        "            mes_label = f\"{mes}/{a√±o}\"\n",
        "            nombre_dataset = tipo.capitalize() + \" Cabs\"\n",
        "        except Exception as e:\n",
        "            mes_label = key\n",
        "            nombre_dataset = key\n",
        "\n",
        "        # --- CASO 1: Dataset con zonas oficiales ---\n",
        "        if all(col in df.columns for col in [\"PULocationID\", \"DOLocationID\"]):\n",
        "            zonas = df.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
        "            zonas_frecuentes = pd.concat([\n",
        "                zonas.groupby('PULocationID')['count'].sum(),\n",
        "                zonas.groupby('DOLocationID')['count'].sum()\n",
        "            ]).groupby(level=0).sum().nlargest(5).index.tolist()\n",
        "\n",
        "            zonas_top5 = zonas[\n",
        "                zonas['PULocationID'].isin(zonas_frecuentes) &\n",
        "                zonas['DOLocationID'].isin(zonas_frecuentes)\n",
        "            ].copy()\n",
        "\n",
        "            # Mapear a nombre de zona si se puede\n",
        "            if zona_nombres:\n",
        "                zonas_top5[\"Zona origen\"] = zonas_top5[\"PULocationID\"].map(zona_nombres)\n",
        "                zonas_top5[\"Zona destino\"] = zonas_top5[\"DOLocationID\"].map(zona_nombres)\n",
        "            else:\n",
        "                zonas_top5[\"Zona origen\"] = zonas_top5[\"PULocationID\"].astype(str)\n",
        "                zonas_top5[\"Zona destino\"] = zonas_top5[\"DOLocationID\"].astype(str)\n",
        "\n",
        "        # --- CASO 2: Dataset con coordenadas (tipo 2009) ---\n",
        "        elif all(c in df.columns for c in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            # Redondear para crear \"zonas sint√©ticas\"\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "            zonas = df.groupby(['PU_zone', 'DO_zone']).size().reset_index(name='count')\n",
        "            zonas_frecuentes = pd.concat([\n",
        "                zonas.groupby('PU_zone')['count'].sum(),\n",
        "                zonas.groupby('DO_zone')['count'].sum()\n",
        "            ]).groupby(level=0).sum().nlargest(5).index.tolist()\n",
        "\n",
        "            zonas_top5 = zonas[\n",
        "                zonas['PU_zone'].isin(zonas_frecuentes) &\n",
        "                zonas['DO_zone'].isin(zonas_frecuentes)\n",
        "            ].copy()\n",
        "\n",
        "            zonas_top5[\"Zona origen\"] = zonas_top5[\"PU_zone\"].astype(str)\n",
        "            zonas_top5[\"Zona destino\"] = zonas_top5[\"DO_zone\"].astype(str)\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {key}: no tiene ni columnas de zonas ni de coordenadas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        zonas_top5[\"Zona\"] = zonas_top5[\"Zona origen\"] + \" ‚Üí \" + zonas_top5[\"Zona destino\"]\n",
        "        zonas_top5[\"Mes-Tipo\"] = f\"{mes_label} ({nombre_dataset})\"\n",
        "\n",
        "        # Pivot para heatmap\n",
        "        pivot_df = zonas_top5.pivot(index=\"Zona\", columns=\"Mes-Tipo\", values=\"count\").fillna(0)\n",
        "\n",
        "        # Dibujar heatmap\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(pivot_df, annot=True, fmt=\".0f\", cmap=\"YlGnBu\", cbar_kws={'label': 'N√∫mero de trayectos'})\n",
        "        plt.title(f\"Heatmap ‚Äì Top 5 zonas m√°s populares ({mes_label}, {nombre_dataset})\")\n",
        "        plt.ylabel(\"Zona origen ‚Üí destino\")\n",
        "        plt.xlabel(\"Mes y tipo de taxi\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Generamos heatmaps para los datasets Limpios\n",
        "plot_heatmap_top5_dict(cleaned_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Segunda Parte: An√°lisis Cualitativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1\t¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s alta? ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s alta? ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s alta?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identificador de Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para detectar outliers multivariables en datasets de taxis\n",
        "def identificar_outliers_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Detecta outliers en m√∫ltiples variables clave de trayectos de taxi mediante el m√©todo IQR (Interquartile Range).\n",
        "\n",
        "    Variables evaluadas:\n",
        "    - trip_distance (distancia del viaje)\n",
        "    - fare_amount (importe del trayecto)\n",
        "    - trip_duration_min (duraci√≥n estimada en minutos)\n",
        "    - fare_per_km (importe por kil√≥metro recorrido)\n",
        "\n",
        "    Se calcula un resumen para cada dataset y se muestra por pantalla:\n",
        "    - N√∫mero de outliers por variable\n",
        "    - L√≠mites inferior y superior seg√∫n IQR\n",
        "\n",
        "    Retorna:\n",
        "        dict con {nombre_dataset: DataFrame con filas outlier y columna adicional 'Outlier en'}\n",
        "    \"\"\"\n",
        "    outliers_dict = {}\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "        print(f\"\\nüìä Analizando dataset: {nombre}\")\n",
        "\n",
        "        # Asegurar columna de duraci√≥n si no existe\n",
        "        if \"trip_duration_min\" not in df.columns:\n",
        "            pickup_cols = [c for c in ['tpep_pickup_datetime', 'lpep_pickup_datetime', 'pickup_datetime'] if c in df.columns]\n",
        "            dropoff_cols = [c for c in ['tpep_dropoff_datetime', 'lpep_dropoff_datetime', 'dropoff_datetime', 'dropOff_datetime'] if c in df.columns]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[dropoff_cols[0]], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df[pickup_cols[0]], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "\n",
        "        # Calcular fare_per_km si no existe\n",
        "        if 'fare_amount' in df.columns and 'trip_distance' in df.columns:\n",
        "            df = df[df['trip_distance'] > 0].copy()\n",
        "            df['fare_per_km'] = df['fare_amount'] / (df['trip_distance'] * 1.60934)  # ajuste a km\n",
        "\n",
        "        # Selecci√≥n de columnas relevantes\n",
        "        columnas = ['trip_distance', 'fare_amount', 'trip_duration_min', 'fare_per_km']\n",
        "        columnas = [col for col in columnas if col in df.columns]\n",
        "\n",
        "        outlier_flags = pd.DataFrame(index=df.index)\n",
        "        resumen = []\n",
        "\n",
        "        # Aplicar m√©todo IQR a cada columna\n",
        "        for col in columnas:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lim_inf = Q1 - 1.5 * IQR\n",
        "            lim_sup = Q3 + 1.5 * IQR\n",
        "\n",
        "            flag = (df[col] < lim_inf) | (df[col] > lim_sup)\n",
        "            outlier_flags[col + '_outlier'] = flag\n",
        "            resumen.append((col, flag.sum(), lim_inf, lim_sup))\n",
        "\n",
        "        # Combinar todos los outliers\n",
        "        outlier_rows = df[outlier_flags.any(axis=1)].copy()\n",
        "        if not outlier_rows.empty:\n",
        "            outlier_rows['Outlier en'] = outlier_flags[outlier_flags.any(axis=1)].apply(\n",
        "                lambda x: ', '.join(x.index[x].str.replace('_outlier', '')), axis=1\n",
        "            )\n",
        "            outliers_dict[nombre] = outlier_rows\n",
        "\n",
        "        # Mostrar resumen por pantalla con detalles\n",
        "        print(\"  üìå Resumen de outliers detectados:\")\n",
        "        for col, count, low, high in resumen:\n",
        "            print(f\"    ‚ñ™ {col:<18}: {count:>5} outliers fuera de rango [{low:.2f}, {high:.2f}]\")\n",
        "\n",
        "        if not resumen:\n",
        "            print(\"    ‚ö†Ô∏è No se encontraron columnas num√©ricas v√°lidas para an√°lisis.\")\n",
        "\n",
        "    print(\"\\n‚úÖ An√°lisis de outliers finalizado.\")\n",
        "    return outliers_dict\n",
        "\n",
        "# Ejecutar la detecci√≥n sobre los datasets limpios\n",
        "detectados = identificar_outliers_dict(cleaned_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Muestra un resumen por DataSet de registros con incoherencias entre distancia y precio. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_incoherencias_distancia_precio(datasets_dict, z_thresh=3.0, min_registros=100):\n",
        "    \"\"\"\n",
        "    Analiza incoherencias entre trip_distance y fare_amount usando regresi√≥n lineal.\n",
        "    No modifica los datasets. Solo informa.\n",
        "\n",
        "    Par√°metros:\n",
        "    - datasets_dict: diccionario {nombre: DataFrame}\n",
        "    - z_thresh: umbral de Z-score para considerar un registro incoherente\n",
        "    - min_registros: m√≠nimo de registros para aplicar regresi√≥n\n",
        "\n",
        "    Muestra un resumen con el n√∫mero de incoherencias detectadas por dataset.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Validar columnas necesarias\n",
        "        if \"trip_distance\" not in df.columns or \"fare_amount\" not in df.columns:\n",
        "            print(f\"‚ùå {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Filtrar registros v√°lidos\n",
        "        df_validos = df[\n",
        "            df[\"trip_distance\"].notnull() &\n",
        "            df[\"fare_amount\"].notnull() &\n",
        "            (df[\"trip_distance\"] > 0) &\n",
        "            (df[\"fare_amount\"] > 0)\n",
        "        ]\n",
        "\n",
        "        if len(df_validos) < min_registros:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: solo {len(df_validos)} registros v√°lidos. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Regresi√≥n lineal\n",
        "        X = df_validos[[\"trip_distance\"]].values\n",
        "        y = df_validos[\"fare_amount\"].values\n",
        "        modelo = LinearRegression()\n",
        "        modelo.fit(X, y)\n",
        "\n",
        "        # Calcular residuos y z-score\n",
        "        y_pred = modelo.predict(X)\n",
        "        residuos = y - y_pred\n",
        "        z_residuos = zscore(residuos)\n",
        "\n",
        "        # Identificar incoherencias\n",
        "        incoherentes = (abs(z_residuos) > z_thresh).sum()\n",
        "        total = len(df_validos)\n",
        "        porcentaje = 100 * incoherentes / total\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Registros v√°lidos\": total,\n",
        "            \"Incoherencias (> z_thresh)\": incoherentes,\n",
        "            \"% Incoherente\": f\"{porcentaje:.2f}%\"\n",
        "        })\n",
        "\n",
        "    # Mostrar resumen\n",
        "    print(\"\\nüìä Resumen de incoherencias entre distancia y precio (regresi√≥n):\")\n",
        "    display(pd.DataFrame(resumen))\n",
        "\n",
        "# Ejemplo de uso\n",
        "analizar_incoherencias_distancia_precio(cleaned_datasets, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trayecto con mayor precio por km"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para encontrar el trayecto con mayor precio por kil√≥metro\n",
        "# incluyendo posibles causas del alto precio (recargos, hora punta, etc.)\n",
        "def trayecto_mayor_precio_por_km(datasets_dict, zona_nombres=None, decimals=3):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con la mayor relaci√≥n precio/km para cada dataset,\n",
        "    mostrando informaci√≥n extendida:\n",
        "\n",
        "    - Origen y destino (ID + nombre o coordenadas)\n",
        "    - Duraci√≥n del trayecto\n",
        "    - Precio/km\n",
        "    - Hora punta (S√≠/No)\n",
        "    - D√≠a de la semana\n",
        "    - Recargos presentes\n",
        "    - Comparaci√≥n precio registrado vs te√≥rico\n",
        "\n",
        "    Funciona con datasets de zonas (PULocationID/DOLocationID) o solo coordenadas (Start_Lon/Lat, End_Lon/Lat).\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # --- CASO MODERNO: Zonas oficiales ---\n",
        "        if all(col in df.columns for col in [\"trip_distance\", \"fare_amount\", \"PULocationID\", \"DOLocationID\", \"pickup_datetime\"]):\n",
        "            # Asegura duraci√≥n\n",
        "            if \"trip_duration_min\" not in df.columns and \"dropoff_datetime\" in df.columns:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[\"dropoff_datetime\"], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "\n",
        "            # Hora pico y d√≠a\n",
        "            if \"Hora_Pico\" not in df.columns:\n",
        "                df[\"Hora\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\").dt.hour\n",
        "                df['Hora_Pico'] = df['Hora'].apply(lambda x: 1 if (7 <= x < 10) or (16 <= x < 19) else 0)\n",
        "            if \"Dia\" not in df.columns:\n",
        "                df[\"Dia\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\").dt.day_name()\n",
        "\n",
        "            df = df[\n",
        "                (df[\"trip_distance\"] > 0) &\n",
        "                (df[\"fare_amount\"] > 0) &\n",
        "                (df[\"trip_duration_min\"] > 0) &\n",
        "                (df[\"PULocationID\"].notnull()) &\n",
        "                (df[\"DOLocationID\"].notnull())\n",
        "            ].copy()\n",
        "\n",
        "            if df.empty:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos para calcular precio/km.\")\n",
        "                continue\n",
        "\n",
        "            tarifa_inicial = 3.00\n",
        "            tarifa_milla = 2.50\n",
        "            tarifa_minuto = 0.70\n",
        "            df[\"precio_teorico_estimado\"] = (\n",
        "                tarifa_inicial +\n",
        "                df[\"trip_distance\"] * tarifa_milla +\n",
        "                df[\"trip_duration_min\"] * tarifa_minuto\n",
        "            )\n",
        "            df[\"precio_ratio\"] = df[\"fare_amount\"] / df[\"precio_teorico_estimado\"]\n",
        "            df[\"fare_per_km\"] = df[\"fare_amount\"] / (df[\"trip_distance\"] * 1.60934)\n",
        "\n",
        "            top = df.loc[df[\"fare_per_km\"].idxmax()]\n",
        "\n",
        "            hora_pico_str = \"S√≠\" if int(top.get(\"Hora_Pico\", 0)) == 1 else \"No\"\n",
        "            weekday = top.get(\"Dia\") if pd.notnull(top.get(\"Dia\")) else None\n",
        "\n",
        "            recargos = []\n",
        "            if \"mta_tax\" in top and top[\"mta_tax\"] > 0:\n",
        "                recargos.append(\"MTA\")\n",
        "            if \"tolls_amount\" in top and top[\"tolls_amount\"] > 0:\n",
        "                recargos.append(\"Peaje\")\n",
        "            if \"congestion_surcharge\" in top and top[\"congestion_surcharge\"] > 0:\n",
        "                recargos.append(\"Congesti√≥n\")\n",
        "            if \"airport_fee\" in top and top[\"airport_fee\"] > 0:\n",
        "                recargos.append(\"Aeropuerto\")\n",
        "            if \"night_surcharge\" in top and top[\"night_surcharge\"] > 0:\n",
        "                recargos.append(\"Noche\")\n",
        "            if \"extra\" in top and top[\"extra\"] > 0:\n",
        "                recargos.append(\"Extra\")\n",
        "\n",
        "            origen = int(top[\"PULocationID\"])\n",
        "            destino = int(top[\"DOLocationID\"])\n",
        "            origen_str = f\"{origen} - {zona_nombres.get(origen, 'Desconocido') if zona_nombres else origen}\"\n",
        "            destino_str = f\"{destino} - {zona_nombres.get(destino, 'Desconocido') if zona_nombres else destino}\"\n",
        "\n",
        "        # --- CASO ANTIGUO: Coordenadas (tipo 2009) ---\n",
        "        elif all(c in df.columns for c in [\"trip_distance\", \"fare_amount\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"pickup_datetime\"]):\n",
        "            # Asegura duraci√≥n\n",
        "            if \"trip_duration_min\" not in df.columns and \"dropoff_datetime\" in df.columns:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[\"dropoff_datetime\"], errors=\"coerce\") -\n",
        "                    pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
        "                ).dt.total_seconds() / 60\n",
        "\n",
        "            # Hora pico y d√≠a\n",
        "            if \"Hora_Pico\" not in df.columns:\n",
        "                df[\"Hora\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\").dt.hour\n",
        "                df['Hora_Pico'] = df['Hora'].apply(lambda x: 1 if (7 <= x < 10) or (16 <= x < 19) else 0)\n",
        "            if \"Dia\" not in df.columns:\n",
        "                df[\"Dia\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\").dt.day_name()\n",
        "\n",
        "            df = df[\n",
        "                (df[\"trip_distance\"] > 0) &\n",
        "                (df[\"fare_amount\"] > 0) &\n",
        "                (df[\"trip_duration_min\"] > 0) &\n",
        "                (df[\"Start_Lon\"].notnull()) & (df[\"Start_Lat\"].notnull()) &\n",
        "                (df[\"End_Lon\"].notnull()) & (df[\"End_Lat\"].notnull())\n",
        "            ].copy()\n",
        "\n",
        "            if df.empty:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos para calcular precio/km.\")\n",
        "                continue\n",
        "\n",
        "            tarifa_inicial = 3.00\n",
        "            tarifa_milla = 2.50\n",
        "            tarifa_minuto = 0.70\n",
        "            df[\"precio_teorico_estimado\"] = (\n",
        "                tarifa_inicial +\n",
        "                df[\"trip_distance\"] * tarifa_milla +\n",
        "                df[\"trip_duration_min\"] * tarifa_minuto\n",
        "            )\n",
        "            df[\"precio_ratio\"] = df[\"fare_amount\"] / df[\"precio_teorico_estimado\"]\n",
        "            df[\"fare_per_km\"] = df[\"fare_amount\"] / (df[\"trip_distance\"] * 1.60934)\n",
        "\n",
        "            # Zonas sint√©ticas: redondeo de coordenadas\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "            top = df.loc[df[\"fare_per_km\"].idxmax()]\n",
        "\n",
        "            hora_pico_str = \"S√≠\" if int(top.get(\"Hora_Pico\", 0)) == 1 else \"No\"\n",
        "            weekday = top.get(\"Dia\") if pd.notnull(top.get(\"Dia\")) else None\n",
        "\n",
        "            recargos = []\n",
        "            if \"mta_tax\" in top and top[\"mta_tax\"] > 0:\n",
        "                recargos.append(\"MTA\")\n",
        "            if \"tolls\" in top and top[\"tolls\"] > 0:\n",
        "                recargos.append(\"Peaje\")\n",
        "            if \"extra\" in top and top[\"extra\"] > 0:\n",
        "                recargos.append(\"Extra\")\n",
        "            # Otros recargos: (puedes agregar m√°s campos seg√∫n tu dataset 2009...)\n",
        "\n",
        "            origen_str = f\"{top['PU_zone']}\"\n",
        "            destino_str = f\"{top['DO_zone']}\"\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas requeridas no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Precio ($)\": top[\"fare_amount\"],\n",
        "            \"Precio/km ($)\": top[\"fare_per_km\"],\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Hora Punta\": hora_pico_str,\n",
        "            \"D√≠a\": weekday,\n",
        "            \"Recargos\": \", \".join(recargos) if recargos else \"Ninguno\",\n",
        "            \"Precio Te√≥rico ($)\": top[\"precio_teorico_estimado\"],\n",
        "            \"Ratio Real/Te√≥rico\": top[\"precio_ratio\"]\n",
        "        })\n",
        "\n",
        "        if global_max is None or top[\"fare_per_km\"] > global_max[\"fare_per_km\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_max is not None:\n",
        "        # Determina origen/destino seg√∫n si es moderno o antiguo (ya comprobado)\n",
        "        if \"PULocationID\" in global_max:\n",
        "            origen = int(global_max[\"PULocationID\"])\n",
        "            destino = int(global_max[\"DOLocationID\"])\n",
        "            origen_str = f\"{origen} - {zona_nombres.get(origen, 'Desconocido') if zona_nombres else origen}\"\n",
        "            destino_str = f\"{destino} - {zona_nombres.get(destino, 'Desconocido') if zona_nombres else destino}\"\n",
        "        else:\n",
        "            origen_str = f\"{global_max['PU_zone']}\"\n",
        "            destino_str = f\"{global_max['DO_zone']}\"\n",
        "\n",
        "        hora_pico_str = \"S√≠\" if int(global_max.get(\"Hora_Pico\", 0)) == 1 else \"No\"\n",
        "        weekday = global_max.get(\"Dia\") if pd.notnull(global_max.get(\"Dia\")) else None\n",
        "\n",
        "        recargos = []\n",
        "        for campo in [\"mta_tax\", \"tolls_amount\", \"tolls\", \"congestion_surcharge\", \"airport_fee\", \"night_surcharge\", \"extra\"]:\n",
        "            if campo in global_max and global_max[campo] > 0:\n",
        "                recargos.append(campo.replace(\"_\", \" \").capitalize())\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Distancia (mi)\": global_max[\"trip_distance\"],\n",
        "            \"Precio ($)\": global_max[\"fare_amount\"],\n",
        "            \"Precio/km ($)\": global_max[\"fare_per_km\"],\n",
        "            \"Duraci√≥n (min)\": global_max[\"trip_duration_min\"],\n",
        "            \"Hora Punta\": hora_pico_str,\n",
        "            \"D√≠a\": weekday,\n",
        "            \"Recargos\": \", \".join(recargos) if recargos else \"Ninguno\",\n",
        "            \"Precio Te√≥rico ($)\": global_max[\"precio_teorico_estimado\"],\n",
        "            \"Ratio Real/Te√≥rico\": global_max[\"precio_ratio\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Ejecutar\n",
        "print(\"\\nüìä Trayecto con mayor precio/km:\")\n",
        "trayecto_mayor_precio_por_km(cleaned_datasets, zona_nombres)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trayecto con la relaci√≥n tiempo/km m√°s alta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Funci√≥n para encontrar el trayecto con mayor duraci√≥n por kil√≥metro\n",
        "# incluyendo posibles causas del alto tiempo (tr√°fico, hora punta, etc.)\n",
        "def trayecto_mayor_duracion_por_km(datasets_dict, zona_nombres=None, decimals=3):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con mayor duraci√≥n por kil√≥metro para cada dataset,\n",
        "    y tambi√©n a nivel global, mostrando nombre de zona o coordenadas y si es hora punta.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        zona_nombres: diccionario {zone_id: nombre_zona} (opcional)\n",
        "        decimals: decimales para agrupar coordenadas en caso de dataset antiguo\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame resumen con los trayectos m√°s lentos (min/km), indicando hora punta (\"S√≠\"/\"No\")\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "    global_origen_str = None\n",
        "    global_destino_str = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Detectar columnas de datetime\n",
        "        pickup_cols = [col for col in df.columns if 'pickup' in col and 'datetime' in col]\n",
        "        dropoff_cols = [col for col in df.columns if 'drop' in col and 'datetime' in col]\n",
        "\n",
        "        if not pickup_cols or not dropoff_cols:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas de datetime no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        pickup = pd.to_datetime(df[pickup_cols[0]], errors='coerce')\n",
        "        dropoff = pd.to_datetime(df[dropoff_cols[0]], errors='coerce')\n",
        "        df[\"duration_min\"] = (dropoff - pickup).dt.total_seconds() / 60\n",
        "\n",
        "        if \"trip_distance\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columna 'trip_distance' no encontrada. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[(df[\"trip_distance\"] > 0) & (df[\"duration_min\"] > 0)].copy()\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos tras filtrado. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df[\"min_per_km\"] = df[\"duration_min\"] / df[\"trip_distance\"]\n",
        "\n",
        "        # Clasificaci√≥n de hora punta (7-10h o 16-19h)\n",
        "        if \"Hora_Pico\" not in df.columns:\n",
        "            df[\"Hora\"] = pickup.dt.hour\n",
        "            df['Hora_Pico'] = df['Hora'].apply(lambda x: 1 if (7 <= x < 10) or (16 <= x < 19) else 0)\n",
        "\n",
        "        # --- Zonas oficiales (modernos) ---\n",
        "        if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "            top = df.loc[df[\"min_per_km\"].idxmax()]\n",
        "            origen_id = int(top[\"PULocationID\"])\n",
        "            destino_id = int(top[\"DOLocationID\"])\n",
        "            origen_str = f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido') if zona_nombres else origen_id}\"\n",
        "            destino_str = f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido') if zona_nombres else destino_id}\"\n",
        "        # --- Coordenadas (2009) ---\n",
        "        elif all(c in df.columns for c in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            # Zonas sint√©ticas: coordenadas redondeadas\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "            top = df.loc[df[\"min_per_km\"].idxmax()]\n",
        "            origen_str = str(top[\"PU_zone\"])\n",
        "            destino_str = str(top[\"DO_zone\"])\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: no se detectan columnas de zonas ni coordenadas para origen/destino.\")\n",
        "            continue\n",
        "\n",
        "        hora_pico_str = \"S√≠\" if int(top.get(\"Hora_Pico\", 0)) == 1 else \"No\"\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": top[\"duration_min\"],\n",
        "            \"Min/km\": top[\"min_per_km\"],\n",
        "            \"Hora Punta\": hora_pico_str\n",
        "        })\n",
        "\n",
        "        # Guardar para el global\n",
        "        if global_max is None or top[\"min_per_km\"] > global_max[\"min_per_km\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "            global_origen_str = origen_str\n",
        "            global_destino_str = destino_str\n",
        "\n",
        "    if global_max is not None:\n",
        "        hora_pico_str = \"S√≠\" if int(global_max.get(\"Hora_Pico\", 0)) == 1 else \"No\"\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": global_origen_str,\n",
        "            \"Destino\": global_destino_str,\n",
        "            \"Distancia (mi)\": global_max[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": global_max[\"duration_min\"],\n",
        "            \"Min/km\": global_max[\"min_per_km\"],\n",
        "            \"Hora Punta\": hora_pico_str\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con mayor duraci√≥n por kil√≥metro\n",
        "print(\"\\nüìä Trayecto con la relaci√≥n tiempo/km m√°s alta:\")\n",
        "trayecto_mayor_duracion_por_km(cleaned_datasets, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trayecto con relaci√≥n precio/tiempo m√°s alta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_mayor_precio_por_minuto(datasets_dict, zona_nombres=None, decimals=3):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con la mayor relaci√≥n precio por minuto para cada dataset y a nivel global,\n",
        "    mostrando or√≠genes y destinos por zona o coordenadas.\n",
        "\n",
        "    Devuelve:\n",
        "        DataFrame resumen por dataset y global, mostrando outliers solo si han pasado la limpieza.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "        # Usar la columna ya ajustada por tarifa m√≠nima y limpieza: 'Precio ($)'\n",
        "        if \"Precio ($)\" not in df.columns or \"trip_duration_min\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: faltan columnas necesarias. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[(df[\"trip_duration_min\"] > 0) & (df[\"Precio ($)\"] > 0)].copy()\n",
        "\n",
        "        # Calcula $/min si no existe, para asegurar robustez\n",
        "        if \"$/min\" not in df.columns:\n",
        "            df[\"$/min\"] = df[\"Precio ($)\"] / df[\"trip_duration_min\"]\n",
        "\n",
        "        # Filtra por el criterio de limpieza final, por si acaso alg√∫n registro suelto lo pas√≥\n",
        "        df = df[df[\"$/min\"] <= 5]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos para calcular $/min.\")\n",
        "            continue\n",
        "\n",
        "        # Zonas modernas (PULocationID/DOLocationID)\n",
        "        if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "            top = df.loc[df[\"$/min\"].idxmax()]\n",
        "            origen_str = f\"{int(top['PULocationID'])} - {zona_nombres.get(int(top['PULocationID']), 'Desconocido') if zona_nombres else int(top['PULocationID'])}\"\n",
        "            destino_str = f\"{int(top['DOLocationID'])} - {zona_nombres.get(int(top['DOLocationID']), 'Desconocido') if zona_nombres else int(top['DOLocationID'])}\"\n",
        "        # Coordenadas antiguas\n",
        "        elif all(col in df.columns for col in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            # Zonas sint√©ticas: coordenadas redondeadas\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "            top = df.loc[df[\"$/min\"].idxmax()]\n",
        "            origen_str = str(top[\"PU_zone\"])\n",
        "            destino_str = str(top[\"DO_zone\"])\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas de zona ni coordenadas detectadas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": top[\"Precio ($)\"],\n",
        "            \"$/min\": top[\"$/min\"]\n",
        "        })\n",
        "\n",
        "        if global_max is None or top[\"$/min\"] > global_max[\"$/min\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "            global_origen_str = origen_str\n",
        "            global_destino_str = destino_str\n",
        "\n",
        "    if global_max is not None:\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": global_origen_str,\n",
        "            \"Destino\": global_destino_str,\n",
        "            \"Duraci√≥n (min)\": global_max[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": global_max[\"Precio ($)\"],\n",
        "            \"$/min\": global_max[\"$/min\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con mayor precio por minuto\n",
        "print(\"\\nüìä Trayecto con la relaci√≥n precio/minuto m√°s alta:\")\n",
        "trayecto_mayor_precio_por_minuto(cleaned_datasets, zona_nombres)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2\t¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s baja? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_precio_por_km(\n",
        "    datasets_dict,\n",
        "    zona_nombres=None,\n",
        "    min_distance=0.1,\n",
        "    min_price_per_km=0.2,\n",
        "    min_price_total=2.0,\n",
        "    decimals=3\n",
        "):\n",
        "    \"\"\"\n",
        "    Versi√≥n optimizada en memoria:\n",
        "    Busca el trayecto con menor precio/km para cada dataset, soportando zonas y coordenadas.\n",
        "    Solo almacena los valores relevantes y nunca el DataFrame completo.\n",
        "    \"\"\"\n",
        "    import gc  # Garbage collector\n",
        "\n",
        "    resultados = []\n",
        "    global_min_value = None\n",
        "    global_min_row = None\n",
        "    global_min_dataset = None\n",
        "    global_min_origen_str = None\n",
        "    global_min_destino_str = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        # Elimina columnas no usadas en el an√°lisis para ahorrar memoria\n",
        "        cols_to_keep = set([\n",
        "            \"trip_distance\", \"Precio ($)\",\n",
        "            \"PULocationID\", \"DOLocationID\",\n",
        "            \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"\n",
        "        ])\n",
        "        df_cols = set(df.columns)\n",
        "        cols_to_drop = list(df_cols - cols_to_keep)\n",
        "        if cols_to_drop:\n",
        "            df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "        # Zonas modernas\n",
        "        if all(col in df.columns for col in [\"trip_distance\", \"Precio ($)\", \"PULocationID\", \"DOLocationID\"]):\n",
        "            # Filtrado vectorizado y c√°lculos en bloque\n",
        "            mask = (\n",
        "                (df[\"trip_distance\"] >= min_distance) &\n",
        "                (df[\"Precio ($)\"] >= min_price_total)\n",
        "            )\n",
        "            temp_df = df.loc[mask, [\"trip_distance\", \"Precio ($)\", \"PULocationID\", \"DOLocationID\"]]\n",
        "            if temp_df.empty:\n",
        "                del df, temp_df; gc.collect()\n",
        "                continue\n",
        "\n",
        "            precio_km = temp_df[\"Precio ($)\"] / (temp_df[\"trip_distance\"] * 1.60934)\n",
        "            temp_df = temp_df.assign(**{\"Precio/km ($)\": precio_km.values})\n",
        "            temp_df = temp_df[temp_df[\"Precio/km ($)\"] >= min_price_per_km]\n",
        "\n",
        "            if temp_df.empty:\n",
        "                del df, temp_df; gc.collect()\n",
        "                continue\n",
        "\n",
        "            idx_min = temp_df[\"Precio/km ($)\"].idxmin()\n",
        "            row = temp_df.loc[idx_min]\n",
        "            origen_str = f\"{int(row['PULocationID'])} - {zona_nombres.get(int(row['PULocationID']), 'Desconocido') if zona_nombres else int(row['PULocationID'])}\"\n",
        "            destino_str = f\"{int(row['DOLocationID'])} - {zona_nombres.get(int(row['DOLocationID']), 'Desconocido') if zona_nombres else int(row['DOLocationID'])}\"\n",
        "\n",
        "        # Dataset antiguo por coordenadas\n",
        "        elif all(col in df.columns for col in [\"trip_distance\", \"Precio ($)\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            mask = (\n",
        "                (df[\"trip_distance\"] >= min_distance) &\n",
        "                (df[\"Precio ($)\"] >= min_price_total)\n",
        "            )\n",
        "            temp_df = df.loc[mask, [\"trip_distance\", \"Precio ($)\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]]\n",
        "            if temp_df.empty:\n",
        "                del df, temp_df; gc.collect()\n",
        "                continue\n",
        "\n",
        "            precio_km = temp_df[\"Precio ($)\"] / (temp_df[\"trip_distance\"] * 1.60934)\n",
        "            temp_df = temp_df.assign(**{\"Precio/km ($)\": precio_km.values})\n",
        "            temp_df = temp_df[temp_df[\"Precio/km ($)\"] >= min_price_per_km]\n",
        "            if temp_df.empty:\n",
        "                del df, temp_df; gc.collect()\n",
        "                continue\n",
        "\n",
        "            temp_df[\"PU_zone\"] = list(zip(temp_df[\"Start_Lon\"].round(decimals), temp_df[\"Start_Lat\"].round(decimals)))\n",
        "            temp_df[\"DO_zone\"] = list(zip(temp_df[\"End_Lon\"].round(decimals), temp_df[\"End_Lat\"].round(decimals)))\n",
        "\n",
        "            idx_min = temp_df[\"Precio/km ($)\"].idxmin()\n",
        "            row = temp_df.loc[idx_min]\n",
        "            origen_str = str(row[\"PU_zone\"])\n",
        "            destino_str = str(row[\"DO_zone\"])\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            del df; gc.collect()\n",
        "            continue\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Distancia (mi)\": row[\"trip_distance\"],\n",
        "            \"Precio ($)\": row[\"Precio ($)\"],\n",
        "            \"Precio/km ($)\": row[\"Precio/km ($)\"]\n",
        "        })\n",
        "\n",
        "        if (global_min_value is None) or (row[\"Precio/km ($)\"] < global_min_value):\n",
        "            global_min_value = row[\"Precio/km ($)\"]\n",
        "            global_min_row = row\n",
        "            global_min_dataset = nombre\n",
        "            global_min_origen_str = origen_str\n",
        "            global_min_destino_str = destino_str\n",
        "\n",
        "        # Borra variables temporales tras cada dataset\n",
        "        del df, temp_df, precio_km\n",
        "        gc.collect()\n",
        "\n",
        "    if global_min_row is not None:\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_min_dataset})\",\n",
        "            \"Origen\": global_min_origen_str,\n",
        "            \"Destino\": global_min_destino_str,\n",
        "            \"Distancia (mi)\": global_min_row[\"trip_distance\"],\n",
        "            \"Precio ($)\": global_min_row[\"Precio ($)\"],\n",
        "            \"Precio/km ($)\": global_min_row[\"Precio/km ($)\"]\n",
        "        })\n",
        "\n",
        "    import pandas as pd\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor precio/km\n",
        "print(\"\\nüìä Trayecto con la relaci√≥n precio/km m√°s baja:\")\n",
        "trayecto_menor_precio_por_km(cleaned_datasets, zona_nombres, min_distance=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s baja? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_tiempo_por_km(datasets_dict, zona_nombres=None, decimals=3):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con menor relaci√≥n tiempo/km (minutos por milla) para cada dataset,\n",
        "    as√≠ como el trayecto global, **optimizado para memoria** y compatible con datasets 2009.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    resultados = []\n",
        "    global_min = None\n",
        "    global_name = None\n",
        "    global_info = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        # SOLO selecciona columnas imprescindibles y evita copiar innecesariamente\n",
        "        columnas_esenciales = [\n",
        "            \"trip_distance\", \"trip_duration_min\",\n",
        "            \"PULocationID\", \"DOLocationID\",\n",
        "            \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"\n",
        "        ]\n",
        "        cols = [col for col in columnas_esenciales if col in df.columns]\n",
        "\n",
        "        # Procesa en memoria solo lo justo\n",
        "        # Filtro previo de filas v√°lidas (no copies el df entero)\n",
        "        mask_dist = df[\"trip_distance\"] > 0 if \"trip_distance\" in df.columns else pd.Series(False)\n",
        "        # Si falta duration, la calculas al vuelo solo para las filas v√°lidas\n",
        "        if \"trip_duration_min\" not in df.columns:\n",
        "            pickup_cols = [c for c in df.columns if \"pickup\" in c and \"datetime\" in c]\n",
        "            dropoff_cols = [c for c in df.columns if \"drop\" in c and \"datetime\" in c]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                pickup = pd.to_datetime(df.loc[mask_dist, pickup_cols[0]], errors='coerce')\n",
        "                dropoff = pd.to_datetime(df.loc[mask_dist, dropoff_cols[0]], errors='coerce')\n",
        "                duration_min = (dropoff - pickup).dt.total_seconds() / 60\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin columnas de fecha v√°lidas. Se omite.\")\n",
        "                continue\n",
        "        else:\n",
        "            duration_min = df.loc[mask_dist, \"trip_duration_min\"]\n",
        "\n",
        "        # Filtro por duraci√≥n v√°lida\n",
        "        mask_valid = (duration_min > 0)\n",
        "        idx_valid = duration_min[mask_valid].index\n",
        "\n",
        "        if idx_valid.empty:\n",
        "            continue\n",
        "\n",
        "        # Solo selecciona lo necesario en esas filas\n",
        "        df_valid = df.loc[idx_valid, cols].copy()\n",
        "        df_valid = df_valid.assign(trip_duration_min=duration_min.loc[idx_valid])\n",
        "\n",
        "        # Calcula la m√©trica SOLO en las filas v√°lidas\n",
        "        min_per_km = df_valid[\"trip_duration_min\"] / df_valid[\"trip_distance\"]\n",
        "        idx_min = min_per_km.idxmin()\n",
        "        row = df_valid.loc[idx_min]\n",
        "\n",
        "        # Zonas modernas\n",
        "        if \"PULocationID\" in row and \"DOLocationID\" in row:\n",
        "            origen_str = f\"{int(row['PULocationID'])} - {zona_nombres.get(int(row['PULocationID']), 'Desconocido') if zona_nombres else int(row['PULocationID'])}\"\n",
        "            destino_str = f\"{int(row['DOLocationID'])} - {zona_nombres.get(int(row['DOLocationID']), 'Desconocido') if zona_nombres else int(row['DOLocationID'])}\"\n",
        "        # Dataset 2009\n",
        "        elif all(c in row for c in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            origen_str = f\"({round(row['Start_Lon'],decimals)}, {round(row['Start_Lat'],decimals)})\"\n",
        "            destino_str = f\"({round(row['End_Lon'],decimals)}, {round(row['End_Lat'],decimals)})\"\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: no hay columnas de zona ni coordenadas para identificar origen/destino.\")\n",
        "            continue\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Distancia (mi)\": row[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": row[\"trip_duration_min\"],\n",
        "            \"Min/km\": min_per_km.loc[idx_min]\n",
        "        })\n",
        "\n",
        "        if (global_min is None) or (min_per_km.loc[idx_min] < global_min):\n",
        "            global_min = min_per_km.loc[idx_min]\n",
        "            global_name = nombre\n",
        "            global_info = {\n",
        "                \"Origen\": origen_str,\n",
        "                \"Destino\": destino_str,\n",
        "                \"Distancia (mi)\": row[\"trip_distance\"],\n",
        "                \"Duraci√≥n (min)\": row[\"trip_duration_min\"],\n",
        "                \"Min/km\": min_per_km.loc[idx_min]\n",
        "            }\n",
        "\n",
        "        # Libera memoria\n",
        "        del df_valid, min_per_km\n",
        "\n",
        "    if global_min is not None:\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            **global_info\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor tiempo/km\n",
        "trayecto_menor_tiempo_por_km(cleaned_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s baja?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_precio_por_minuto(datasets_dict, zona_nombres, min_duracion=0.5, min_precio_total=2.0):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con menor relaci√≥n precio/tiempo (USD por minuto) para cada dataset,\n",
        "    y el trayecto global con la menor relaci√≥n.\n",
        "    Usa siempre la columna 'Precio ($)' tras limpieza, nunca 'fare_amount' original.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre: DataFrame}\n",
        "        zona_nombres: diccionario {zone_id: nombre_zona}\n",
        "        min_duracion: duraci√≥n m√≠nima para evitar divisiones por cero (min)\n",
        "        min_precio_total: precio m√≠nimo total permitido para evitar outliers (USD)\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame con los trayectos m√°s baratos por minuto, ya filtrados.\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    global_min = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Usar la columna limpia y ajustada de precio\n",
        "        if not all(col in df.columns for col in [\"Precio ($)\", \"trip_duration_min\", \"PULocationID\", \"DOLocationID\"]):\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        # Filtro de duraci√≥n y precio total m√≠nimo\n",
        "        df = df[(df[\"trip_duration_min\"] >= min_duracion) & (df[\"Precio ($)\"] >= min_precio_total)].copy()\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        # Calcular el precio por minuto\n",
        "        df[\"Precio/min ($)\"] = df[\"Precio ($)\"] / df[\"trip_duration_min\"]\n",
        "\n",
        "        # Seleccionamos el trayecto con menor precio/min v√°lido\n",
        "        top = df.loc[df[\"Precio/min ($)\"].idxmin()]\n",
        "        origen_id = int(top[\"PULocationID\"])\n",
        "        destino_id = int(top[\"DOLocationID\"])\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": top[\"Precio ($)\"],\n",
        "            \"Precio/min ($)\": top[\"Precio/min ($)\"]\n",
        "        })\n",
        "\n",
        "        if global_min is None or top[\"Precio/min ($)\"] < global_min[\"Precio/min ($)\"]:\n",
        "            global_min = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_min is not None:\n",
        "        origen_id = int(global_min[\"PULocationID\"])\n",
        "        destino_id = int(global_min[\"DOLocationID\"])\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Duraci√≥n (min)\": global_min[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": global_min[\"Precio ($)\"],\n",
        "            \"Precio/min ($)\": global_min[\"Precio/min ($)\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor precio/min\n",
        "print(\"\\nüìä Trayecto con la relaci√≥n precio/minuto m√°s baja:\")\n",
        "trayecto_menor_precio_por_minuto(cleaned_datasets, zona_nombres)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3\tMuestra la evoluci√≥n del tiempo medio de trayecto a lo largo del d√≠a. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tiempo_medio_por_hora_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Grafica la duraci√≥n media (en minutos) del trayecto a lo largo del d√≠a para cada dataset del diccionario.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    \"\"\"\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if 'pickup_datetime' in df.columns and 'dropoff_datetime' in df.columns:\n",
        "            df['hora'] = pd.to_datetime(df['pickup_datetime'], errors='coerce').dt.hour\n",
        "            df['duracion_min'] = (\n",
        "                pd.to_datetime(df['dropoff_datetime'], errors='coerce') -\n",
        "                pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
        "            ).dt.total_seconds() / 60\n",
        "        else:\n",
        "            print(f\"‚ùå {nombre}: columnas de fecha no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df['duracion_min'] > 0]\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin datos v√°lidos para calcular duraci√≥n.\")\n",
        "            continue\n",
        "\n",
        "        media_por_hora = df.groupby('hora')['duracion_min'].mean()\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.lineplot(x=media_por_hora.index, y=media_por_hora.values, marker=\"o\", color=\"orange\")\n",
        "        plt.title(f\"Duraci√≥n media del trayecto por hora ‚Äì {nombre}\")\n",
        "        plt.xlabel(\"Hora del d√≠a\")\n",
        "        plt.ylabel(\"Duraci√≥n media (minutos)\")\n",
        "        plt.xticks(range(0, 24))\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generar gr√°ficos de duraci√≥n media por hora para los datasets limpios\n",
        "plot_tiempo_medio_por_hora_dict(cleaned_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Muestra la evoluci√≥n de la distancia media de trayecto a lo largo del d√≠a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distancia_media_por_hora_dict(datasets_dict):\n",
        "    \"\"\"\n",
        "    Grafica la distancia media (millas) del trayecto a lo largo del d√≠a para cada dataset del diccionario.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "    \"\"\"\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Verificar columnas necesarias\n",
        "        if 'pickup_datetime' in df.columns and 'trip_distance' in df.columns:\n",
        "            df['hora'] = pd.to_datetime(df['pickup_datetime'], errors='coerce').dt.hour\n",
        "        else:\n",
        "            print(f\"‚ùå {nombre}: columnas necesarias no encontradas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df['trip_distance'] > 0]\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin datos v√°lidos para calcular distancia.\")\n",
        "            continue\n",
        "\n",
        "        media_por_hora = df.groupby('hora')['trip_distance'].mean()\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.lineplot(x=media_por_hora.index, y=media_por_hora.values, marker=\"o\", color=\"steelblue\")\n",
        "        plt.title(f\"Distancia media del trayecto por hora ‚Äì {nombre}\")\n",
        "        plt.xlabel(\"Hora del d√≠a\")\n",
        "        plt.ylabel(\"Distancia media (millas)\")\n",
        "        plt.xticks(range(0, 24))\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generar gr√°ficos de distancia media por hora para los datasets limpios\n",
        "plot_distancia_media_por_hora_dict(cleaned_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4\tElige dos zonas cualesquiera de la ciudad y calcula la probabilidad de desplazarse de una zona a otra en menos de X minutos. (El valor X, as√≠ como las zonas deben ser f√°cilmente modificables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import time\n",
        "\n",
        "\n",
        "def calcular_probabilidad_desplazamiento_rapido_dict(\n",
        "    datasets_dict, \n",
        "    origen, \n",
        "    destino, \n",
        "    umbral_min=10,\n",
        "    filtro_horas=None,\n",
        "    rango_duracion_valida=(1, 180),\n",
        "    incluir_analisis_temporal=False,\n",
        "    column_groups=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Calcula la probabilidad de que un trayecto entre dos zonas espec√≠ficas dure menos de X minutos\n",
        "    para cada dataset del diccionario, con normalizaci√≥n de nombres de columnas.\n",
        "\n",
        "    Par√°metros:\n",
        "        - datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "        - origen: ID de la zona de origen (int)\n",
        "        - destino: ID de la zona de destino (int)\n",
        "        - umbral_min: duraci√≥n m√°xima para considerar un trayecto como r√°pido (por defecto 10 minutos)\n",
        "        - filtro_horas: tupla (hora_inicio, hora_fin) para filtrar por franja horaria (opcional)\n",
        "        - rango_duracion_valida: tupla (min, max) para considerar trayectos v√°lidos (por defecto 1-180 mins)\n",
        "        - incluir_analisis_temporal: si True, a√±ade an√°lisis por hora y d√≠a de la semana (opcional)\n",
        "        - column_groups: diccionario con mapeo de nombres de columnas alternativos\n",
        "\n",
        "    Retorna:\n",
        "        - DataFrame con resultados consolidados\n",
        "        - Si incluir_analisis_temporal=True, retorna tupla (resultados, analisis_temporal)\n",
        "    \"\"\"\n",
        "    # Configuraci√≥n por defecto de grupos de columnas\n",
        "    if column_groups is None:\n",
        "        column_groups = {\n",
        "            'PULocationID': ['PUlocationID', 'PULocationID'],\n",
        "            'DOLocationID': ['DOlocationID', 'DOLocationID'],\n",
        "            'pickup_datetime': [\n",
        "                'tpep_pickup_datetime', 'lpep_pickup_datetime',\n",
        "                'pickup_datetime', 'Trip_Pickup_DateTime',\n",
        "                'request_datetime'\n",
        "            ],\n",
        "            'dropoff_datetime': [\n",
        "                'tpep_dropoff_datetime', 'lpep_dropoff_datetime',\n",
        "                'dropOff_datetime', 'dropoff_datetime', 'on_scene_datetime',\n",
        "                'Trip_Dropoff_DateTime'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    resultados = []\n",
        "    analisis_temporal = {}\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Normalizaci√≥n de nombres de columnas\n",
        "        def encontrar_columna(column_group):\n",
        "            for nombre_estandar in column_group:\n",
        "                if nombre_estandar in df.columns:\n",
        "                    return nombre_estandar\n",
        "            return None\n",
        "        \n",
        "        columnas_encontradas = {}\n",
        "        for col_estandar, alternativas in column_groups.items():\n",
        "            col_encontrada = encontrar_columna([col_estandar] + alternativas)\n",
        "            if col_encontrada:\n",
        "                columnas_encontradas[col_estandar] = col_encontrada\n",
        "            else:\n",
        "                print(f\"‚ùå {nombre}: no se encontr√≥ columna para {col_estandar} (alternativas: {alternativas})\")\n",
        "                break\n",
        "        \n",
        "        if len(columnas_encontradas) != len(column_groups):\n",
        "            continue\n",
        "            \n",
        "        # Renombrar columnas a nombres estandarizados\n",
        "        df = df.rename(columns={v: k for k, v in columnas_encontradas.items()})\n",
        "        \n",
        "        try:\n",
        "            # Convertir fechas y calcular duraci√≥n\n",
        "            df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
        "            df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], errors='coerce')\n",
        "            df['duracion_min'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
        "            \n",
        "            # Filtrar por franja horaria si se especifica\n",
        "            if filtro_horas:\n",
        "                hora_inicio, hora_fin = filtro_horas\n",
        "                mask_hora = (df['pickup_datetime'].dt.time >= time(hora_inicio)) & \\\n",
        "                            (df['pickup_datetime'].dt.time <= time(hora_fin))\n",
        "                df = df[mask_hora].copy()\n",
        "            \n",
        "            # Filtrar trayectos v√°lidos\n",
        "            min_duracion, max_duracion = rango_duracion_valida\n",
        "            trayectos = df[\n",
        "                (df['PULocationID'] == origen) &\n",
        "                (df['DOLocationID'] == destino) &\n",
        "                (df['duracion_min'].notna()) &\n",
        "                (df['duracion_min'] >= min_duracion) &\n",
        "                (df['duracion_min'] <= max_duracion)\n",
        "            ].copy()\n",
        "            \n",
        "            total = len(trayectos)\n",
        "            if total == 0:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin trayectos v√°lidos entre {origen} ‚Üí {destino}.\")\n",
        "                continue\n",
        "                \n",
        "            rapidos = trayectos[trayectos['duracion_min'] < umbral_min]\n",
        "            probabilidad = len(rapidos) / total\n",
        "            \n",
        "            # Resultado principal\n",
        "            resultados.append({\n",
        "                'Dataset': nombre,\n",
        "                'Trayectos': total,\n",
        "                'R√°pidos (< X min)': len(rapidos),\n",
        "                'Probabilidad (%)': round(probabilidad * 100, 2),\n",
        "                'Duraci√≥n media (min)': round(trayectos['duracion_min'].mean(), 1),\n",
        "                'Duraci√≥n mediana (min)': round(trayectos['duracion_min'].median(), 1),\n",
        "                'Desviaci√≥n est√°ndar': round(trayectos['duracion_min'].std(), 1)\n",
        "            })\n",
        "            \n",
        "            # An√°lisis temporal adicional\n",
        "            if incluir_analisis_temporal and not trayectos.empty:\n",
        "                trayectos['hora'] = trayectos['pickup_datetime'].dt.hour\n",
        "                trayectos['dia_semana'] = trayectos['pickup_datetime'].dt.day_name()\n",
        "                \n",
        "                prob_por_hora = trayectos.groupby('hora')['duracion_min'] \\\n",
        "                    .apply(lambda x: (x < umbral_min).mean()) \\\n",
        "                    .reset_index()\n",
        "                prob_por_hora.columns = ['Hora', 'Probabilidad']\n",
        "                \n",
        "                prob_por_dia = trayectos.groupby('dia_semana')['duracion_min'] \\\n",
        "                    .apply(lambda x: (x < umbral_min).mean()) \\\n",
        "                    .reset_index()\n",
        "                prob_por_dia.columns = ['D√≠a', 'Probabilidad']\n",
        "                \n",
        "                analisis_temporal[nombre] = {\n",
        "                    'por_hora': prob_por_hora,\n",
        "                    'por_dia': prob_por_dia,\n",
        "                    'datos_trayectos': trayectos[['pickup_datetime', 'duracion_min']]\n",
        "                }\n",
        "            \n",
        "            print(f\"\\nüìä {nombre}:\")\n",
        "            print(f\"- Total trayectos {origen} ‚Üí {destino}: {total}\")\n",
        "            print(f\"- Trayectos < {umbral_min} min: {len(rapidos)} ({probabilidad:.2%})\")\n",
        "            print(f\"- Duraci√≥n media: {trayectos['duracion_min'].mean():.1f} min\")\n",
        "            print(f\"- Desviaci√≥n est√°ndar: {trayectos['duracion_min'].std():.1f} min\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {nombre}: error en procesamiento - {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    # Resultados finales\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "    \n",
        "    if incluir_analisis_temporal:\n",
        "        return df_resultados, analisis_temporal\n",
        "    return df_resultados\n",
        "\n",
        "\n",
        "# Ejemplo: ¬øQu√© probabilidad hay de ir de Midtown Center (161) a West Chelsea (246) en menos de 15 minutos?\n",
        "# Uso avanzado con an√°lisis temporal\n",
        "zona_origen = 161  # Midtown Center\n",
        "zona_destino = 246 # West Chelsea\n",
        "nombre_origen = zona_nombres.get(zona_origen, \"Desconocido\")\n",
        "nombre_destino = zona_nombres.get(zona_destino, \"Desconocido\")\n",
        "\n",
        "print(f\"\\nProbabilidad de llegar de {zona_origen} - {nombre_origen} a {zona_destino} - {nombre_destino} en menos de 15 min:\")\n",
        "resultados, analisis = calcular_probabilidad_desplazamiento_rapido_dict(\n",
        "    datasets_dict=cleaned_datasets,\n",
        "    origen=161,\n",
        "    destino=246,\n",
        "    umbral_min=15,\n",
        "    filtro_horas=(7, 20),  # Solo trayectos entre 7am y 8pm\n",
        "    incluir_analisis_temporal=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5\tRepite los apartados 1.1, 1.2, 1.3, 1.4, 2.1, 2.2, 2.3 y 2.4 con un dataset del mismo mes, pero del a√±o 2009. Comenta todas las diferencias que vayas encontrando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1\tPrimer examen preliminar del dataset.  ¬øEn qu√© formato est√° el dataset y qu√© tiene que ver este formato con Big Data? ¬øQu√© par√°metros hay en el dataset? ¬øCu√°l es su significado? ¬øExisten valores aparentemente incorrectos?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_El dataset tiene otras columnas y solo yellow, no existe el green taxi.  Se normalizan las columnas_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cargamos el Dataset Febrero de 2009"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ruta base de trip-data\n",
        "data_path = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
        "\n",
        "# Cargar datasets originales de 2009\n",
        "original_dataset_2009_02= {\"yellow_df_2009_02\":cargar_parquet_local_o_url(\"original_parquets/yellow_tripdata_2009-02.parquet\", data_path + \"yellow_tripdata_2009-02.parquet\")}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Renombrar columnas de datasets originales\n",
        "renamed_dataset_2009_02 = rename_column_names_with_flags(original_dataset_2009_02, column_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar columnas renombradas\n",
        "print(f\"Columnas del Dataset yellow_tripdata_2024-02 renombrado {renamed_datasets['yellow_tripdata_2024-02'].columns}\")\n",
        "print(f\"Columnas del Dataset renamed_dataset_2009_02 renombrado {renamed_dataset_2009_02['yellow_df_2009_02'].columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpiar dataset 2009-02 (faltan columnas m√≠nimas para la limpieza)\n",
        "cleaned_dataset_2009_02= limpiar_dataset_integrado(renamed_dataset_2009_02, zona_nombres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ¬øExisten valores aparentemente incorrectos?\n",
        "# Verificaci√≥n de calidad de columnas\n",
        "df_errores_2009_02 = analizar_valores_invalidos_dict(cleaned_dataset_2009_02)\n",
        "display(df_errores_2009_02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.2 Plot de trayectos entre las 20 zonas m√°s activas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap de trayectos entre las 20 zonas m√°s activas, este gr√°fico es pesado y no tiene un mapeo de las localizaciones\n",
        "# respecto a las zonas de NYC, ser√≠a √∫til crear el mapeo de las zonas\n",
        "# para poder ver la relaci√≥n entre las zonas de origen y destino\n",
        "heatmap_zonas_o_coordenadas(cleaned_dataset_2009_02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Haz un Diagrama de Sankey que muestre los trayectos entre las 5 zonas m√°s populares (las que tienen m√°s trayectos). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gr√°fico de Sankey para trayectos entre las 20 zonas m√°s activas de febrero 2009\n",
        "plot_sankey_top5_dict(cleaned_dataset_2009_02, top_n=5, decimals=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4\tMejora la visualizaci√≥n anterior con un heat map. Ay√∫date de los Zone Map‚Ä¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generamos heatmaps para los datasets Limpios\n",
        "plot_heatmap_top5_dict(cleaned_dataset_2009_02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1\t¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s alta? ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s alta? ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s alta?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 ¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s alta?\n",
        "print(\"\\nüìä ¬øCu√°l es el trayecto en el que la relaci√≥n precio/km es m√°s alta?\")\n",
        "trayecto_mayor_precio_por_km(cleaned_dataset_2009_02, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s alta? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s alta? \n",
        "print(\"\\nüìä ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s alta? \")\n",
        "trayecto_mayor_duracion_por_km(cleaned_dataset_2009_02, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1  ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s alta? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1  ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s alta? \n",
        "def trayecto_mayor_precio_por_minuto(datasets_dict, zona_nombres=None, decimals=3):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con la mayor relaci√≥n precio por minuto para cada dataset y a nivel global,\n",
        "    mostrando or√≠genes y destinos por zona o coordenadas.\n",
        "\n",
        "    Devuelve:\n",
        "        DataFrame resumen por dataset y global, mostrando outliers solo si han pasado la limpieza.\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    global_max = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "        # Usar la columna ya ajustada por tarifa m√≠nima y limpieza: 'Precio ($)'\n",
        "        if \"Precio ($)\" not in df.columns or \"trip_duration_min\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: faltan columnas necesarias. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[(df[\"trip_duration_min\"] > 0) & (df[\"Precio ($)\"] > 0)].copy()\n",
        "\n",
        "        # Calcula $/min si no existe, para asegurar robustez\n",
        "        if \"$/min\" not in df.columns:\n",
        "            df[\"$/min\"] = df[\"Precio ($)\"] / df[\"trip_duration_min\"]\n",
        "\n",
        "        # Filtra por el criterio de limpieza final, por si acaso alg√∫n registro suelto lo pas√≥\n",
        "        df = df[df[\"$/min\"] <= 5]\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin registros v√°lidos para calcular $/min.\")\n",
        "            continue\n",
        "\n",
        "        # Zonas modernas (PULocationID/DOLocationID)\n",
        "        if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
        "            top = df.loc[df[\"$/min\"].idxmax()]\n",
        "            origen_str = f\"{int(top['PULocationID'])} - {zona_nombres.get(int(top['PULocationID']), 'Desconocido') if zona_nombres else int(top['PULocationID'])}\"\n",
        "            destino_str = f\"{int(top['DOLocationID'])} - {zona_nombres.get(int(top['DOLocationID']), 'Desconocido') if zona_nombres else int(top['DOLocationID'])}\"\n",
        "        # Coordenadas antiguas\n",
        "        elif all(col in df.columns for col in [\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]):\n",
        "            # Zonas sint√©ticas: coordenadas redondeadas\n",
        "            df[\"PU_zone\"] = list(zip(df[\"Start_Lon\"].round(decimals), df[\"Start_Lat\"].round(decimals)))\n",
        "            df[\"DO_zone\"] = list(zip(df[\"End_Lon\"].round(decimals), df[\"End_Lat\"].round(decimals)))\n",
        "            top = df.loc[df[\"$/min\"].idxmax()]\n",
        "            origen_str = str(top[\"PU_zone\"])\n",
        "            destino_str = str(top[\"DO_zone\"])\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: columnas de zona ni coordenadas detectadas. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": origen_str,\n",
        "            \"Destino\": destino_str,\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": top[\"Precio ($)\"],\n",
        "            \"$/min\": top[\"$/min\"]\n",
        "        })\n",
        "\n",
        "        if global_max is None or top[\"$/min\"] > global_max[\"$/min\"]:\n",
        "            global_max = top\n",
        "            global_name = nombre\n",
        "            global_origen_str = origen_str\n",
        "            global_destino_str = destino_str\n",
        "\n",
        "    if global_max is not None:\n",
        "        resumen.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": global_origen_str,\n",
        "            \"Destino\": global_destino_str,\n",
        "            \"Duraci√≥n (min)\": global_max[\"trip_duration_min\"],\n",
        "            \"Precio ($)\": global_max[\"Precio ($)\"],\n",
        "            \"$/min\": global_max[\"$/min\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resumen)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con mayor precio por minuto\n",
        "print(\"\\nüìä Trayecto con la relaci√≥n precio/minuto m√°s alta:\")\n",
        "trayecto_mayor_precio_por_minuto(cleaned_dataset_2009_02, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2\t¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s baja? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nüìä Trayecto con la relaci√≥n precio/km m√°s baja:\")\n",
        "trayecto_menor_precio_por_km(cleaned_dataset_2009_02, zona_nombres, min_distance=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ¬øCu√°l es el trayecto en el que la relaci√≥n tiempo/km es m√°s baja? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trayecto_menor_tiempo_por_km(datasets_dict):\n",
        "    \"\"\"\n",
        "    Encuentra el trayecto con menor relaci√≥n tiempo/km (minutos por milla) para cada dataset,\n",
        "    as√≠ como el trayecto global con la menor relaci√≥n.\n",
        "\n",
        "    Par√°metros:\n",
        "        datasets_dict: diccionario {nombre_dataset: DataFrame}\n",
        "\n",
        "    Retorna:\n",
        "        DataFrame con los trayectos m√°s r√°pidos por km\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    global_min = None\n",
        "    global_name = None\n",
        "\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        if \"trip_distance\" not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è {nombre}: sin columna 'trip_distance'. Se omite.\")\n",
        "            continue\n",
        "\n",
        "        df = df[df[\"trip_distance\"] > 0]\n",
        "\n",
        "        if \"trip_duration_min\" not in df.columns:\n",
        "            pickup_cols = [c for c in df.columns if \"pickup\" in c and \"datetime\" in c]\n",
        "            dropoff_cols = [c for c in df.columns if \"drop\" in c and \"datetime\" in c]\n",
        "            if pickup_cols and dropoff_cols:\n",
        "                df[\"trip_duration_min\"] = (\n",
        "                    pd.to_datetime(df[dropoff_cols[0]], errors='coerce') -\n",
        "                    pd.to_datetime(df[pickup_cols[0]], errors='coerce')\n",
        "                ).dt.total_seconds() / 60\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {nombre}: sin columnas de fecha v√°lidas. Se omite.\")\n",
        "                continue\n",
        "\n",
        "        df = df[df[\"trip_duration_min\"] > 0]\n",
        "        df[\"min_per_km\"] = df[\"trip_duration_min\"] / df[\"trip_distance\"]\n",
        "\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        top = df.loc[df[\"min_per_km\"].idxmin()]\n",
        "        origen_id = int(top[\"PULocationID\"])\n",
        "        destino_id = int(top[\"DOLocationID\"])\n",
        "\n",
        "        resultados.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": top[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": top[\"trip_duration_min\"],\n",
        "            \"Min/km\": top[\"min_per_km\"]\n",
        "        })\n",
        "\n",
        "        if global_min is None or top[\"min_per_km\"] < global_min[\"min_per_km\"]:\n",
        "            global_min = top\n",
        "            global_name = nombre\n",
        "\n",
        "    if global_min is not None:\n",
        "        origen_id = int(global_min[\"PULocationID\"])\n",
        "        destino_id = int(global_min[\"DOLocationID\"])\n",
        "        resultados.append({\n",
        "            \"Dataset\": f\"GLOBAL ({global_name})\",\n",
        "            \"Origen\": f\"{origen_id} - {zona_nombres.get(origen_id, 'Desconocido')}\",\n",
        "            \"Destino\": f\"{destino_id} - {zona_nombres.get(destino_id, 'Desconocido')}\",\n",
        "            \"Distancia (mi)\": global_min[\"trip_distance\"],\n",
        "            \"Duraci√≥n (min)\": global_min[\"trip_duration_min\"],\n",
        "            \"Min/km\": global_min[\"min_per_km\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "# Ejecutar la funci√≥n para encontrar el trayecto con menor tiempo/km\n",
        "trayecto_menor_tiempo_por_km(cleaned_dataset_2009_02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s baja?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ¬øCu√°l es el trayecto en el que la relaci√≥n precio/tiempo es m√°s baja?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tercera Parte: An√°lisis Predictivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1\t¬øCu√°les son las zonas donde es m√°s probable coger un taxi en funci√≥n de la hora del d√≠a?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_probabilidad_pickup_por_zona(datasets_dict):\n",
        "    \"\"\"\n",
        "    Calcula y visualiza las zonas con m√°s probabilidad de coger un taxi seg√∫n la hora del d√≠a.\n",
        "    Incluye:\n",
        "    - Heatmap general de todas las zonas.\n",
        "    - Gr√°fico de l√≠neas para el top 10 zonas.\n",
        "    - Heatmap restringido al top 20 zonas.\n",
        "    \"\"\"\n",
        "    # Concatenar todos los datasets que tengan pickup_datetime y PULocationID\n",
        "    data = []\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        if 'pickup_datetime' in df.columns and 'PULocationID' in df.columns:\n",
        "            temp = df[['pickup_datetime', 'PULocationID']].copy()\n",
        "            temp[\"dataset\"] = nombre\n",
        "            data.append(temp)\n",
        "    if not data:\n",
        "        print(\"‚ùå Ning√∫n dataset tiene las columnas requeridas.\")\n",
        "        return\n",
        "    \n",
        "    df_all = pd.concat(data)\n",
        "    df_all['pickup_datetime'] = pd.to_datetime(df_all['pickup_datetime'], errors='coerce')\n",
        "    df_all = df_all.dropna(subset=['pickup_datetime', 'PULocationID'])\n",
        "    df_all['hour'] = df_all['pickup_datetime'].dt.hour\n",
        "\n",
        "    # Calcular n√∫mero de pickups por zona y hora\n",
        "    pickup_counts = df_all.groupby(['hour', 'PULocationID']).size().reset_index(name='count')\n",
        "\n",
        "    # Normalizar por hora (para obtener probabilidad relativa)\n",
        "    total_por_hora = pickup_counts.groupby('hour')['count'].transform('sum')\n",
        "    pickup_counts['probabilidad'] = pickup_counts['count'] / total_por_hora\n",
        "\n",
        "    # Pivot para heatmap completo\n",
        "    pivot = pickup_counts.pivot(index='PULocationID', columns='hour', values='probabilidad').fillna(0)\n",
        "\n",
        "    # Visualizaci√≥n 1: Gr√°fico de l√≠neas para top 10 zonas\n",
        "    top_zonas = df_all[\"PULocationID\"].value_counts().head(10).index\n",
        "    df_top = df_all[df_all[\"PULocationID\"].isin(top_zonas)]\n",
        "    top_line_df = df_top.groupby([\"hour\", \"PULocationID\"]).size().unstack().fillna(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_line_df.plot(marker=\"o\")\n",
        "    plt.title(\"üìà Evoluci√≥n horaria de los 10 PULocationID m√°s activos\")\n",
        "    plt.xlabel(\"Hora del d√≠a\")\n",
        "    plt.ylabel(\"N√∫mero de pickups\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(title=\"Zona\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualizaci√≥n 2: Heatmap de top 20 zonas\n",
        "    top_20 = df_all[\"PULocationID\"].value_counts().head(20).index\n",
        "    filtered = df_all[df_all[\"PULocationID\"].isin(top_20)].copy()\n",
        "    filtered[\"hour\"] = pd.to_datetime(filtered[\"pickup_datetime\"]).dt.hour\n",
        "    heat_df = filtered.groupby([\"PULocationID\", \"hour\"]).size().unstack(fill_value=0)\n",
        "    heat_df = heat_df.div(heat_df.sum(axis=1), axis=0)  # Normalizar por zona\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(heat_df, cmap=\"YlOrBr\", annot=True, fmt=\".2f\")\n",
        "    plt.title(\"üî• Probabilidad normalizada por zona (Top 20)\")\n",
        "    plt.xlabel(\"Hora del d√≠a\")\n",
        "    plt.ylabel(\"Zona\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar la funci√≥n para analizar la probabilidad de recoger un taxi por hora y zona\n",
        "analizar_probabilidad_pickup_por_zona(cleaned_added_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2\t¬øCu√°l es la mejor hora del d√≠a para ir al aeropuerto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objetivo: Saber a qu√© hora los trayectos al aeropuerto son m√°s r√°pidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_horas_para_ir_aeropuerto(dict_datasets, zona_nombres):\n",
        "\n",
        "    # 1. Detectar zonas de aeropuerto autom√°ticamente\n",
        "    aeropuerto_ids = [\n",
        "        zona_id for zona_id, nombre in zona_nombres.items()\n",
        "        if isinstance(nombre, str) and (\n",
        "            'airport' in nombre.lower() or 'jfk' in nombre.lower() or 'laguardia' in nombre.lower()\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if not aeropuerto_ids:\n",
        "        print(\"‚ùå No se detectaron zonas de aeropuerto en zona_nombres.\")\n",
        "        return\n",
        "\n",
        "    print(\"üõ´ IDs de aeropuerto detectados:\", aeropuerto_ids)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # 2. Recolectar trayectos a aeropuerto\n",
        "    for nombre, df in dict_datasets.items():\n",
        "        if all(col in df.columns for col in ['pickup_datetime', 'DOLocationID', 'trip_duration_min']):\n",
        "            temp = df[['pickup_datetime', 'DOLocationID', 'trip_duration_min']].copy()\n",
        "            temp = temp[temp['DOLocationID'].isin(aeropuerto_ids)]\n",
        "            temp['hour'] = pd.to_datetime(temp['pickup_datetime'], errors='coerce').dt.hour\n",
        "            temp.dropna(subset=['hour', 'trip_duration_min'], inplace=True)\n",
        "            data.append(temp)\n",
        "\n",
        "    if not data:\n",
        "        print(\"‚ùå No hay trayectos con destino a aeropuerto.\")\n",
        "        return\n",
        "\n",
        "    df_aeropuerto = pd.concat(data)\n",
        "\n",
        "    # 3. Agrupaciones\n",
        "    duracion_media = df_aeropuerto.groupby('hour')['trip_duration_min'].mean().reset_index(name='duraci√≥n_media_min')\n",
        "    frecuencia = df_aeropuerto.groupby('hour').size().reset_index(name='n_trayectos')\n",
        "\n",
        "    resumen = pd.merge(duracion_media, frecuencia, on='hour')\n",
        "\n",
        "    # 4. Selecci√≥n de mejor hora\n",
        "    top_frecuencia = resumen['n_trayectos'].quantile(0.7)\n",
        "    candidatos = resumen[resumen['n_trayectos'] >= top_frecuencia]\n",
        "    mejor_hora = candidatos.sort_values('duraci√≥n_media_min').iloc[0]\n",
        "\n",
        "    # 5. Visualizaci√≥n\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    fig.suptitle(\"üõ´ An√°lisis de la mejor hora para ir al aeropuerto\", fontsize=16)\n",
        "\n",
        "    sns.lineplot(x='hour', y='duraci√≥n_media_min', data=resumen, marker='o', ax=axs[0], color='orange')\n",
        "    axs[0].axvline(mejor_hora['hour'], color='green', linestyle='--', label=f'Mejor hora: {int(mejor_hora[\"hour\"])}h')\n",
        "    axs[0].set_title(\"Duraci√≥n media del trayecto por hora\")\n",
        "    axs[0].set_ylabel(\"Duraci√≥n media (min)\")\n",
        "    axs[0].set_xlabel(\"Hora\")\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    sns.barplot(x='hour', y='n_trayectos', data=resumen, ax=axs[1], palette='Blues_d')\n",
        "    axs[1].axvline(mejor_hora['hour'], color='green', linestyle='--')\n",
        "    axs[1].set_title(\"Frecuencia de trayectos al aeropuerto por hora\")\n",
        "    axs[1].set_ylabel(\"N¬∫ de trayectos\")\n",
        "    axs[1].set_xlabel(\"Hora\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # 6. Imprimir resumen\n",
        "    print(\"\\nüìù Recomendaci√≥n:\")\n",
        "    print(f\"La mejor hora para ir al aeropuerto es las **{int(mejor_hora['hour'])}:00 h**.\")\n",
        "    print(f\"- Duraci√≥n media estimada: {mejor_hora['duraci√≥n_media_min']:.1f} minutos.\")\n",
        "    print(f\"- N√∫mero de trayectos registrados en esa hora: {mejor_hora['n_trayectos']}.\")\n",
        "\n",
        "    return resumen\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resumen_aeropuerto = analizar_horas_para_ir_aeropuerto(cleaned_added_datasets, zona_nombres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3\tDise√±a un modelo que, dada una hora, una zona origen, y una zona destino, predice la duraci√≥n del trayecto y su coste. Muestra la relevancia de los atributos del dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicci√≥n de duraci√≥n y coste del trayecto dada la hora, la zona de origen y la zona de destino.\n",
        "- Dado:\n",
        "    - hora (extra√≠da de pickup_datetime)\n",
        "    - zona origen (PULocationID)\n",
        "    - zona destino (DOLocationID)\n",
        "\n",
        "- Predecir:\n",
        "    - trip_duration_min\n",
        "    - total_amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preparar_dataset_modelo_total_amount_log(datasets_dict):\n",
        "    \"\"\"\n",
        "    Prepara los datos a partir de un diccionario de datasets para predecir duraci√≥n y total_amount.\n",
        "    - Combina todos los DataFrames en uno.\n",
        "    - Filtra y limpia outliers.\n",
        "    - Crea variables: hora, d√≠a_semana, log_duraci√≥n.\n",
        "    - Devuelve X, y_duracion_log, y_total, df final.\n",
        "    \"\"\"\n",
        "    # Unir todos los datasets v√°lidos en uno\n",
        "    dfs = []\n",
        "    for nombre, df in datasets_dict.items():\n",
        "        if {\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\", \"total_amount\"}.issubset(df.columns):\n",
        "            dfs.append(df[[\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\", \"total_amount\", \"trip_duration_min\"]].copy())\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No hay datasets v√°lidos con las columnas requeridas.\")\n",
        "    \n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    \n",
        "    # Limpiar registros inv√°lidos\n",
        "    df = df.dropna(subset=[\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\", \"total_amount\", \"trip_duration_min\"])\n",
        "    df = df[\n",
        "        (df[\"trip_distance\"] > 0) & \n",
        "        (df[\"total_amount\"] > 0) & \n",
        "        (df[\"trip_duration_min\"] > 0)\n",
        "    ].copy()\n",
        "    \n",
        "    # Transformaciones\n",
        "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
        "    df[\"hour\"] = df[\"pickup_datetime\"].dt.hour\n",
        "    df[\"day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
        "    df[\"log_duration\"] = np.log1p(df[\"trip_duration_min\"])\n",
        "    \n",
        "    # Preparar features y targets\n",
        "    X = df[[\"hour\", \"day_of_week\", \"PULocationID\", \"DOLocationID\", \"trip_distance\"]]\n",
        "    y_duracion_log = df[\"log_duration\"]\n",
        "    y_total = df[\"total_amount\"]\n",
        "    \n",
        "    return X, y_duracion_log, y_total, df\n",
        "\n",
        "# Probamos si carga correctamente\n",
        "X, y_duracion_log, y_total, df_modelo = preparar_dataset_modelo_total_amount_log(cleaned_added_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenar y evaluar los modelos de RandomForest: duraci√≥n y coste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "def entrenar_y_evaluar_modelos(df_modelo):\n",
        "    \"\"\"\n",
        "    Entrena y eval√∫a dos modelos de RandomForest: duraci√≥n y coste.\n",
        "    \"\"\"\n",
        "    # Variables predictoras y objetivo\n",
        "    X = df_modelo[[\"hour\", \"PULocationID\", \"DOLocationID\"]]\n",
        "    y_duracion = df_modelo[\"trip_duration_min\"]\n",
        "    y_precio = df_modelo[\"total_amount\"]\n",
        "\n",
        "    # Codificaci√≥n categ√≥rica con OneHot\n",
        "    preprocesador = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), [\"PULocationID\", \"DOLocationID\"])\n",
        "        ],\n",
        "        remainder='passthrough'  # deja pasar la hora\n",
        "    )\n",
        "\n",
        "    # Modelo para duraci√≥n\n",
        "    modelo_duracion = Pipeline([\n",
        "        (\"preprocesado\", preprocesador),\n",
        "        (\"rf\", RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    # Modelo para precio\n",
        "    modelo_precio = Pipeline([\n",
        "        (\"preprocesado\", preprocesador),\n",
        "        (\"rf\", RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    # Divisi√≥n train-test\n",
        "    X_train, X_test, y_train_d, y_test_d = train_test_split(X, y_duracion, test_size=0.2, random_state=42)\n",
        "    _, _, y_train_p, y_test_p = train_test_split(X, y_precio, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"üîÅ Entrenando modelo de duraci√≥n...\")\n",
        "    modelo_duracion.fit(X_train, y_train_d)\n",
        "\n",
        "    print(\"üîÅ Entrenando modelo de precio total...\")\n",
        "    modelo_precio.fit(X_train, y_train_p)\n",
        "\n",
        "    # Predicciones\n",
        "    y_pred_d = modelo_duracion.predict(X_test)\n",
        "    y_pred_p = modelo_precio.predict(X_test)\n",
        "\n",
        "    # Evaluaci√≥n\n",
        "    print(\"\\nüìä Evaluaci√≥n modelo de duraci√≥n:\")\n",
        "    print(f\"- MAE: {mean_absolute_error(y_test_d, y_pred_d):.2f} minutos\")\n",
        "    print(f\"- RMSE: {np.sqrt(mean_squared_error(y_test_d, y_pred_d)):.2f}\")\n",
        "    print(f\"- R¬≤: {r2_score(y_test_d, y_pred_d):.2f}\")\n",
        "\n",
        "    print(\"\\nüìä Evaluaci√≥n modelo de precio total:\")\n",
        "    print(f\"- MAE: {mean_absolute_error(y_test_p, y_pred_p):.2f} $\")\n",
        "    print(f\"- RMSE: {np.sqrt(mean_squared_error(y_test_p, y_pred_p)):.2f}\")\n",
        "    print(f\"- R¬≤: {r2_score(y_test_p, y_pred_p):.2f}\")\n",
        "\n",
        "    return modelo_duracion, modelo_precio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar y evaluar los modelos (duraci√≥n y precio)\n",
        "# 12minutos\n",
        "modelo_duracion, modelo_precio = entrenar_y_evaluar_modelos(df_modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predecir_trayecto(modelo_duracion, modelo_precio, hora, zona_origen, zona_destino):\n",
        "    \"\"\"\n",
        "    Realiza la predicci√≥n para un trayecto y muestra la importancia de los atributos.\n",
        "\n",
        "    Par√°metros:\n",
        "        modelo_duracion: modelo entrenado para duraci√≥n\n",
        "        modelo_precio: modelo entrenado para coste\n",
        "        hora: int (0-23)\n",
        "        zona_origen: int (PULocationID)\n",
        "        zona_destino: int (DOLocationID)\n",
        "\n",
        "    Retorna:\n",
        "        dict con predicciones y DataFrame con importancia de atributos\n",
        "    \"\"\"\n",
        "    entrada = pd.DataFrame([{\n",
        "        \"hour\": hora,\n",
        "        \"PULocationID\": zona_origen,\n",
        "        \"DOLocationID\": zona_destino\n",
        "    }])\n",
        "\n",
        "    # Predicci√≥n\n",
        "    duracion_pred = modelo_duracion.predict(entrada)[0]\n",
        "    precio_pred = modelo_precio.predict(entrada)[0]\n",
        "\n",
        "    # Extraer importancias del modelo RandomForest (tras preprocessing)\n",
        "    rf_duracion = modelo_duracion.named_steps[\"rf\"]\n",
        "    rf_precio = modelo_precio.named_steps[\"rf\"]\n",
        "    preproc = modelo_duracion.named_steps[\"preprocesado\"]\n",
        "    feature_names = preproc.get_feature_names_out()\n",
        "\n",
        "    importancias_duracion = pd.Series(rf_duracion.feature_importances_, index=feature_names)\n",
        "    importancias_precio = pd.Series(rf_precio.feature_importances_, index=feature_names)\n",
        "\n",
        "    top_importancia_duracion = importancias_duracion.sort_values(ascending=False).head(10)\n",
        "    top_importancia_precio = importancias_precio.sort_values(ascending=False).head(10)\n",
        "\n",
        "    return {\n",
        "        \"duracion_minutos_estimada\": round(duracion_pred, 2),\n",
        "        \"precio_estimado_dolares\": round(precio_pred, 2),\n",
        "        \"importancia_duracion\": top_importancia_duracion,\n",
        "        \"importancia_precio\": top_importancia_precio\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultado = predecir_trayecto(modelo_duracion, modelo_precio, 14, 132, 230)\n",
        "print(f\"üïí Duraci√≥n estimada: {resultado['duracion_minutos_estimada']} min\")\n",
        "print(f\"üíµ Precio estimado: {resultado['precio_estimado_dolares']} $\")\n",
        "print(\"\\nüìä Atributos m√°s relevantes para duraci√≥n:\")\n",
        "print(resultado[\"importancia_duracion\"])\n",
        "print(\"\\nüìä Atributos m√°s relevantes para precio:\")\n",
        "print(resultado[\"importancia_precio\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def evaluar_con_cross_validation(modelo, X, y, nombre=\"Modelo\"):\n",
        "    \"\"\"\n",
        "    Aplica validaci√≥n cruzada y muestra MAE, RMSE y R¬≤ promedio.\n",
        "\n",
        "    Par√°metros:\n",
        "        modelo: Pipeline sklearn (con preprocesamiento y modelo)\n",
        "        X: variables predictoras\n",
        "        y: variable objetivo\n",
        "        nombre: identificador del modelo\n",
        "\n",
        "    Retorna:\n",
        "        dict con m√©tricas promedio\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÅ Evaluando {nombre} con cross-validation (cv=5)...\")\n",
        "\n",
        "    # MAE (negativo por convenci√≥n sklearn)\n",
        "    scores_mae = cross_val_score(modelo, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "    scores_rmse = cross_val_score(modelo, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
        "    scores_r2 = cross_val_score(modelo, X, y, cv=5, scoring='r2')\n",
        "\n",
        "    resultados = {\n",
        "        \"MAE\": -scores_mae.mean(),\n",
        "        \"RMSE\": -scores_rmse.mean(),\n",
        "        \"R2\": scores_r2.mean()\n",
        "    }\n",
        "\n",
        "    for metric, value in resultados.items():\n",
        "        print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "    return resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variables predictoras\n",
        "X = df_modelo[[\"hour\", \"PULocationID\", \"DOLocationID\"]]\n",
        "y_duracion = df_modelo[\"trip_duration_min\"]\n",
        "y_precio = df_modelo[\"total_amount\"]\n",
        "\n",
        "evaluar_con_cross_validation(modelo_duracion, X, y_duracion, nombre=\"Duraci√≥n\")\n",
        "evaluar_con_cross_validation(modelo_precio, X, y_precio, nombre=\"Precio\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
